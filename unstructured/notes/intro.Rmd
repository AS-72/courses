---
title: "Unstructured Data Analytics"
description: |
  Introduction and Data
output:
  radix::radix_article:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# What Is Unstructured Data?

The term *unstructured data* is rife with wackiness and will certainly shift with time. We can, however, work from a place where we just consider unstructured data to be data that does not nicely fit into a pre-defined tabular manner. If we take this consideration to a logical end, then we can see that many forms of data might come to us as unstructured. However, we need to impose structure on our data so that we can achieve our goals (whatever they might be).

<aside>
Once we get rolling, it will become clear that most data is unstructured.
</aside>

And this is where the challenge of unstructured data rests -- getting that data into a usable form. Some data that we consider to be unstructured is structured, but not in an immediately useful manner. 

# Data Types

## Text

Text is likely to be what you will find the most (and will be our biggest focus). 

## Images

You may not believe this, but we are at the point now where there are tons of images floating around online. Not only do people put an obscene number of pictures of themselves online, but we also have an incredible number of images from computers (drones, etc.).

## Video

Have you pumped gas lately and one of those new-fangled pumps with the ads on the screen? What about self-checkout Target or the tool aisles at Home Depot? If so, we are part of the rapid expansion of video data. 

## Audio

Alexa, Google, and any other machine that you talk to has recorded that data for analysis.

# Acquisition & Types

Where exactly do we find these data types? All over the place. The location of the data does not (generally) pose an issue for us; instead, we primarily need to be concerned about what form the data takes.

## Hypertext Markup Language

Where would the web be without HTML? Probably a much different place. For us, though, html is a great place to find data. Life is easy when we are pulling data out of html tables:

```{r}
library(rvest)

songs = read_html("https://en.wikipedia.org/wiki/List_of_Hot_Country_Songs_number_ones_of_2005") %>% 
  html_table() %>% 
  `[[`(1)

songs
```


What about this <a href="https://www.billboard.com/charts/country-songs">page</a>? How would we go about getting that data?

### Cascading Style Sheets

Before CSS, the web was styled purely with HTML -- and what a style it was! While we might not really care about style for our purpose, CSS can help find stuff within a webpage. CSS has a wide variety of selectors, combinators, pseudo-classes, and pseudo-elements that we can use to pull data off of webpages.

Whenever you are trying to pull data off of a page, it is always a good idea to inspect the source for any hints offered. Let's say that I want to do some headhunting for an analytics department and we want to find some <a href="https://psychology.nd.edu/graduate-programs/areas-of-study/quantitative/area-faculty/">Quant Psych faculty</a> who might be worth a visit. We also want to know where those people went to school. We might also want to grab everyone else, but we don't need to know their educational background.

```{r}
facultyData = read_html("https://psychology.nd.edu/graduate-programs/areas-of-study/quantitative/area-faculty/") %>% 
  html_nodes(".columns.medium-9.large-10")

faculty = lapply(1:length(facultyData), function(x) {
  
  out = tryCatch({
    name = facultyData[x] %>% 
      html_nodes("h3") %>% 
      html_text()
    
    area = facultyData[x] %>% 
      html_nodes(".faculty-areas ul li") %>% 
      html_text()
    
    email = facultyData[x] %>% 
      html_nodes(".faculty-email") %>% 
      html_text()
    
    webpage = facultyData[x] %>% 
      html_nodes("h3 a") %>% 
      html_attr("href") %>% 
      paste("https://psychology.nd.edu/", ., sep = "")
    
    if(area == "Quantitative") {
      
      education = read_html(webpage) %>% 
        html_nodes(".faculty-education p") %>% 
        html_text()
      
      allFaculty = data.frame(name = name, 
                              area = area, 
                              education = education,
                              email = email,
                              webpage = webpage,
                              stringsAsFactors = FALSE)
    } else {
      allFaculty = data.frame(name = name, 
                              area = area, 
                              education = NA,
                              email = email,
                              webpage = webpage,
                              stringsAsFactors = FALSE)
    }
  }, error = function(e) {
    return(NA)
  })
  
  return(out)
})


```


## JavaScript Object Notation

More and more, we are seeing JavaScript Object Notation (JSON) creep up everywhere on the web, because it is designed to facilitate smooth information transfer between a server and an individual's browser. When nice data gets served up to a fancy table or visualization on the web, it is probably coming through in JSON. An added benefit (not for us), but this data cannot really be scraped anymore; instead, we often need to look through the network structure find where it is located and then read it in directly (note -- it cannot always be found).

```
'[
  "firstName": "Seth",
  "lastName": "Berry",
  "isAlive": true,
  "age": 33,
  "address": {
    "streetAddress": "337 Mendoza",
    "city": "Notre Dame",
    "state": "IN",
    "postalCode": "46566"
  },
  "phoneNumbers": [
    {
      "type": "home",
      "number": "217 884-2303"
    },
    {
      "type": "office",
      "number": "574 631-0018"
    },
    {
      "type": "mobile",
      "number": "217 822-0018"
    }
  ],
  "children": [],
  "spouse": null
]'
```

Here is how we can read json right into R:

```{r}
jsonlite::fromJSON("https://api.dibitnow.com/api/v1/eventPerformers/getEventPerformers?eventId=1")
```


Simple json is not really very difficult to parse; however, json can be incredibly nested, so you need to do a lot of flattening and joining.



## Extensible Markup Language

Extensible markup language (XML) was created with the intention of being both human and machine readable -- and it generally fits that bill. Querying XML, however, tends to require the use of XPaths. XPaths are situated somewhere along the lesser circles of Hell, but can be used to great effectiveness when trying to pull stuff out of XML.
