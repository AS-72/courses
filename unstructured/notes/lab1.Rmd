---
title: "Unstructured Data Analytics"
description: |
  Lab 1
output: distill::distill_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Step 1 -- Messing With APIs

Using APIs is an important part of modern data acquisition tasks. Fortunately, they are pretty easy to put together (so long as you can follow some instructions). We are going to be working with REST APIs, so we can essentially just pass a URL request to a server -- the words make it sound more complicated than it actually is. 

Let's start by playing with the <a href="https://www.yelp.com/developers/documentation/v3">Yelp Fusion API</a>. If you were doing this on your own, you would need to register an app so that you could get a *token* (or *key*, or *secret*) -- mine is given below if you don't want to mess with getting your own. While I have provided mine for your temporary convenience, you should really mess with getting your own sooner rather than later. All you need to do is register for an account and create a new app.

<aside>
You will see many service will ask you to create an app and it is never complicated. They really just want a name and sometimes and page (I direct them to the Mendoza page).
</aside>

When creating these URLs, we are given a base url, usually something like `https://api.yelp.com/v3/` and then we start adding stuff to it. Most of the stuff that we would want from Yelp is on the *business* directory, so we can add that to the link -- `https://api.yelp.com/v3/businesses/`. Next, we might want to do something very simple, like search for a restaurant. We will need to tell the API what we are doing -- `https://api.yelp.com/v3/businesses/search`. 

The API now knows that we will be utilizing the search service. If you have ever included some of the extra information on a Youtube link, the next part will look pretty familiar to you: `https://api.yelp.com/v3/businesses/search?term=cambodian+thai`. Right after our `search` parameter, we pass an argument called `term` and set it equal to our search query. You likely noticed that what should have been a space was replaced by a `+` -- URLs do not do spaces and other special characters. While the `+` works just fine for a space, you might want to familiarize yourself with <a href="https://www.w3schools.com/tags/ref_urlencode.asp">URL encoding</a>. Finally, our search parameter needs another argument, `location`, giving us `https://api.yelp.com/v3/businesses/search?term=cambodian+thai&location=south+bend`. 

That is the first, and most difficult, step in getting an API call ready. The next part is pretty easy -- we just need to pass our authentication key into the request. Again, this is something that sounds tricky, but really is pretty easy. 

Remember how we explored the headers within a request? These headers are where we will pass information along to the server. Most APIs only require a little bit of information to be passed into the header. This information takes the form of the authentication aspect of the request.

```{r}
library(httr)

library(jsonlite)

library(rvest)

apiKey = "8my5DQcAI1nxMW7oMTyZd-NIFHFQElynCZ35OWJSBMihg0YPJQfYxDUrK_2kx1qxdTWMneXwLOvYgkAbHpzIVXRsxXyLlnFA2gLACOBz9BsdCdowYo5rnZVp3khHXHYx"

thaiSearch = GET("https://api.yelp.com/v3/businesses/search?term=cambodian+thai&location=south+bend&limit=2",
                 add_headers(Authorization = paste("Bearer", apiKey, sep = " ")))
```

<aside>
The `Authorization: Bearer key` construction is common.
</aside>

## Step 2 -- Parsing The Return

There will be times when working with an API is a one-step process, but...this is rarely the case. First, most API requests are going to come through in JSON. We will need to parse the request: 

```{r}
searchParsed = jsonlite::fromJSON((content(thaiSearch, as = "text")))

searchParsed
```

There are many different ways of parsing this request (e.g., `content(thaiSearch, as = "parsed")`), but the above will work pretty well for our current case. 

In looking at our `searchParsed` object, we should have a pretty good idea about what we want:

```{r}
thaiResults = searchParsed$businesses[1, ]
```


## Step 3 -- Passing Information

In many API scenarios, you will be looking at a few steps to get to where you want to be. If we want to have more information about Cambodian Thai, we can get the information directly once we have the ID (which we got from our search). 

```{r}
thaiID = paste("https://api.yelp.com/v3/businesses/", thaiResults$id, sep = "")

cambodianThai = GET(thaiID,
    add_headers(Authorization = paste("Bearer", apiKey, sep = " ")))

thaiParsed = jsonlite::fromJSON(content(cambodianThai, as = "text"))
```

On occasion, you might need to do some silly stuff to work that json stuff into a reasonable data frame:

```{r}
content(cambodianThai, as = "text") %>% 
  jsonlite::fromJSON(., simplifyDataFrame = TRUE, flatten = TRUE) %>% 
  unlist() %>% 
  t() %>%  
  tibble::as_tibble()
```


## Step 4 -- Scraping Some Text

Now we want to scrape some text about Cambodian Thai. We can do this in the standard way with our `rvest` code.

### IMPORTANT NOTE

What you see that follows worked one day, but it is no promise that it will work today. If it has changed, you might need to look through the source to reformulate the selectors.

```{r}
library(rvest)

cambodianThai = "https://www.yelp.com/biz/cambodian-thai-south-bend"

cambodianThaiHTML = read_html(cambodianThai)

ctRatings = cambodianThaiHTML %>%
  html_nodes(".lemon--div__373c0__1mboc.arrange-unit__373c0__1piwO.arrange-unit-fill__373c0__17z0h.border-color--default__373c0__2oFDT .lemon--span__373c0__3997G.display--inline__373c0__1DbOG.border-color--default__373c0__2oFDT .lemon--div__373c0__1mboc.i-stars__373c0__Y2F3O") %>%
  html_attr("aria-label") %>% 
  stringr::str_extract("[0-5]")

ctReviews = cambodianThaiHTML %>% 
  html_nodes("p.lemon--p__373c0__3Qnnj.text__373c0__2pB8f.comment__373c0__3EKjH.text-color--normal__373c0__K_MKN.text-align--left__373c0__2pnx_ .lemon--span__373c0__3997G") %>% 
  html_text()

ctData = data.frame(ratings = ctRatings, 
                    reviews = ctReviews, 
                    restaurant = "cambodian thai", 
                    stringsAsFactors = FALSE)

DT::datatable(ctData)
```


### An Aside

I have mentioned that website will change and they are not concerned with telling you about it. This is how the same code looked just a year ago:

```{r, eval = FALSE}
ctRatings = cambodianThaiHTML %>%
  html_nodes(".ratings-stars .i-stars") %>% 
  html_attr("aria-label") %>% 
  stringr::str_extract("[0-5]")

ctReviews = cambodianThaiHTML %>% 
  html_nodes(".ratings-reviews") %>% 
  html_text()
```

Definitely an easier scrape!

## Step 4b

You should notice that we only pulled in 20 reviews, while we know that we have around `r thaiParsed$review_count` reviews (I have no doubt that you saw that in the parsed data). If we want them all, we need to do just a little more work. Since there are 20 reviews per page, we are looking at `r ceiling(thaiParsed$review_count) / 20` pages worth of reviews. And since we know how the links are constructed, we can just create them:

```{r, eval = FALSE}
linkNumbers = seq(from = 0, to = (floor(thaiParsed$review_count / 20) * 20), by = 20)

links = paste("https://www.yelp.com/biz/cambodian-thai-south-bend?start=", 
              linkNumbers, sep = "")

allReviews = lapply(links, function(x) {
  cambodianThai = x
  
  cambodianThaiHTML = read_html(cambodianThai)
  
  ctRatings = cambodianThaiHTML %>%
  html_nodes(".lemon--div__373c0__1mboc.arrange-unit__373c0__1piwO.arrange-unit-fill__373c0__17z0h.border-color--default__373c0__2oFDT .lemon--span__373c0__3997G.display--inline__373c0__1DbOG.border-color--default__373c0__2oFDT .lemon--div__373c0__1mboc.i-stars__373c0__Y2F3O") %>%
  html_attr("aria-label") %>% 
  stringr::str_extract("[0-5]")
  
  ctReviews = cambodianThaiHTML %>% 
    html_nodes("p.lemon--p__373c0__3Qnnj.text__373c0__2pB8f.comment__373c0__3EKjH.text-color--normal__373c0__K_MKN.text-align--left__373c0__2pnx_ .lemon--span__373c0__3997G") %>% 
    html_text()
  
  ctData = data.frame(ratings = ctRatings, 
                      reviews = ctReviews, 
                      restaurant = "cambodian thai", 
                      stringsAsFactors = FALSE)
  
  return(ctData)
})

allReviews = data.table::rbindlist(allReviews)
```


## Step 5 -- Analyze The Text

Given what we have already seen, try to learn a little bit about what words are being used in the reviews. Can you pick out a dish that people seem to talk about frequently? 

Just as a starting place, this might start to get at the dishes that are discussed:

```{r}
library(dplyr)

library(tidytext)

bigrams = ctData %>% 
  unnest_tokens(., ngrams, reviews, token = "ngrams", n = 2) %>% 
  tidyr::separate(ngrams, c("word1", "word2"), sep = "\\s") %>% 
  count(word1, word2, sort = TRUE)

rmarkdown::paged_table(bigrams)
```

You might also want to find and grab the date for each review. This might make for some interesting plotting. 

You can also produce some handy visualizations for your text:

```{r}
library(ggplot2)

ctData %>% 
  unnest_tokens(., word, reviews) %>% 
  count(word, sort = TRUE) %>% 
  slice(1:30) %>% 
  mutate(word = factor(word, levels = unique(word))) %>% 
  ggplot(., aes(word, n)) +  
  geom_col() + 
  coord_flip() +
  theme_minimal()
```

Can you remove the stopwords? If you create a data frame with multiple restaurants, can you reproduce that plot with multiple restaurants?

# Your Deliverable

No matter how much work you get done, your goal is to produce a "three slide story".