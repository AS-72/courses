---
title: "Images"
output:
  radix::radix_article:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Geospatial Analysis

## Plotting

By now, you have learned several ways of plotting geospatial data, so I am just going to leave you with the following:

```{r}
library(ggplot2)

library(leaflet)

library(rgdal)

library(sp)

shpFile = readOGR(dsn = "C:/Users/sberry5/Documents/teaching/courses/unstructured/data/Active_Demolition_Orders/Active_Demolition_Orders.shp", 
                  stringsAsFactors = FALSE)

latLon = spTransform(shpFile, CRS("+proj=longlat +datum=WGS84"))

leaflet(latLon) %>% 
  addTiles() %>% 
  addPolygons()
```


## Point Pattern Analysis

With so much focus on creating clickable maps, we should focus some attention on actual analyses of those maps. One way we can do this is by using *Point Pattern Analysis* (PPA). PPA comes in many different forms, but we are ultimately going to end up testing if our data has Complete Spatial Randomness (CSR) or if it is distributed in some other way (uniform/regular or clustered).  In PPA, we view CSR as being the null hypothesis.

In PPA, we are not looking at the frequency with which a point occurs.  Instead, we are only looking for a point's presence and where it is in space compared to other points.  The figure below contains all of homes marked for destruction, the geographic mean, and an ellipse that captures the standard distance deviation of the houses. The ellipses are largely stacked on top of each other (save for Anthropology), but they give us a good idea of how the points are distributed (the horizontal ovals match the shape of our data pretty nicely).  If our points were distributed in a different manner, the "deviation ellipses" would give us an idea of shape and direction. 


```{r}
latLonOnly = lapply(1:length(shpFile@polygons), function(x) {
  res = data.frame(x = shpFile@polygons[[x]]@labpt[1], 
                   y = shpFile@polygons[[x]]@labpt[2])
})

latLonOnly = dplyr::bind_rows(latLonOnly)

meanCenterX = mean(latLonOnly$x)

meanCenterY = mean(latLonOnly$y)

stDevX = sd(latLonOnly$x)

stDevY = sd(latLonOnly$y)

stDistance = sqrt(sum(((latLonOnly$x - meanCenterX)^2 + 
                         (latLonOnly$y - meanCenterY)^2) / 
                        nrow(latLonOnly)))

segment <- c(0, 360)

angle <- 0 * pi/180

segment <- segment * pi/180

z = seq(segment[1], segment[2], length = 100 + 1)

xx = stDevX * cos(z)

yy = stDevY * sin(z)

alpha = atan2(yy, xx)

# alpha <- alpha%%pi

alpha = alpha * 180/pi

rad = sqrt(xx^2 + yy^2)

xp = rad * cos(alpha + angle) + meanCenterX

yp = rad * sin(alpha + angle) + meanCenterY

leaflet(latLon) %>% 
  addProviderTiles("CartoDB.Positron") %>% 
  addPolygons() %>% 
  addCircles(lng = meanCenterX, lat = meanCenterY, color = "red") %>% 
  addPolygons(lng = xp, lat = yp, color = "red")

plot(latLon, pch = "+", col = "blue")

points(meanCenterX, meanCenterY, col = "red", pch = 16)

plotrix::draw.ellipse(meanCenterX, meanCenterY, a = stDevX, b = stDevY, border = "red")

```


You should notice that the mean locations are different from the mean locations in the interactive map.  Recall that we are only looking at the presence of the points in the PPA, not the number of points at a location.  We are only concerned that Harvard has a point, not that there are 8 points there.  Within the PPA perspective, this makes intuitive sense.  Things are rarely stacked on top of each other in nature (single-family homes, trees, beaver dams), so PPA does not deal with stacked points.

### Intensity

We can also take a look at local intensity by using kernel intensity estimation -- kernel intensity is essentially kernel density, it just has a different name in the spatial context. You will, however, see them frequently interchanged.  Kernel intensity estimation allows us to determine where in our space points are most likely to occur.  In this particular example, we used likelihood cross-validation bandwidth (i.e., radius) selection.  We could have used a number of different *bandwidth* methods (e.g., Diggle, Stoyan), but this method works well with our data; ideally, we would have selected a method based more on theory, but this is an exploratory process.  

In selecting a method, we essentially have a choice between capturing highly-local events very well or viewing larger smoothed areas without the apparent presence of local instances.  Methods like Diggle and Stoyan pick up on all of the universities better because they are calculating intensity with a smaller radius; however, these offer very little more than the point pattern plot for the data in our example.  In the current example, we use a leave-one-out intensity calculation; in other words it does not assess the kernel intensity contribution of a point in calculating its own intensity.

In the following figure, we can see that the areas with the greatest intensities are clearly hitting where we have the most universities.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
library(maptools)

library(spatstat)

shpFile = raster::shapefile(x = "C:/Users/sberry5/Documents/teaching/courses/unstructured/data/Active_Demolition_Orders/Active_Demolition_Orders.shp")

window = as.owin(W = shpFile)

allPPP = ppp(x = latLonOnly$x, y = latLonOnly$y, 
                window = window)

plot(allPPP, pch = "+")

plot(quadratcount(allPPP, nx = 4, ny = 2), add = T, col = "blue")

regionalIntensity = data.frame(usRegion = factor(), number = numeric())

for (i in unique(contUS@data$SUB_REGION)){
  subPop = contUS[contUS@data$SUB_REGION==i, ]
  
  subPPP = ppp(allPPP$x, allPPP$y, window = as.owin(subPop))
  
  regionalIntensity = rbind(regionalIntensity, 
                            data.frame(usRegion = factor(i, levels = contUS@data$SUB_REGION),
                                       number = subPPP$n))
}

colorScale = color.scale(regionalIntensity[order(regionalIntensity[, 2]), 2], 
                         color.spec = "rgb", extremes = c("blue", "orange"), alpha = .05)

par(mar=c(5,13,4,2)) 

barplot(regionalIntensity[order(regionalIntensity[, 2]), 2], 
        names.arg = regionalIntensity[order(regionalIntensity[, 2]), 1], 
        horiz = T, las = 2, space = 1, col = colorScale)

plot(density.ppp(allPPP, sigma = bw.ppl(allPPP), edge = T))

plot(Gest(allPPP))

envLTest = envelope(allPPP, Lest, nsim = 150, global = TRUE, verbose = FALSE)
```


### Ripley's K Function

All of the descriptive stuff for the PPA is great, but we need to know how our point pattern is distributed.  There are several different nearest neighbor analyses of spatial dependence from which we may choose (e.g., F, G), but we are going to use Ripley's K function, $K(r) =  \lambda^{-1}$, for obvious reasons (less obviously, it handles more points and uses different scale lengths for estimation).  The K function is not exactly a nearest neighbor function in a strict sense, but is often grouped with them.    

```{r, echo=FALSE, message=FALSE, warning=FALSE, results="hide", fig.align='center'}
envLTest = envelope(allPPP, Lest, nsim = 150, global = TRUE, verbose = FALSE)

plot(envLTest, main = "K Function Test")
```

Examining the plot tells us several things.  The most obvious is that our data (represented by the black line) is below the CSR line.  This would indicate that our point pattern is clustered.  The next bit of information we get from this plot is that our data is not within the CSR envelope; this can be interpreted as "significant"" clustering.  Finally, we can see that the clustering increases as our distance measure unit (*r*) increases.  In looking at our point pattern, this is all readily apparent.   



# Item Detection

```{r}
library(OpenImageR)

jason = readImage("C:/Users/sberry5/Documents/teaching/courses/unstructured/data/f13JV.png")

longImage = reshape2::melt(jason)
  
rgbImage = reshape(longImage, timevar = "Var3",
                      idvar = c("Var1", "Var2"), direction = "wide")

rgbImage = rgbImage %>% 
  mutate(Var1 = rev(Var1), 
         var2 = rev(Var2),)

##########
# Part 4 # Finding a bright object
##########

with(rgbImage, plot(Var2, Var1, col = rgb(rgbImage[, 3:5]), asp = 1, pch = "."))

forestOnly = rgbImage[rgbImage$Var2 > 850 | rgbImage$Var2 < 625, ]

forestMean <- apply(forestOnly[, 3:5], 2, mean)  # Find typical sand color values

forestSD <- apply(forestOnly[, 3:5], 2, sd)

colorZ <- sweep(rgbImage[, 3:5], 2, forestMean, "-")  # How much do all colors

colorZ <- sweep(colorZ, 2, forestSD, "/")  # deviate from sand colors?

plot(density(rowSums(colorZ)))

# Plot a "binarized" image, based on color Z-scores

plot(rgbImage$Var2, rgbImage$Var1, col = rgb(colorZ > 4), asp = 1, pch = ".")
```


https://appsilon.com/ship-recognition-in-satellite-imagery-part-i/

```{r}
devtools::install_github("bnosac/image", subdir = "image.darknet", build_vignettes = TRUE)
```


```{r}
library(image.darknet)

yolo_voc <- image_darknet_model(type = "classify", 
                                     model = "darknet.cfg", 
                                     weights = system.file(package="image.darknet", "models", "tiny-yolo-voc.weights"), 
                                     labels = system.file(package="image.darknet", "include", 'darknet', "data", "voc.names"))

x <- image_darknet_classify(file = "C:/Users/sberry5/Downloads/test.jpeg", 
                          object = yolo_voc)
```
