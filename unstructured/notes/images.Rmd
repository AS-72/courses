---
title: "Images"
output:
  distill::distill_article:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Packages

```{r, eval = FALSE}
install.packages(c("leaflet", "rgdal" , "sp", "plotrix", "maptools", 
                   "spatstat", "OpenImageR"))

devtools::install_github("bnosac/image", subdir = "image.darknet", build_vignettes = TRUE)
```


# Geospatial Analysis

## Plotting

By now, you have learned several ways of plotting geospatial data, so I am just going to leave you with the following:

```{r}
library(leaflet)

library(rgdal)

library(sp)

shpFile = readOGR(dsn = "C:/Users/sberry5/Documents/teaching/courses/unstructured/data/Active_Demolition_Orders/Active_Demolition_Orders.shp",
                  stringsAsFactors = FALSE)

# shpFile = readOGR(dsn = "D:/projects/courses/unstructured/data/Active_Demolition_Orders/Active_Demolition_Orders.shp", 
#                   stringsAsFactors = FALSE)

latLon = spTransform(shpFile, CRS("+proj=longlat +datum=WGS84"))

leaflet(latLon) %>% 
  addTiles() %>% 
  addPolygons()
```


## Point Pattern Analysis

With so much focus on creating clickable maps, we should focus some attention on actual analyses of those maps. One way we can do this is by using *Point Pattern Analysis* (PPA). PPA comes in many different forms, but we are ultimately going to end up testing if our data has Complete Spatial Randomness (CSR) or if it is distributed in some other way (uniform/regular or clustered).  In PPA, we view CSR as being the null hypothesis.

In PPA, we are not looking at the frequency with which a point occurs.  Instead, we are only looking for a point's presence and where it is in space compared to other points.  The figure below contains all of homes marked for destruction, the geographic mean, and an ellipse that captures the standard distance deviation of the houses. The ellipses give us a good idea of how the points are distributed.  If our points were distributed in a different manner, the "deviation ellipses" would give us an idea of shape and direction. 


```{r}
latLonOnly = lapply(1:length(shpFile@polygons), function(x) {
  res = data.frame(x = shpFile@polygons[[x]]@labpt[1], 
                   y = shpFile@polygons[[x]]@labpt[2])
})

latLonOnly = dplyr::bind_rows(latLonOnly)

# If you want to even mess around with getting an ellipse on a leaflet map.

meanCenterX = mean(latLonOnly$x)

meanCenterY = mean(latLonOnly$y)

stDevX = sd(latLonOnly$x)

stDevY = sd(latLonOnly$y)

stDistance = sqrt(sum(((latLonOnly$x - meanCenterX)^2 + 
                         (latLonOnly$y - meanCenterY)^2) / 
                        nrow(latLonOnly)))

segment <- c(0, 360)

angle <- 0 * pi/180

segment <- segment * pi/180

z = seq(segment[1], segment[2], length = 100 + 1)

xx = stDevX * cos(z)

yy = stDevY * sin(z)

alpha = atan2(yy, xx)

alpha = alpha * 180/pi

rad = sqrt(xx^2 + yy^2)

xp = rad * cos(alpha + angle) + meanCenterX

yp = rad * sin(alpha + angle) + meanCenterY

leaflet(latLon) %>% 
  addProviderTiles("CartoDB.Positron") %>% 
  addPolygons() %>% 
  addCircles(lng = meanCenterX, lat = meanCenterY, color = "red") %>% 
  addPolygons(lng = xp, lat = yp, color = "red")

plot(latLon, pch = "+", col = "blue")

points(meanCenterX, meanCenterY, col = "red", pch = 16)

plotrix::draw.ellipse(meanCenterX, meanCenterY, a = stDevX, b = stDevY, border = "red")

```


Recall that we are only looking at the presence of the points in the PPA, not the number of points at a location.  We are only concerned that a house has a point, not that there are any number of points there.  Within the PPA perspective, this makes intuitive sense.  Things are rarely stacked on top of each other in nature (single-family homes, trees, beaver dams), so PPA does not deal with stacked points.

### Intensity

We can also take a look at local intensity by using kernel intensity estimation -- kernel intensity is essentially kernel density, it just has a different name in the spatial context. You will, however, see them frequently interchanged.  Kernel intensity estimation allows us to determine where in our space points are most likely to occur.  In this particular example, we used likelihood cross-validation bandwidth (i.e., radius) selection.  We could have used a number of different *bandwidth* methods (e.g., Diggle, Stoyan), but this method works well with our data; ideally, we would have selected a method based more on theory, but this is an exploratory process.  

In selecting a method, we essentially have a choice between capturing highly-local events very well or viewing larger smoothed areas without the apparent presence of local instances.  Methods like Diggle and Stoyan pick up on all of the points better because they are calculating intensity with a smaller radius; however, these offer very little more than the point pattern plot for the data in our example.  In the current example, we use a leave-one-out intensity calculation; in other words it does not assess the kernel intensity contribution of a point in calculating its own intensity.

```{r}
library(maptools)

library(spatstat)

shpFile = raster::shapefile(x = "C:/Users/sberry5/Documents/teaching/courses/unstructured/data/Active_Demolition_Orders/Active_Demolition_Orders.shp")

window = as.owin(W = SpatialPolygons(shpFile@polygons))

allPPP = ppp(x = latLonOnly$x, y = latLonOnly$y, 
                window = window)

plot(allPPP, pch = "+")

plot(quadratcount(allPPP, nx = 4, ny = 2), add = TRUE, col = "blue")

plot(density.ppp(allPPP, sigma = bw.ppl(allPPP), edge = TRUE))
```


### Ripley's K Function

All of the descriptive stuff for the PPA is great, but we need to know how our point pattern is distributed.  There are several different nearest neighbor analyses of spatial dependence from which we may choose (e.g., F, G), but we are going to use Ripley's K function, $K(r) =  \lambda^{-1}$, for obvious reasons (less obviously, it handles more points and uses different scale lengths for estimation).  The K function is not exactly a nearest neighbor function in a strict sense, but is often grouped with them.    

```{r}
envKTest = envelope(allPPP, Kest, nsim = 100, global = TRUE, verbose = FALSE)

plot(envKTest, main = "K Function Test")
```

Examining the plot tells us several things.  The most obvious is that our data (represented by the black line) is below the CSR line.  This would indicate that our point pattern is more dispersed than what random data might suggest (if it were above the line, it would mean that our data was more clustered than what random data might suggest).  The next bit of information we get from this plot is that our data is within the CSR envelope; this can be interpreted as "non significant"" dispersion.  Finally, we can see that the dispersion increases as our distance measure unit (*r*) increases.  In looking at our point pattern, this is all readily apparent.   

# Item Detection

If faced with finding a needle in a haystack, we can use some pretty simple image detection.

```{r}
library(dplyr)

library(OpenImageR)

jason = readImage("C:/Users/sberry5/Documents/teaching/courses/unstructured/data/f13JV.png")

longImage = reshape2::melt(jason)
  
rgbImage = reshape(longImage, timevar = "Var3",
                      idvar = c("Var1", "Var2"), direction = "wide") # That is stats::reshape()!

rgbImage = rgbImage %>% 
  mutate(Var1 = rev(Var1), 
         var2 = rev(Var2))

plot(rgbImage$Var2, rgbImage$Var1, col = rgb(rgbImage[, 3:5]), 
     asp = 1, pch = ".")

forestOnly = rgbImage[rgbImage$Var2 > 850 | rgbImage$Var2 < 625, ]

forestMean = apply(forestOnly[, 3:5], 2, mean)

forestSD = apply(forestOnly[, 3:5], 2, sd)

colorZ = sweep(rgbImage[, 3:5], 2, forestMean, "-")

colorZ = sweep(colorZ, 2, forestSD, "/")

plot(rgbImage$Var2, rgbImage$Var1, col = rgb(colorZ > 4), 
     asp = 1, pch = ".")
```

You might want to try this with the *boats* image.

# Feature Extraction

We saw a little bit about feature extraction when looking at the data for the letter. Feature extraction is an important point that we will elaborate more on next time.

```{r}
library(magick)

image_read("C:/Users/sberry5/Documents/teaching/courses/unstructured/data/f13JV.png") %>% 
  image_resize("200") %>%
  image_convert(type = "Grayscale", colorspace = "gray") %>% 
  image_write(format = "png", path = "C:/Users/sberry5/Documents/teaching/courses/unstructured/data/jasonSmall.png", 
              density = "300x300", quality = 100)

jasonSmall = readImage("C:/Users/sberry5/Documents/teaching/courses/unstructured/data/jasonSmall.png")

jasonSmall = base::matrix(jasonSmall, nrow = 83, ncol = 200)

init_gb = GaborFeatureExtract$new()

gb_im = init_gb$gabor_feature_extraction(image = jasonSmall, scales = 5, orientations = 8,
                                         downsample_gabor = FALSE, downsample_rows = NULL,
                                          downsample_cols = NULL, gabor_rows = 39, 
                                          gabor_columns = 39, plot_data = TRUE, 
                                          normalize_features = FALSE, threads = 3)

init_gb$plot_gabor(real_matrices = gb_im$gabor_features_real,
                            margin_btw_plots = 0.65, thresholding = TRUE)
```

While this is a bit of a mess, you could try it with the *giraffe* image.


# Image Identifcation

*You Only Look Once* image identification is an increasingly-popular technique because of its speed. Instead of using really deep neural networks to transverse an image, YOLO looks at the entire image, imposes a grid, and starts creating bounding boxes with some probability of being an object.

These are also fast because we are dealing with pretrained models -- *tiny yolo* is an existing model and we can use already established *Visual Object Classes* for the weights.

```{r, eval = FALSE}
library(image.darknet)

yoloVOC = image_darknet_model(type = "classify", 
                                     model = "tiny-yolo-voc.cfg", 
                                     weights = system.file(package="image.darknet", "models", "tiny-yolo-voc.weights"), 
                                     labels = system.file(package="image.darknet", "include", 'darknet', "data", "voc.names"))

jasonClassifcation = image_darknet_classify(file = "C:/Users/sberry5/Documents/teaching/courses/unstructured/data/f13JV.png", 
                          object = yoloVOC, top = 10)

jasonClassifcation

yoloVOCDetect = image_darknet_model(type = "detect", 
                                     model = "tiny-yolo-voc.cfg", 
                                     weights = system.file(package="image.darknet", "models", "tiny-yolo-voc.weights"), 
                                     labels = system.file(package="image.darknet", "include", 'darknet', "data", "voc.names"))

jasonDetect = image_darknet_detect(file = "C:/Users/sberry5/Documents/teaching/courses/unstructured/data/f13JV.png", 
                          object = yoloVOCDetect, threshold = .001)
```


If we were using computer vision to detect this specific thing, we might not fair too well. We can try some different models -- these can be downloaded <a href="https://pjreddie.com/darknet/imagenet/">here</a>.


```{r, eval = FALSE}
yoloVOC = image_darknet_model(type = "classify", 
                              model = "darknet19.cfg", 
                              weights = system.file(package="image.darknet", "models", "darknet19_448.weights"), 
                              labels = system.file(package="image.darknet", "include", 'darknet', "data", "voc.names"))

jasonClassifcation = image_darknet_classify(file = "C:/Users/sberry5/Documents/teaching/courses/unstructured/data/f13JV.png", 
                                            object = yoloVOC, top = 10)

jasonClassifcation

yoloVOCDetect = image_darknet_model(type = "detect", 
                                    model = "darknet19.cfg", 
                                    weights = system.file(package="image.darknet", "models", "darknet19_448.weights"), 
                                    labels = system.file(package="image.darknet", "include", 'darknet', "data", "voc.names"))

jasonDetect = image_darknet_detect(file = "C:/Users/sberry5/Documents/teaching/courses/unstructured/data/f13JV.png", 
                                   object = yoloVOCDetect, threshold = .01)
```

Still not too good, so let's find something easier:

```{r, eval = FALSE}
yoloVOC = image_darknet_model(type = "classify", 
                                     model = "darknet19.cfg", 
                                     weights = system.file(package="image.darknet", "models", "darknet19.weights"), 
                                     labels = system.file(package="image.darknet", "include", 'darknet', "data", "voc.names"))

animalClassifcation = image_darknet_classify(file = "C:/Users/sberry5/Documents/teaching/courses/unstructured/data/giraffe.jpg", 
                          object = yoloVOC, top = 10)

yoloVOCDetect = image_darknet_model(type = "detect", 
                                     model = "darknet.cfg", 
                                     weights = system.file(package="image.darknet", "models", "tiny-yolo-voc.weights"), 
                                     labels = system.file(package="image.darknet", "include", 'darknet', "data", "voc.names"))

animalDetect = image_darknet_detect(file = "C:/Users/sberry5/Documents/teaching/courses/unstructured/data/giraffe.jpg", 
                          object = yoloVOCDetect, threshold = .001)
```