---
title: "Images"
output:
  radix::radix_article:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Geospatial Analysis

## Plotting

By now, you have learned several ways of plotting geospatial data, so I am just going to leave you with the following:

```{r}
library(leaflet)

library(rgdal)

library(sp)

# shpFile = readOGR(dsn = "C:/Users/sberry5/Documents/teaching/courses/unstructured/data/Active_Demolition_Orders/Active_Demolition_Orders.shp", 
                  stringsAsFactors = FALSE)

shpFile = readOGR(dsn = "D:/projects/courses/unstructured/data/Active_Demolition_Orders/Active_Demolition_Orders.shp", 
                  stringsAsFactors = FALSE)

latLon = spTransform(shpFile, CRS("+proj=longlat +datum=WGS84"))

leaflet(latLon) %>% 
  addTiles() %>% 
  addPolygons()
```


## Point Pattern Analysis

With so much focus on creating clickable maps, we should focus some attention on actual analyses of those maps. One way we can do this is by using *Point Pattern Analysis* (PPA). PPA comes in many different forms, but we are ultimately going to end up testing if our data has Complete Spatial Randomness (CSR) or if it is distributed in some other way (uniform/regular or clustered).  In PPA, we view CSR as being the null hypothesis.

In PPA, we are not looking at the frequency with which a point occurs.  Instead, we are only looking for a point's presence and where it is in space compared to other points.  The figure below contains all of homes marked for destruction, the geographic mean, and an ellipse that captures the standard distance deviation of the houses. The ellipses give us a good idea of how the points are distributed.  If our points were distributed in a different manner, the "deviation ellipses" would give us an idea of shape and direction. 


```{r}
latLonOnly = lapply(1:length(shpFile@polygons), function(x) {
  res = data.frame(x = shpFile@polygons[[x]]@labpt[1], 
                   y = shpFile@polygons[[x]]@labpt[2])
})

latLonOnly = dplyr::bind_rows(latLonOnly)

meanCenterX = mean(latLonOnly$x)

meanCenterY = mean(latLonOnly$y)

stDevX = sd(latLonOnly$x)

stDevY = sd(latLonOnly$y)

stDistance = sqrt(sum(((latLonOnly$x - meanCenterX)^2 + 
                         (latLonOnly$y - meanCenterY)^2) / 
                        nrow(latLonOnly)))

segment <- c(0, 360)

angle <- 0 * pi/180

segment <- segment * pi/180

z = seq(segment[1], segment[2], length = 100 + 1)

xx = stDevX * cos(z)

yy = stDevY * sin(z)

alpha = atan2(yy, xx)

alpha = alpha * 180/pi

rad = sqrt(xx^2 + yy^2)

xp = rad * cos(alpha + angle) + meanCenterX

yp = rad * sin(alpha + angle) + meanCenterY

leaflet(latLon) %>% 
  addProviderTiles("CartoDB.Positron") %>% 
  addPolygons() %>% 
  addCircles(lng = meanCenterX, lat = meanCenterY, color = "red") %>% 
  addPolygons(lng = xp, lat = yp, color = "red")

plot(latLon, pch = "+", col = "blue")

points(meanCenterX, meanCenterY, col = "red", pch = 16)

plotrix::draw.ellipse(meanCenterX, meanCenterY, a = stDevX, b = stDevY, border = "red")

```


You should notice that the mean locations are different from the mean locations in the interactive map.  Recall that we are only looking at the presence of the points in the PPA, not the number of points at a location.  We are only concerned that Harvard has a point, not that there are 8 points there.  Within the PPA perspective, this makes intuitive sense.  Things are rarely stacked on top of each other in nature (single-family homes, trees, beaver dams), so PPA does not deal with stacked points.

### Intensity

We can also take a look at local intensity by using kernel intensity estimation -- kernel intensity is essentially kernel density, it just has a different name in the spatial context. You will, however, see them frequently interchanged.  Kernel intensity estimation allows us to determine where in our space points are most likely to occur.  In this particular example, we used likelihood cross-validation bandwidth (i.e., radius) selection.  We could have used a number of different *bandwidth* methods (e.g., Diggle, Stoyan), but this method works well with our data; ideally, we would have selected a method based more on theory, but this is an exploratory process.  

In selecting a method, we essentially have a choice between capturing highly-local events very well or viewing larger smoothed areas without the apparent presence of local instances.  Methods like Diggle and Stoyan pick up on all of the universities better because they are calculating intensity with a smaller radius; however, these offer very little more than the point pattern plot for the data in our example.  In the current example, we use a leave-one-out intensity calculation; in other words it does not assess the kernel intensity contribution of a point in calculating its own intensity.

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center'}
library(maptools)

library(spatstat)

shpFile = raster::shapefile(x = "C:/Users/sberry5/Documents/teaching/courses/unstructured/data/Active_Demolition_Orders/Active_Demolition_Orders.shp")

shpFile = raster::shapefile(x = "D:/projects/courses/unstructured/data/Active_Demolition_Orders/Active_Demolition_Orders.shp")

window = as.owin(W = shpFile)

allPPP = ppp(x = latLonOnly$x, y = latLonOnly$y, 
                window = window)

plot(allPPP, pch = "+")

plot(quadratcount(allPPP, nx = 4, ny = 2), add = TRUE, col = "blue")

plot(density.ppp(allPPP, sigma = bw.ppl(allPPP), edge = TRUE))
```


### Ripley's K Function

All of the descriptive stuff for the PPA is great, but we need to know how our point pattern is distributed.  There are several different nearest neighbor analyses of spatial dependence from which we may choose (e.g., F, G), but we are going to use Ripley's K function, $K(r) =  \lambda^{-1}$, for obvious reasons (less obviously, it handles more points and uses different scale lengths for estimation).  The K function is not exactly a nearest neighbor function in a strict sense, but is often grouped with them.    

```{r, echo=FALSE, message=FALSE, warning=FALSE, results="hide", fig.align='center'}
envKTest = envelope(allPPP, Kest, nsim = 150, global = TRUE, verbose = FALSE)

plot(enKLTest, main = "K Function Test")
```

Examining the plot tells us several things.  The most obvious is that our data (represented by the black line) is below the CSR line.  This would indicate that our point pattern is more dispersed than what random data might suggest (if it were above the line, it would mean that our data was more clustered than what random data might suggest).  The next bit of information we get from this plot is that our data is within the CSR envelope; this can be interpreted as "non significant"" dispersion.  Finally, we can see that the dispersion increases as our distance measure unit (*r*) increases.  In looking at our point pattern, this is all readily apparent.   

# Item Detection

```{r}
library(OpenImageR)

# jason = readImage("C:/Users/sberry5/Documents/teaching/courses/unstructured/data/f13JV.png")

jason = readImage("D:/projects/courses/unstructured/data/f13JV.png")

longImage = reshape2::melt(jason)
  
rgbImage = reshape(longImage, timevar = "Var3",
                      idvar = c("Var1", "Var2"), direction = "wide")

rgbImage = rgbImage %>% 
  mutate(Var1 = rev(Var1), 
         var2 = rev(Var2))

plot(rgbImage$Var2, rgbImage$Var1, col = rgb(rgbImage[, 3:5]), 
     asp = 1, pch = ".")

forestOnly = rgbImage[rgbImage$Var2 > 850 | rgbImage$Var2 < 625, ]

forestMean = apply(forestOnly[, 3:5], 2, mean)

forestSD = apply(forestOnly[, 3:5], 2, sd)

colorZ = sweep(rgbImage[, 3:5], 2, forestMean, "-")

colorZ = sweep(colorZ, 2, forestSD, "/")

plot(rgbImage$Var2, rgbImage$Var1, col = rgb(colorZ > 4), 
     asp = 1, pch = ".")
```

# Feature Extraction

```{r}
library(magick)

image_read("D:/projects/courses/unstructured/data/f13JV.png") %>% 
  image_resize("200") %>%
  image_convert(type = "Grayscale", colorspace = "gray") %>% 
  image_write(format = "png", path = "D:/projects/courses/unstructured/data/jasonSmall.png", 
              density = "300x300", quality = 100)

jasonSmall = readImage("D:/projects/courses/unstructured/data/jasonSmall.png")

jasonSmall = base::matrix(jasonSmall, nrow = 83, ncol = 200)

init_gb = GaborFeatureExtract$new()

gb_im = init_gb$gabor_feature_extraction(image = jasonSmall, scales = 5, orientations = 8,
                                         downsample_gabor = FALSE, downsample_rows = NULL,
                                          downsample_cols = NULL, gabor_rows = 39, 
                                          gabor_columns = 39, plot_data = TRUE, 
                                          normalize_features = FALSE, threads = 3)

init_gb$plot_gabor(real_matrices = gb_im$gabor_features_real,
                            margin_btw_plots = 0.65, thresholding = TRUE)
```



https://appsilon.com/ship-recognition-in-satellite-imagery-part-i/

```{r}
devtools::install_github("bnosac/image", subdir = "image.darknet", build_vignettes = TRUE)
```


```{r}
library(image.darknet)

yolo_voc <- image_darknet_model(type = "classify", 
                                     model = "darknet.cfg", 
                                     weights = system.file(package="image.darknet", "models", "tiny-yolo-voc.weights"), 
                                     labels = system.file(package="image.darknet", "include", 'darknet', "data", "voc.names"))

x <- image_darknet_classify(file = "C:/Users/sberry5/Downloads/test.jpeg", 
                          object = yolo_voc)
```
