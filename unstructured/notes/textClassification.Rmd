---
title: "Text Analysis"
description: |
  Text Classification
output:
  distill::distill_article:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```


# Naive Bayes

With any exercise in statistical learning, the *Naive Bayesian* model is a great place to start. Why? Mostly because it tends to perform pretty well without too much hassle (i.e., tuning and various parameter tweaking). This reasonable performance even comes with some pretty central assumptions violated -- the naive part of the method comes from an assumption that observations are always completely and totally independent with regard to outcome variable. Another handy feature of the Naive Bayes is that it can handle missing observations without any issue. It will also work on smaller training set or data with higher correlations among variables. Perhaps most importantly, it is blazing fast compared to more complex methods with often similar performance. 

With all of this together, it becomes a really solid baseline for problems -- if another technique cannot beat a Naive Bayes on a problem, then it is probably not going to be worth using for that problem. The Naive Bayes has been used with great effect from everything to spam detection to determining the probability of a specific classes of customers cancelling a service (e.g., what is the probability that a 50 year-old, with a limited data plan, using a flip phone, would cancel his cell service). 

With "Bayesian" in the name, you would probably guess that we are going to be dealing with probabilities and we certainly are. Not that you need it, but let's do a really quick refresher on probability, odds, and likelihoods.

*Probability* is bound between 0 and 1, and indicates the chance of an event occuring. 

*Odds* are scaled from 0 to $\infty$ and are the ratio of the probability of a particular event occurring to the probability of it not occurring. With a probability of occurrance at .2, we would have an odds of 1 to 4 of the event occurring.

*Likelihood* is the ratio of two related conditional probabilites and can be expressed in two different forms:

- The probability of outcome *A*, given *B*, and the probability of *A*, given not *B* (*A* is conditional on *B*)

- The odds of *A*, given *B*, and the overall odds of *A*

Conversion between probability and odds is as follows:

$$odds = -1 +  \frac{1}{1 - probability}$$
$$probability = 1 - \frac{1}{1 + odds}$$

If we know that we have a probability of .7, we find an odds of:

```{r}
-1 + (1 / (1 - .7))
```

If we have an odds of 1.75, we can find a probability of:

```{r}
1 - (1 / (1 + 1.75))
```

With this information in hand, we can then compute the independent conditional probability distribution (conditioned on the outcome variable) for each and every predictor variable. From there, we are taking the product of those conditional probabilities.

Let's see how it works with some text:

```{r}
data.frame(songID = 1:4, 
           huntin = c(1, 1, 1, 1), 
           fishing = c(0, 0, 1, 0), 
           lovin = c(1, 0, 0, 0),
           day = c(0, 1, 1, 1),
           country = c(1, 0, 0, 0), 
           boy = c(1, 0, 0, 0), 
           street = c(0, 0, 0, 1), 
           city = c(0, 1, 0, 0),
           rapCountry = c(1, 1, 0, 0)
)
```

We can find the probability for any given thing within this data. For example, there is a probability of .75 that "day" will occur -- P(day) = $3/4$ = .75. The probability that a song is rap is .5 -- P(rap) = $2/4$ = .5.  


We could just compute the conditional probability for each song belonging to one of the two classes, but this becomes increasingly intensive as we add more variables. Instead, we can use the following equation to make things easier:

$$P(C_k|x_1,x_2,..., x_n) = \frac{P(C_k)P(x_1|C_k)P(x_2|C_k)...P(X_n|C_k)}{P(x_1,x_2,...,x_n)}$$
If we expand our data a bit:

```{r, echo = FALSE}
data.frame(class = rep(c("country", "rap"), each = 4), 
           word = rep(c("country", "boy", "street", "city"), times = 2), 
           Yes = c(10, 4, 10, 8, 15, 2, 25, 5), 
           No = c(10, 16, 10, 12, 65, 78, 55, 75), 
           grandTotal = c(20, "", "", "", 80, "", "", ""), stringsAsFactors = FALSE)
```

Given what we know about the totals and probabilities, we could look at new lyrics and figure out the probability that it is either rap or country.

> I'm a country boy, won't see me in the city.

We can see that we don't see "street" in this lyric, so we will need to account for that with our "No" column.

This means that we have the following problem to solve:

$$\frac{P(country)P(country|country)P(boy|country)P(\neg street|country)P(city|country)}{P(country, boy,\neg street, city)}$$

We can compute the likelihood of this new lyric belonging to a country song by:

$$(20⁄100)*(10⁄20)*(16⁄20)*(10⁄20)*(8⁄20)$$

```{r}
countryLikelihood <- (20⁄100)*(10⁄20)*(16⁄20)*(10⁄20)*(8⁄20)

countryLikelihood
```

And the likelihood it is rap:

$$(80⁄100)*(15⁄80)*(78⁄80)*(25⁄80)*(5⁄80)$$

```{r}
rapLikelihood <- (80⁄100)*(15⁄80)*(78⁄80)*(25⁄80)*(5⁄80)

rapLikelihood
```

This gives us the probability of the song being country of:

```{r}
countryLikelihood / (countryLikelihood + rapLikelihood)
```


## A Quick And Dirty Example

We will step away from text just for a little bit to see a good example of how Naive Bayes can be put to great use. The `rsample` package is great for creating data for us in cross-validation, but it also has a data set called "attrition" (created by IBM for Watson training). Attrition contains a lot of demographic-flavored variables and a variable called...Attrition! Let's see if we can use all of the features within the data to predict attrition. 

```{r}
library(caret)

library(dplyr)

library(klaR)

library(rsample)

# Looking at the data, there are some variables needing conversion
# to factors.

attrition = attrition %>%
  mutate_at(c("JobLevel", "StockOptionLevel", "TrainingTimesLastYear"), factor)

# We could use base R or caret to perform our splitting, but we can 
# keep rolling with rsample (praise be to Hadley and Max).

set.seed(1001)

split = initial_split(attrition, prop = .6, strata = "Attrition")

attritionTrain = training(split)

attritionTest  = testing(split)

y = attritionTrain$Attrition

x = dplyr::select(attritionTrain, -Attrition)

# set up 10-fold cross validation procedure

nbTrainControl = trainControl(method = "cv", number = 10, verboseIter = FALSE)

nbAttrition = train(x = x, y = y,
  method = "nb", trControl = nbTrainControl, metric = "Accuracy")

nbAttrition

# results
confusionMatrix(nbAttrition)
```

<aside>
We have taken our normal train/test cross-validation and bumped it up to a *k*-fold cross-validation. It will create *k* paritions within the data, use one as a validation set and the remaining training sets. This is repeated for every fold, so that all folds (and thus all observations) are used for validation once, and the results are averaged. It tends to offer a less biased model than basic train/test CV.
</aside>

Out of the box, that is not too bad:

```{r}
# Incorporating a Laplacian smooth for 0 value probability cells:

searchGrid = expand.grid(fL = 0:5, usekernel = FALSE, adjust = 1)

nbAttritionSmoothed = train(x = x, y = y,
  method = "nb", trControl = nbTrainControl,
  tuneGrid = searchGrid, preProc = c("center", "scale"))

nbAttritionSmoothed

confusionMatrix(nbAttritionSmoothed)
```

By adding a smoothing parameter alone, we actually did worse on average. Let's try adding a non-parametric kernel to tweak our continuous variables (in theory they should be normally distributed, so the kernel will help to take care of any non-normal variables).

```{r}
library(doParallel)

cl <- makePSOCKcluster(parallel::detectCores() - 1)

registerDoParallel(cl)

searchGrid = expand.grid(usekernel = c(TRUE, FALSE), 
                           fL = 0:5, adjust = seq(0, 5, by = 1))

nbAttritionTuned = train(x = x, y = y,
  method = "nb", trControl = nbTrainControl,
  tuneGrid = searchGrid, preProc = c("center", "scale"))

nbAttritionTuned

confusionMatrix(nbAttritionTuned)

stopCluster(cl)
```

Now we are getting somewhere. Let's add one more feature to our model to reduce down our feature space:

```{r}
searchGrid = expand.grid(usekernel = TRUE, 
                         fL = 5, adjust = 5)

nbAttritionTunedPCA = train(x = x, y = y,
  method = "nb", trControl = nbTrainControl,
  tuneGrid = searchGrid, preProc = c("center", "scale", "pca"))

nbAttritionTunedPCA

confusionMatrix(nbAttritionTunedPCA)
```


Let's see how our best run does with our test data:

```{r}
pred = predict(nbAttritionTuned, newdata = attritionTest)

confusionMatrix(pred, attritionTest$Attrition, positive = "Yes")
```

A few key pieces of information for us beyond the confusion matrix:

We should hope to find that our Accuracy rate is significantly higher than our *No Information Rate*.

*Kappa* is an agreement statistic. Here it is the agreement between our observed accuracy rate and an expected accuracy rate. Anything over .4 is deemed by most sources as adequate, but it gets better as we approach 1.

*Mcnemar's test* is looking at the marginal values of the confusion matrix to see if they are significantly different. 

*Sensitivity* is the true positive rate.  We did not do too well. 

*Specificity* is the true negative rate. We did a fine job here.

Look at the help file for `confusionMatrix` for the calculations of everything.

# Artificial Neural Networks

Artificial Neural Networks (ANN) are a major part of the artificial intelligence toolkit and for many good reasons.

## Necessary Elements

There are 4 major necessary elements needed for an ANN:

1.  The inputs need to be well understood.

2.  The output is well understood.

3.  Experience is available.

4.  It is a black box.

<aside>
The "articifical" is usually dropped, but it does help to distinguish it from the a biological perspective.
</aside>


## The Basics 

The set-up is the same as our typically classification problem: we have predictors (inputs) and an outcome (output). The difference, though, is in what happens between the input and the output. In an ANN, there are any number of hidden layers that help to transform the input to the output. 

![](nnImage.png)

This is what is known as a multilayer perceptron (MLP).

In this MLP, an input vector will travel to the first hidden layer (i.e., the neuron), in which some calculation will be performed on that input -- this is known as the *activation function*. The activation function is split into two parts: a *combination function* and a *transfer function*. The combination function will combine the inputs into a single weighted input and the transfer function will transform the weighted values before outputting the variable into the next node. Each node is also going to receive a *bias* weight -- this is a constant weight applied to the all units in the layer, much in the way of a regression intercept beta. This process will continue until the values reach the output layer.

It is worth paying some attention to the transfer function, as it can take many different forms. Some more common forms include step, linear, logistic, and hyperbolic functions. In any of these function, weighting is going to occur -- with the weighting, we should be sure to standardize our values or the largest values will dominate for many runs of the model. This notion of weighting goes hand in hand with the number of hidden layers. If our hidden layer becomes too wide, we will run the risk of overfitting the model (it will essentially learn the exact patterns found within the training data). In many cases, a single hidden layer with a hyperbolic transfer function can be enough to get reasonable results.

ANNs have another interesting feature in that they learn from their mistakes (and they indeed know that they have made mistakes). When we reach the output from our first iteration, the model will examine the errors. our ANN does not really want errors beyond a certain magnitude, so it will take those errors and run them back through the layers to try to re-tune them; this is a process called backpropagation (the backward propagation of errors). It does this by adjusting the weights applied throughout the nodes. As our errors are backpropogated, the ANN will change a weight and see whether it increases or reduces the error -- it will seek to reduce the error, but not to eliminate the error (this would lead to overfitting!).

This is a point, along with the previous point about the number of layers, is one that bears repeating. We want our ANN to be flexible to predicting new data; we do not want our ANN to learn everything about the training data. If your model underperforms on the test set, then you likely have overfit the ANN with too many hidden layers. 

There are several different types of neural networks.

## Basics: In Action

The most basic of all neural nets can be done with `nnet`. 

```{r}
numFolds = trainControl(method = 'cv', number = 10, classProbs = TRUE, 
                        verboseIter = FALSE, summaryFunction = twoClassSummary)

attritionFit = train(Attrition ~ ., data = attritionTrain, method = 'nnet', 
              trControl = numFolds, metric = "Accuracy", 
              preProc = c("center", "scale"), trace = FALSE)

attritionFit

results1 = predict(attritionFit, newdata = attritionTest)

confusionMatrix(results1, attritionTest$Attrition)
```


## On To Lyrics

```{r, echo = TRUE}
load("C:/Users/sberry5/Documents/teaching/courses/unstructured/data/textFeatures.RData")
```

With our data loaded in, we need to determine the mix in proportions:

```{r}
summary(as.factor(textFeatures$genre))
```

We certainly have some class imbalance, but hopefully the flexibility of the ANN will shine through.

With the class split in mind, we need to develop a training set that will represent the proprotion of country and zappa. We would find that the zappa class represents `r 447 / 2363`% of the total data. If we want to train our model on 70% of the data, we can just take 80% from each class for training.

```{r}
zappaRows = which(textFeatures$genre == "zappa")

countryRows = which(textFeatures$genre == "country")

genre = ifelse(textFeatures$genre == "zappa", 1, 0)

genre = as.factor(make.names(genre))

minMaxScale = function(x){
  (x - min(x)) / (max(x) - min(x))
}

scaledVars = textFeatures %>% 
  dplyr::select(-genre) %>% 
  mutate_all(., funs(minMaxScale(.)))

scaledData = cbind(genre, scaledVars)

set.seed(1001)

trainRows = c(sample(zappaRows, floor(length(zappaRows) * .7), replace = FALSE), 
              sample(countryRows, floor(length(countryRows) * .7), replace = FALSE))

testingData = scaledData[-trainRows, ]

trainingData = scaledData[trainRows, ]
```

Now, we can train our model:

```{r}
numFolds = trainControl(method = 'cv', number = 10, classProbs = TRUE, 
                        verboseIter = FALSE, summaryFunction = twoClassSummary)

fit2 = train(genre ~ ., data = trainingData, method = 'nnet', 
              trControl = numFolds, trace = FALSE)

# fit2 = train(genre ~ ., data = trainingData, method = 'mlpML',
#               trControl = numFolds)

# fit2 = train(genre ~ ., data = trainingData, method = 'mlpKerasDropout',
#               trControl = numFolds)

results1 = predict(fit2, newdata = trainingData)

confusionMatrix(results1, trainingData$genre)
```

This is a very basic neural network and much has occurred in the NN space. 

For the sake of it, let's see how a Naive Bayes would have done...

```{r}
y = trainingData$genre

x = dplyr::select(trainingData, -genre)

searchGrid = expand.grid(usekernel = TRUE, 
                           fL = 1:2, adjust = seq(1, 3, by = 1))

nbTrainControl = trainControl(method = "cv", number = 10, verboseIter = FALSE)

nbAttritionTunedPCA = train(x = x, y = y,
  method = "nb", trControl = nbTrainControl,
  tuneGrid = searchGrid, preProc = c("center", "scale"))

nbAttritionTunedPCA

# results
confusionMatrix(nbAttritionTunedPCA)
```

Not so good!