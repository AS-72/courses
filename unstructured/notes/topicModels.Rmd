---
title: "Text Analysis"
description: |
  Topic Models
output:
  radix::radix_article:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Package Install

```{r, eval = FALSE}
install.packages(c("lsa", "NMF", "stm"))
```


# Topic Models

We already know that text is everywhere and our sentiment analysis was a reasonable crack at extracting some meaning from text. A common line of inquiry relates to what is expressed within the text. In traditionally social sciences, this was done through some form of content analysis -- a coding scheme was developed by researchers, people read each comment and assigned it a code, some agreement statistics were computed, and any discrepencies were hashed out by the researchers. When this was the only option, it was servicable. 

The amount information available to us now does not work for those traditional methods. Could you do this for thousands of tweets? With time, maybe. Could you do this with a few million articles? No. This is where topic models come to save the day. Our topic models are going to give us a pretty good idea about what texts are similar to other texts.

For the sake of exploration, it is great to know what topics are *latent* within a body of texts. It also has broader uses. When you search for something, topic models will return a set of documents that are likely to be related to that search. 

## Latent Semantic Analysis

LSA is a building-block technique in the topic modeling field. It starts by building a *term-document matrix* (TDM). LSA, though, takes this TDM and replaces the raw counts with the tf-idf score. To this point, we are not doing anything that we already have not seen. 

For our dive into LSA, we are going to rely on the `tm` package for our data prep. We will use it for its flexibility in data types -- it can handle a corpus of .txt files in a directory, a single vector, or a data frame. 

<aside>
For clarity and immediate exploration, we will put them into a mutate line. We could easily do it tm_map(), but things become wonky as soon as we produce a corpus.
</aside>

```{r}
library(dplyr)

library(stringr)

library(tm)

library(textstem)

load("C:/Users/sberry5/Documents/teaching/courses/unstructured/data/allLyricsDF.RData")

# load("D:/projects/courses/unstructured/data/allLyricsDF.RData")

hardRemove = c(34, 78, 91, 94, 107, 136, 150, 202, 210, 213, 222, 229, 232, 
               239, 245, 248, 268, 276, 277, 284, 285, 291, 309, 310, 356, 
               403, 419, 424, 425, 460, 467, 482, 490, 497, 519, 532, 545, 
               555, 585, 586, 616, 639, 642, 648, 653, 654, 658, 682, 704, 
               705, 719, 722, 760, 780, 794, 795, 802, 832, 836, 851, 867, 
               872, 878, 891, 921, 924, 931, 944, 972, 985, 995, 1026, 1029, 
               1032, 1041, 1042, 1063, 1101, 1102, 1111, 1115, 1121, 1124,
               1126, 1128, 1144, 1145, 1158, 1166, 1172, 1175, 1186, 1202, 
               1204, 1209, 1211, 1215, 1218, 1225, 1240, 1251, 1256, 1257, 
               1272, 1287, 1311, 1316, 1326, 1327, 1342, 1361, 1421, 1462, 
               1493, 1534, 1584, 1609, 1627, 1629, 1635, 1638, 1649, 1667, 
               1676, 1679, 1690, 1696, 1707, 1710, 1717, 1720, 1755, 1758, 
               1760, 1780, 1786, 1788, 1798, 1805, 1847, 1855, 1858, 1861, 
               1866, 1882, 1895, 1918, 1929, 1946, 1952, 1994, 2000, 2011, 
               2017, 2018, 2019, 2027, 2032, 2039, 2055, 2069, 2143, 2164, 
               2193, 2200, 2201, 2204, 2246, 2254, 2285, 2303, 2318, 2334,
               2347, 2373, 2383, 2395, 2397, 2401, 2402, 2406, 2434, 2443,
               2459, 2469, 2472, 2475, 2499, 2507, 2527, 2534, 2540, 2563,
               2564, 2566, 2570, 2572, 2597, 2604, 2632, 2638, 2655, 2661,
               2666, 2685, 2696, 2719, 2721, 2724, 2729, 2736, 2740, 2754,
               2764, 2767, 2768, 2771, 2803, 2843, 2877, 2903, 2905, 2913, 
               2944, 2951, 2969, 2972, 2978, 2995, 3016, 3041, 3043, 3048, 3092)

cleanLyrics = allLyricsDF[-c(hardRemove)] %>%
  filter(warningIndicator == 0) %>% 
  dplyr::select(lyrics, returnedArtistName, returnedSong) %>%
  mutate(text = as.character(lyrics), 
         text = str_replace_all(text, "\n", " "),   
         text = str_replace_all(text, "(\\[.*?\\])", ""),
         text = str_squish(text), 
         text = gsub("([a-z])([A-Z])", "\\1 \\2", text), 
         text = tolower(text), 
         text = removeWords(text, c("â€™", stopwords(kind = "en"))), 
         text = removePunctuation(text), 
         text = removeNumbers(text),
         text = lemmatize_strings(text), 
         doc_id = returnedSong, 
         author = returnedArtistName) %>% 
  select(doc_id, text, author)

lyricsCorpus = Corpus(DataframeSource(cleanLyrics))

lyricsCorpus[[1]][[2]]

meta(lyricsCorpus[1])
```

After creating our corpus, we can then pass our corpus into a term-document matrix:

```{r}
lyricsTDM = TermDocumentMatrix(lyricsCorpus, control = list(weighting =
                                                              function(x)
                                                                weightTfIdf(x, normalize =
                                                                              FALSE)))

inspect(lyricsTDM)
```

<aside>
To illustrate tm's flexibility, we could have done all of our cleaning in the control list.
</aside>

The `lyricsTDM` object that we created can be easily explored like we would any other R object, so feel free to poke around in there.

If we just look at the information, though, we can see that we are dealing in very sparse space (99% sparse). This means that most words are not in most documents -- sparsity is incredibly common in data. To solve the problem of sparsity, we need to perform some dimension reduction on our matrix (you will often here this referred to as *rank reduction*). Our dimension reduction, in this case, can work well because we are really trying to find words that belong to the same topic; for example, "car" and "truck" would reduce down to the same topic space.

How does this dimension reduction happen in LSA? It happens with singular value decomposition (SVD). SVD is but one of many forms of matrix factorization (we will see another soon) that will take a matrix (like the one we have) and break it down into 3 component matrices: $M=U*S*V$. The *S* is where SVD gets its name because it is a diagonal matrix of the singular values of *M* -- these are scaling values. If we take the dot product (matrix multiplication) of these 3 matrices, we can get the original matrix back. While getting the original matrix back is good, we are want to reduce our dimensionality by specifying the number of topics that we select. When we decide upon a topic number, we are only keeping that number of columns within the *S* matrix. In effect, we are cutting out the lowest weights within the *S* matrix and the ensuing multiplication will provide a best fit matrix.

For a great run down of SVD (and the steps that come before), here is a great <a href="https://davetang.org/file/Singular_Value_Decomposition_Tutorial.pdf">demonstration</a>.

```{r}
aMatrix <- matrix(c(3, 2, 2, 2, 3, -2), ncol = 3, nrow = 2)

aMatrix
```

```{r}
aaT <- aMatrix %*% t(aMatrix)
```


<aside>
Diving into SVD isn't beyond us, we just need to keep moving along. It is definitely worth looking at a little more if you ever need to perform rank reduction on a matrix.
</aside>

Any package that will do LSA will perform the SVD step for you, so you won't need to do it manually -- just know that it is being done!

Knowing the appropriate number of topics is impossible *a priori*, so we need to use various function to do that work for us. The lsa package has `dimcalc_share` (among others) that will work for finding the number of dimensions from our SVD.

**Warning!** This will take some time to run! Matrix multipliation on large matrices is always going to be computationally intensive!

```{r}
library(lsa)

lyricsRawMatrix = as.matrix(lyricsTDM)

s = svd(lyricsRawMatrix)$d # D is the diagonal matrix for our weights.

topicCount = dimcalc_share()(s)
```


That feels like a lot of topics, but let's go with it. Again, this will take some time to run!

```{r}
lyricsLSA = lsa(lyricsTDM, topicCount)

lyricsSpace = as.textmatrix(lyricsLSA)

lyricDistance = dist(t(lyricsSpace))
```

Now we can do some exploration, like which words are associated with the word "dance"?

```{r}
associate(lyricsSpace, "dance")
```

We can lower our threshold to get more terms:

```{r}
associate(lyricsSpace, "dance", threshold = .3)
```

## Non-negative Matrix Factorization

Latent semantic analysis works and it is a great first step towards topic models; however, it is not really anything approaching state of the art. At this point there are other techniques that are far superior and one of those techniques is non-negative matrix factorization (NNMF or NMF). Some of the basic set-up remains the same: we have a sparse matrix and we need to reduce the dimensionality of our matrix by somehow breaking the original matrix down into different components. 

NNMF works as follows:

- We propose that we can multiple two component matrices to reproduce our original matrix. All of these matrices contain only 0 or positive values.

- We will start with our original term-document matrix (*O* -- typically denoted as *V*). We know that is 8807 rows (words) by 2385 columns (songs). 

- We can then choose a number of topics (*k* = 15). This will give us a features matrix (*F* -- typically denoted as *W*) that is 8807 rows by 15 columns. It will also give us a coefficient matrix (*C* -- typically denoted as *H*) that is 15 rows by 2385 columns.

- Before splitting *O* into component parts, each column is clustered so that *C* represents cluster membership and *F* represents the centroids.

- When taking $FC$, we get a matrix that is 8807 by 2385 (our original dimensions). 

Since our data is already prepped, we can just pass our TDM right into the `nmf` function...in theory.

In reality, though, we need to add a small constant to our matrix. Why? The *k*-means that is being implemented on the backend of `nmf` will have some issues with all of those 0's; in other words, it will handle a smaller sparse matrix, not a bigger sparse matrix. It is not big deal to add this small constant.

```{r}
library(tibble)

lyricConvert = as.data.frame(as.matrix(lyricsTDM))

lyricsTibble = as_tibble(lyricConvert, .name_repair = "universal")

lyricsTibble = lyricsTibble %>% 
  mutate_all(., funs(. + .1))

rownames(lyricsTibble) = rownames(lyricConvert)
```

**Warning!** This takes a really long time to run! The clustering that is happening within the matrices is a pain -- if you have ever messed around with cluster selection, this run time will be familiar. Run it with smaller numbers of *k* to have it finish in decent time; the saved model object is in the data folder. 

```{r, eval = FALSE}
library(NMF)

lyricsNNMF = nmf(lyricsTibble, 5, seed = 1001)

# save(lyricsNNMF, file = "D:/projects/courses/unstructured/data/nnmfOut.RData")

save(lyricsNNMF, file = "C:/Users/sberry5/Documents/teaching/courses/unstructured/data/nnmfOut.RData")
```

After waiting, we can finally start to see what words belong to what topics.

```{r}
# load("D:/projects/courses/unstructured/data/nnmfOut.RData")

load("C:/Users/sberry5/Documents/teaching/courses/unstructured/data/nnmfOut.RData")

library(NMF)

wMatrix = as.data.frame(basis(lyricsNNMF))

head(wMatrix[order(-wMatrix$V1), ], 25)

head(wMatrix[order(-wMatrix$V2), ], 25)

head(wMatrix[order(-wMatrix$V3), ], 25)

head(wMatrix[order(-wMatrix$V4), ], 25)

head(wMatrix[order(-wMatrix$V5), ], 25)
```

We can find all of the words that fit within a topic (the *features*) based upon a pre-defined threshold.

```{r}
lyricFeatures = extractFeatures(lyricsNNMF)

rownames(wMatrix[lyricFeatures[[1]], ])
```

## Latent Dirichlet Allocation

LSA is the godfather of topic models and non-negative matrix factorization is a useful tool that we will see again (it is great for working with images and has great extensions to factor analysis). For topic models, though, the current state of the art is some variation of latent dirichlet allocation (LDA).

Let's start with a brief demonstration of a standard latent dirichlet allocation (LDA) for topic modeling.

Suffice it to say, one can approach this in (at least) one of two ways. In one sense, LDA is a dimension reduction technique, much like the family of techniques that includes PCA, factor analysis, non-negative matrix factorization, etc. We will take a whole lot of terms, loosely defined, and boil them down to a few topics. In this sense, LDA is akin to discrete PCA. Another way to think about this is more from the perspective of factor analysis, where we are keenly interested in interpretation of the result, and want to know both what terms are associated with which topics, and what documents are more likely to present which topics. The following is the plate diagram and description for standard LDA from Blei, Jordan, and Ng (2003).

![](bleiPlate.png)

- $\alpha$ is the parameter of the Dirichlet prior on the per-document topic distributions
- $\eta$ is the parameter of the Dirichlet prior on the per-topic word distribution
- $\theta_m$ is the topic distribution for document *m*
- $\beta_k$ is the word distribution for topic *k*
- $z_{mn}$ is the topic for the n-th word in document *m*
- $w_{mn}$ is the specific word

Both *z* and *w* are from a multinomial draw based on the $\theta$ and $\beta$ distributions respectively. The key idea is that, to produce a given document, one draws a topic, and given the topic, words are drawn from it.

## Topic Probabilities

We will start by creating topic probabilities. There will be *k* = 3 topics. Half of our documents will have probabilities of topics for them ($\theta_1$) which will be notably different from the other half ($\theta_2$). Specifically, the first half will show higher probability of topic "A" and "B", while the second half of documents show higher probability of topic "C". What weâ€™ll end up with here is an $m X k$ matrix of probabilities $\theta$ where each *m* document has a non-zero probability for each *k* topic. Note that in the plate diagram, these would come from a Dirichlet($\alpha$) draw rather than be fixed like we are doing, but hopefully this will make things clear starting out.


```{r}
library(tidyverse)

nDocs = 500                                       # Number of documents

wordsPerDoc = rpois(nDocs, 100)                   # Total words/terms in a document

thetaList = list(c(A = .60, B = .25, C = .15),    # Topic proportions for first and second half of data 
                 c(A = .10, B = .10, C = .80))    # These values represent a Dir(alpha) draw

theta_1 = t(replicate(nDocs / 2, thetaList[[1]]))

theta_2 = t(replicate(nDocs / 2, thetaList[[2]]))

theta = rbind(theta_1, theta_2)      
```


## Topic Assignments and Labels

With topic probabilities in hand, weâ€™ll draw topic assignments from a categorical distribution.

```{r}
firsthalf = 1:(nDocs / 2)
secondhalf = (nDocs / 2 + 1):nDocs

Z = apply(theta, 1, rmultinom, n = 1, size = 1)   # draw topic assignment

rowMeans(Z[, firsthalf])                           # roughly equal to theta_1
```

Now, we can see which is the most likely topic.

```{r}
z = apply(Z, 2, which.max) 
```

We have our list of documents and each document's topic assignment. 


## Topics

Next we need the topics themselves. Topics are probability distributions of terms, and in what follows weâ€™ll use the Dirichlet distribution to provide the prior probabilities for the terms. With topic A, weâ€™ll make the first ~40% of terms have a higher probability of occurring, the last ~40% go with topic C, and the middle more associated with topic B. To give a sense of the alpha settings, alpha = c(8, 1, 1) would result in topic probabilities of .8, .1, .1, as would alpha = c(80, 10, 10). Weâ€™ll use the gtools package for the rdirichlet function. 

```{r}
nTerms = max(wordsPerDoc)

breaks = quantile(1:nTerms, c(.4,.6,1)) %>% round()

cuts = list(1:breaks[1], (breaks[1] + 1):breaks[2], 
            (breaks[2] + 1):nTerms)

library(gtools)

B_k = matrix(0, ncol = 3, nrow = nTerms)

B_k[,1] = rdirichlet(n=1, alpha=c(rep(10, length(cuts[[1]])),    # topics for 1st 40% of terms
                                  rep(1,  length(cuts[[2]])),
                                  rep(1,  length(cuts[[3]]))))

B_k[,2] = rdirichlet(n=1, alpha=c(rep(1,  length(cuts[[1]])),    # topics for middle 20%
                                  rep(10, length(cuts[[2]])),
                                  rep(1,  length(cuts[[3]]))))

B_k[,3] = rdirichlet(n=1, alpha=c(rep(1,  length(cuts[[1]])),    # topics for last 40%
                                  rep(1,  length(cuts[[2]])),
                                  rep(10, length(cuts[[3]]))))
```


Here is a visualization of the term-topic matrix, where the dark represents terms that are notably less likely to be associated with a particular topic.

```{r}
library(ggplot2)

as.data.frame(B_k) %>%
  mutate(document = 1:nrow(.)) %>% 
  tidyr::gather(key = topic, value = prob, -document) %>% 
  ggplot(., aes(topic, document, color = prob)) +
  geom_tile()
```

Remember how we specified this -- we assigned 40% to topic 1, 40% to topic 3, and left the middle 20% to topic 2. This visualization clearly shows this.

Now, given the topic assignment, we draw words for each document according to its size via a multinomial draw, and with that, we have our document-term matrix. However, we can also think of each document as merely a bag of words, where order, grammar etc. is ignored, but the frequency of term usage is retained.

```{r}
wordlist_1 = sapply(1:nDocs, 
                    function(i) t(rmultinom(1, size = wordsPerDoc[i], prob = B_k[, z[i]])), 
                    simplify = FALSE)  

# smash to doc-term matrix
dtm_1 = do.call(rbind, wordlist_1)

colnames(dtm_1) = paste0('word', 1:nTerms)

# bag of words representation
wordlist_1 = lapply(wordlist_1, function(wds) rep(paste0('word', 1:length(wds)), wds))
```

If you print out the wordlist_1 object, you will see the words asssociated with each document.

## Topic Models

Now with some theory under our belt, we can take a look at analyzing real data.

Just like our sentiment analysis, there is a fair chunk of cleaning to do. The `textProcessor` function would do a lot of work for us, but we don't really need it to do as much since we already created a pretty clean object. We are, however, going to randomly remove some observations from our data (if the reasons are not obvious now, they will be soon). 


```{r}
library(stm)

set.seed(1001)

holdoutRows = sample(1:nrow(cleanLyrics), 100, replace = FALSE)

lyricText = textProcessor(documents = cleanLyrics$text[-c(holdoutRows)], 
                          metadata = cleanLyrics[-c(holdoutRows), ], 
                          stem = FALSE)

lyricPrep = prepDocuments(documents = lyricText$documents, 
                               vocab = lyricText$vocab,
                               meta = lyricText$meta)
```

The stm package has some pretty nice facilities for determining a number of topics:

```{r}
kTest = searchK(documents = lyricPrep$documents, 
             vocab = lyricPrep$vocab, 
             K = c(3, 4, 5, 10, 20), verbose = FALSE)

plot(kTest)
```

The 4 plots that are returned are going to try to help us determine the best number of topics to take. I like to focus on semantic coherence (how well the words hang together) and the residuals. We want to have low residual and high semantic coherence. The residuals definitely take a sharp dive as we increase K. If we consider one of the major assumptions of LDA, we could almost always guess that we would need a great number of topics (i.e., if every topic that has ever existed, existed before writing, then we could have a huge numebr of topics). Our coherence seems to do pretty well with some of the lower numbers (not entirely surprising). With all of these together, we can settle on 5 topics for our subsequent analyses. 

It is worthwhile to note that we can also do model selection with the stm package, but that is some work that will be best done if you want some additional playtime.

With our 5 topics, we can start our actual model:

```{r}
topics5 = stm(documents = lyricPrep$documents, 
             vocab = lyricPrep$vocab, seed = 1001,
             K = 5, verbose = FALSE)
```

We can get a lot of output from our model, but we can focus on the expected topic proportions plot:

```{r}
plot(topics5)
```

We are essentially looking at the proportion of each topic, with a few of the highest probability words. 

This is great, but it is really fun to see what emerges from the topics. 

```{r}
labelTopics(topics5)
```

Let's focus on the frex words (they occur *fr*equently within the topic and are *ex*clusive to that topic) and the highest probability words (i.e., the words that have the highest probability of occuring within that topic). The Lift and Score words (just a few different ways of weighting occurance) can be useful, but are a bit less intuitive than the other two. Let's put some names to the topics. 

We can look at lyrics that have a high probability of being associated with each topic:

```{r}
findThoughts(topics5, texts = lyricPrep$meta$text, n = 1)
```

We can see that the story we put to our topics makes sense, given what we see in the actual texts.

One of the great things about topic models is that they are probablistic, meaning that each document has a certain probability of belonging to a topic:

```{r}
head(topics5$theta, 15)
```

Document 1, for example has a probability of ~.19 for belonging to topic 1 and .78 for belonging to topic 2. Document 3 has a bit of a range, with probabilities of .31, .20, and .42 for belonging to topics 1, 2, and 3.

```{r}
lyricPrep$meta[1, ]

lyricPrep$meta[3, ]
```

We can also see what terms are in documents 1:

```{r}
lyricPrep$documents[[1]]
```


```{r}
lyricPrep$vocab[lyricPrep$documents[[1]][1, ]]
```

And 3:

```{r}
lyricPrep$documents[[3]]
```

```{r}
lyricPrep$vocab[lyricPrep$documents[[3]][1, ]]
```

While the previous methods would certainly allow us to classify new document, they do work best for informational retrieval. There are times that we don't want to train a new model (and go through the step of determining a topic number), but instread just want to apply our trained model on new data. Topic models, on the other hand, are great for predicting topic probabilities for unseen document.

```{r}
newLyricText = textProcessor(documents = cleanLyrics$text[holdoutRows], 
                          metadata = cleanLyrics[holdoutRows, ], 
                          stem = FALSE)

newLyricCorp = alignCorpus(new = newLyricText, old.vocab = topics5$vocab)

newLyricsFitted = fitNewDocuments(model = topics5, documents = newLyricCorp$documents, 
                newData = newLyricCorp$meta, origData = lyricPrep$meta)
```


Topic models can be extended to include covariates, where we are examining how much each topic contributes to a document given some other data.

We can also add additional information to our models: content. What we get here is a better understanding about how people use language within a topic differently.

<!-- ```{r} -->
<!-- set.seed(1001) -->

<!-- topicPredictor = stm(documents = executionsPrep$documents,  -->
<!--              vocab = executionsPrep$vocab, prevalence = ~ totalVictims, -->
<!--              data = executionsPrep$meta, K = 5, verbose = FALSE) -->

<!-- labelTopics(topicPredictor) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- victimEffect = estimateEffect(1:5 ~ totalVictims, stmobj = topicPredictor,  -->
<!--                metadata = executionsPrep$meta) -->

<!-- summary(victimEffect, topics = c(1:5)) -->

<!-- plot.estimateEffect(victimEffect, "totalVictims", method = "continuous",  -->
<!--                     model = topicPredictor, topics = 1, labeltype = "frex") -->

<!-- plot.estimateEffect(victimEffect, "totalVictims", method = "continuous",  -->
<!--                     model = topicPredictor, topics = 2, labeltype = "frex") -->

<!-- plot.estimateEffect(victimEffect, "totalVictims", method = "continuous",  -->
<!--                     model = topicPredictor, topics = 3, labeltype = "frex") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- set.seed(1001) -->

<!-- topicContent = stm(documents = executionsPrep$documents,  -->
<!--              vocab = executionsPrep$vocab, content = ~ codefsYes, -->
<!--              data = executionsPrep$meta, K = 5, verbose = FALSE) -->

<!-- labelTopics(topicContent) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- plot(topicContent, type = "perspectives", topics = 1) -->
<!-- ``` -->

