---
title: "Practical Data Wrangling With R"
output:
  html_document:
    toc: true
    toc_float: true
    theme: flatly
    highlight: zenburn
    css: documentCSS.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, error = FALSE, comment = "")
```

# Our Purpose {.tabset .tabset-fade .tabset-pills}

We are going to learn how to wrangle data, but to what end? Wrangling purely for the sake of it is exceedingly rare and certainly useless. 

In all of our examples, we are going to do the following:

1.  Import
2.  Tweak
3.  Visualize

We are going to see examples in a few different ways: namely base R and tidyverse.

# The Tidyverse {.tabset .tabset-fade .tabset-pills}

We are going to be working around in the *tidyverse* for a good chunk of our time together. The whole point of the tidyverse is to offer a grammer of verbs. It is going to help us in a lot of the situations that we are going to be seeing.

Another great feature of the tidyverse is the pipe: %>%

It does the same thing as the Unix |, but | in R is an or operator. 

With all of the glowing praise for the tidyverse, we are still going to see some base R. Sometimes, it will demonstrate great reasons for using the tidyverse. In other situations, it will help you to not be afraid to use it when situations arise.


# Some Demonstrations {.tabset .tabset-fade .tabset-pills}

## Table

```{r}

library(ggplot2)

plotDat = aggregate(diamonds$cut, by = list(cut = diamonds$cut), FUN = length)

colnames(plotDat)[2] = "n"

plotDat
```

## Visual

```{r}
ggplot(plotDat, aes(x = cut, y = n)) +
  geom_point(aes(size = n)) +
  theme_minimal()
```

## (Im)Proper Plotting

Look at help(mtcars) and check out the variables. Can you spot what is wrong with this plot?

```{r}
ggplot(mtcars, aes(x = wt, y = mpg, color = am)) + 
  geom_point() +
  theme_minimal()
```


## Proper Plotting

The plot below is likely better.

```{r}
library(dplyr)

mtcars$amFactor = as.factor(mtcars$am) 

ggplot(mtcars, aes(x = wt, y = mpg, color = amFactor)) + 
  geom_point() +
  theme_minimal()
```


## Pipes: Making Life Easier


Recall some of the things that we just saw:

```{r, eval = FALSE}
plotDat = aggregate(diamonds$cut, by = list(cut = diamonds$cut), FUN = length)

colnames(plotDat)[2] = "n"

ggplot(plotDat, aes(x = cut, y = n)) +
  geom_point(aes(size = n)) +
  theme_minimal()
```

This is somewhat tricky code. We have to create a new object with the oft-muddy aggregate function and reset a column name (by magic number, no less). 

This can be made much easier with dplyr:

```{r}
diamonds %>% 
  group_by(cut) %>% 
  summarize(n = n()) %>% 
  ggplot(., aes(x = cut, y = n)) +
  geom_point(aes(size = n)) +
  theme_minimal()
  
```

It isn't a reduction in lines, but it is certainly clearer and follows a more logical thought process. This is the whole point of the tidyverse (and dplyr specifically) -- allowing you to write how you would explain the process. 

As an added bonus, we don't need to create a bunch of different objects to do something simple.

We can see that dplyr will also make the plot for am easier.

```{r}
mtcars %>% 
  mutate(am = as.factor(am)) %>%  
  ggplot(., aes(x = wt, y = mpg, color = am)) + 
  geom_point() +
  theme_minimal()
```

### On Code Golf

You will often notice that a dplyr chunk might take a few more lines to work through than base R alone -- don't consider this as a bad thing. There will be many times in this course and in your potential work that you might think that you need to use as few lines as possible. Resist this temptation. Sometime you need to break something up into many lines and create new objects -- this ability is exactly why we use R!

## Your Turn

1.  Read in the following data:
    - Try read.csv()
2.  Make a quick plot
    - Follow the ggplot stuff we just saw
3.  Tweak
4.  Replot

## Curves

![Curves Ahead](http://www.firstsign.com/Shared/Images/Product/Right-Winding-Road-Traffic-sign-W1-5R-30-clone/W1-5R_winding-road_S-curve_right_left_right_arrow_firstsign_com.jpg)


# Data Import {.tabset .tabset-fade .tabset-pills}

Importing data is often the easiest part (never too hard to import a nice .csv). Sometimes, though, we need some other strategies.

## Delimited Files

Frequently, you will see nicely delimited text files that are not .csv files -- these are often tab-delimited file, but they can take other forms. 

```{r, eval = FALSE}
read.table("https://download.bls.gov/pub/time.series/ce/ce.data.42a.RetailTrade.Employment", 
           header = TRUE, sep = "\t")
```

Is the same as:

```{r, eval = FALSE}
read.delim("https://download.bls.gov/pub/time.series/ce/ce.data.42a.RetailTrade.Employment")
```

The read.table() function gives you added flexibility to specify the delimiter (these can be semicolons, periods, or any number of odd things). 

Examine the following file from SDC and read it in properly:

```{r sdcFile}

```


## Haven and readxl

You will often get data in proprietary formats:

### Excel

```{r, eval = FALSE}
readxl::read_excel(path = "folder/fileName")
```


### SAS

```{r, eval = FALSE}
haven::read_sas(data_file = "folder/fileName.sas7dbat")
```


### Stata

```{r, eval = FALSE}
haven::read_dta(file = "folder/fileName.dta")
```

### SPSS

We often see the -99 added as the missing value in SPSS (of course, there is no way that -99 would ever be an actual value, right?).

```{r, eval = FALSE}
haven::read_spss(file = "folder/fileName.sav", user_na = "-99")
```


## HTML

Depending on your needs, reading an html table into R is getting to be too easy.

```{r, eval = FALSE}
library(rvest)

read_html() %>% 
  html_table()
```


## rio

For many of these tasks, you can just use the rio package -- you give it the file and it will do the rest!

```{r, eval = FALSE}
rio::import("folder/file")
```


## Nested Structures

### JSON

Web-based graphics started getting popular not too long ago. Generally, stats people were not using them, but web developer-type folks were. They needed a structure that would work well for the web and interact with their JavaScript-based graphics -- thus, JavaScript Object Notation (JSON) was born. You will see JSON come out of many web-based interfaces. 

There are a few JSON-reading packages in R, but jsonlite tends to work pretty well.

```{r, eval = FALSE}
jsonlite::read_json(path = )
```


This is a very simple form of JSON. We are going to see a hairier verson of this data soon. 

#### JSON Dangers

There is JSON and then there is JSON. You might find yourself some interesting data and want to bring it in, but an error happens and you have no idea why the read_json function is telling you that the file is not JSON. 

Not all JSON is pure JSON! When that is the case, you will need to create pure JSON.

```{r amazonExample}

```


### XML

It is getting to be a bit less common (still prevalent), but XML (eXtensible Markup Language) is another type of nested structure common on the web.

```{r, eval = FALSE}
xml2::read_xml()
```

## Mass Reading

Everything we just learned is great and you will use them all in your data wrangling missions.

Fortunately (or unfortunately, depending on how you look at it), it is not the whole story -- you will frequently be reading in many files of the same time.

If you have two files, you might be able to get away with brute force:

```{r, eval = FALSE}
myData1 = read.csv("test.csv")

myData2 = read.csv("test2.csv")
```

Would you want to do this for 5 files? What about 100? Or 1000? I will answer it for you: no!


The chunks below introduce some very important functions. We are going to see the lapply function again -- it is important that you learn to love the apply family!

```{r, eval = FALSE}
allFiles = list.files(path = "", all.files = TRUE, full.names = TRUE, 
                      recursive = TRUE, include.dirs = FALSE)

allFilesRead = lapply(allFiles, function(x) read.csv(x, stringsAsFactors = FALSE))

allData = do.call("rbind", allFilesRead)
```


You can also use rio:

```{r, eval = FALSE}
rio::import_list("", rbind = TRUE)
```


## Your Turn

1.  Take a look at this file:
2.  Determine how it should be read in.
3.  Summarize the data.
4.  Visualize the data in any way you wish!

If that was easy for you, then do the same thing with the following file:



# Selecting {.tabset .tabset-fade .tabset-pills}

## Base

There are many ways to select variables with base R:

```{r, eval = FALSE}
mtcars[, c(1:5, 7:8)]

keepers = c("mpg", "cyl", "disp", "hp", "drat", "qsec", "vs")

mtcars[, keepers]

mtcars[, c("mpg", grep("^c", names(mtcars), values = TRUE))]
```


You can also drop variables:

```{r}
mtcars[, -c(1:2)]

dropVars = c("vs", "drat")

mtcars[, !(names(mtcars) %in% dropVars)]
```

Issues?

For starters, the magic numbers are a no-go. The keepers lines could work, but would be a pain if we had a lot of variables. 

Let's check this wacky stuff out where we want all variables that start with "age" and variables that likely represent questions (x1, x2, x3, ...):

```{r}
library(lavaan)

testData = HolzingerSwineford1939

names(testData)

keepers = c(grep("^age", names(testData), value = TRUE), 
            paste("x", 1:9, sep = ""))

testData = testData[, keepers]

```


Not only do we have another regular expression, but we also have this paste line to create variable names. It seems like too much work to do something simple!

While not beautiful, these are perfectly valid ways to do this work. I have such sights to show you, but don't forget about this stuff -- you never know when you might need to use it.

## dplyr

We have already seen a bit of dplyr, but we are going to dive right into some of the functions now.

In base R, we have to do some chanting to select our variables. With dplyr, we can just use the select function:

```{r}
mtcars %>% 
  select(mpg, cyl, am)
```


We can also drop variables with the -:

```{r}
mtcars %>% 
  select(-vs)
```


We also have several helper functions that we can use:

```{r}
HolzingerSwineford1939 %>% 
  select(num_range("x", 1:9), starts_with("age"), 
         matches("^s.*.l$"))
```


### Not Important, But Helpful

Changing variable position in R is a pain:

```{r}
head(HolzingerSwineford1939[, c(1, 7:15, 2:6)])
```



```{r}
HolzingerSwineford1939 %>% 
  select(id, starts_with("x"), everything()) %>% 
  head()
```


## Your Turn!



# Subsetting/Filtering {.tabset .tabset-fade .tabset-pills}

One of the more frequent tasks is related to filtering/subsetting your data. You often want to impose some types of rules on your data (e.g., US only, date ranges).

## Base

Base R gives us all the ability in the world to filter data.

```{r}
summary(mtcars[mtcars$mpg < mean(mtcars$mpg), ])
```

Unless you know exactly what you are doing, this is a bit hard to read -- you might be asking yourself what the comma means and why there is nothing after it.


## dplyr

When we use filter, we are specifying what it is that we want to keep.

Filter this or that:

```{r}
mtcars %>% 
  filter(cyl == 4 | cyl == 8) %>% 
  summary()
```

Filter this and that:

```{r}
mtcars %>% 
  filter(cyl == 4 & mpg > 25) %>% 
  summary()
```

Filter this out:

```{r}
mtcars %>% 
  filter(cyl != 4) %>% 
  summary()
```



Naturally, it can also take a function

```{r}
mtcars %>% 
  filter(mpg < mean(mpg)) %>% 
  summary()
```


## Your Turn



# New Variables and Recoding {.tabset .tabset-fade .tabset-pills}

## Base

Adding a new variable in base R is as easy as the following:

```{r}
mtcars$roundedMPG = round(mtcars$mpg)
```

## dplyr

If, however, we want to do things in a tidy chunk, we need to use mutate.

```{r}
mtcars = mtcars %>% 
  mutate(roundedMPG = round(mpg))
```


## Base Recoding

You will need to recode variables at some point. Depending on the nature of the recode it can be easy (e.g., to reverse code a scale, you just subtract every value by max value + 1).

You will need to do some more elaborate stuff:

```{r, eval = FALSE}

mtcars$mpgLoHi = 0

mtcars$mpgLoHi[mtcars$mpg > median(mtcars$mpg)] = 1
```


```{r, eval = FALSE}
mtcars$mpgLoHi = ifelse(mtcars$mpg > median(mtcars$mpg), 1, 0)
```

These are pretty good ways to do recoding of this nature, but what about this:

```{r, eval = FALSE}
mtcars$vs[which(mtcars$vs == 0)] = "v"

mtcars$vs[which(mtcars$vs == 1)] = "s"
```


Or this:

```{r, eval = FALSE}
mtcars$vs = ifelse(mtcars$vs == 0, "v", "s")
```


## dplyr recoding

```{r, eval = FALSE}
recode(mtcars$vs, `0` = "v", `1` = "s")
```


# Day 1 Wrap

We covered a pretty good chunk of stuff today: importing, filtering, and subsetting. 

While none of this stuff was super fancy, the introduction to dplyr will serve us well going forward. 

As an end of the day exercise, let's do the following:

1.  Read this data:

2.  Keep observations with x or y.

3.  Keep only the following variables:

# Day 2

# Reshaping {.tabset .tabset-fade .tabset-pills}

Now, things are going to get weird.

Data can take many different forms.

We can have data that looks like this:

```{r}
wideDat = data.frame(id = 1:3, 
                     age = c(33, 35, 37), 
                     employeeType = c("full", "full", "part"))
```


Or like this:

```{r}
wideDat = data.frame(id = rep(1:3, times = 2), 
                     variable = rep(c("age", "employeeType"), each = 3), 
                     value = c(33, 35, 37, 
                               "full", "full", "part"))
```


The first type, is what many will recongize as standard tabular data. Each row represents an observation, each column is a variable, and each "cell" holds one value.

The second type, long data, is what many will call key-value pairs. You will often see data like this in timeseries data.

You will encounter people who will swear that one way or the other is the ideal way to represent data -- we are going to opt for pragmatic as opposed to dogmatic. We can easily switch between these two types of data representations -- this is called reshaping.

There is a bit of a hierarchy in R with regard to reshaping data. The reshape function in the stats package can handle most of your needs, but to resulting data is a bit on the ugly side (bad default row names, weird automatic column names, and a bunch of arguments). The reshape package gives you all of the power, but with clearer code and better output. The reshape2 package has all of the power, but with some added functionality. the tidyr package makes things incredibly easy, but at the expense of some flexibility. 

## Base/stats

Try the following chunk of code with the as.data.frame(). Why, you might ask? Almost everything in dplyr converts data to a tibble. Many base R functions will go crazy when they encounter a tibble, so you need to explicitly make it a data frame. You might ask what is the trouble tibbles (anyone?)...

```{r}

data("starwars")

as.data.frame(starwars) %>% 
  filter(species == "Human" & grepl("(Skywalker)|(Rey)|(Vader)|(Kylo)", .$name)) %>% 
  select(name, height, mass) %>% 
  reshape(., idvar = "name", v.names = "values", varying = list(2:3), 
          times = c("height", "mass"), direction = "long") %>% 
  ggplot(., aes(x = name, y = values, color = time)) + 
  geom_point(size = 3.5) +
  scale_color_brewer(palette = "Dark2") +
  theme_minimal()
  
```


## reshape

Let's use the reshape package to do the same thing. You are going to notice a few differences in the function arguments. The reshape packages have this notion of melting (going from wide to long) and casting (going from long to wide). Melting makes plenty of sense to me, but I can only imagine what casting means. 

```{r}
starwars %>% 
  as.data.frame() %>% 
  filter(species == "Human" & 
           grepl("(Skywalker)|(Rey)|(Vader)|(Kylo)", 
                 .$name)) %>% 
  select(name, height, mass) %>% 
  reshape::melt.data.frame(., id.vars = "name", 
                           measure.vars = 2:3, 
                           variable_name = "type", na.rm = TRUE) %>% 
  ggplot(., aes(x = name, y = value, color = type)) + 
  geom_point(size = 3.5) +
  scale_color_brewer(palette = "Dark2") +
  theme_minimal()
```


## reshape2

We don't need to worry about the tibble issue with reshape2!

```{r}
starwars %>% 
  filter(species == "Human" & grepl("(Skywalker)|(Rey)|(Vader)|(Kylo)", .$name)) %>% 
  select(name, height, mass) %>% 
  reshape2::melt(., id.vars = "name", 
                           measure.vars = 2:3, variable.name = "type", 
                           value.name = "value", na.rm = TRUE) %>% 
  ggplot(., aes(x = name, y = value, color = type)) + 
  geom_point(size = 3.5) +
  scale_color_brewer(palette = "Dark2") +
  theme_minimal()
```

## tidyr

Allows for dplyr variable selection and a little bit more clarity with creating the id(s) variables.

```{r}
starwars %>% 
  filter(species == "Human" & grepl("(Skywalker)|(Rey)|(Vader)|(Kylo)", .$name)) %>% 
  select(name, height, mass) %>% 
  tidyr::gather(., key = type, value = value, -name) %>% 
  ggplot(., aes(x = name, y = value, color = type)) + 
  geom_point(size = 3.5) +
  scale_color_brewer(palette = "Dark2") +
  theme_minimal()
```


## Your Turn

# Merging {.tabset .tabset-fade .tabset-pills}

Now we are playing with power! Having multiple datasets in memory is one of R's strong points (not everything can manage such a modern feat). Once you get out there, this becomes important.

Not only can we have multiple datasets open, but we can also merge those datasets together, with the proper variables, of course. 

## base

The merge function in base R, like everything else, can do us a great amount of good. 

```{r, eval = FALSE}

board = haven::read_sas()

organization = haven::read_sas()

mergedDat = merge(x = board, y = organization, by = "", 
      all.x = TRUE, all.y = FALSE)

```

If there is anything good to be gotten from SQL, it is the notion of different joins and the handy language that it provides for specifying those joins. The merge function gives us no such explicit conventions (we would need to intuit or...read the documentation).


### Simulated Merryment

#### Live And Onstage!

Left join = all rows from x and all columns from x and y

Right join = all rows from y and all columns from x and y

Inner join = all rows from x with matching values in y and all columns from x and y

Semi join = all rows from x with matching values in y and just columns from x

Full join = everything

With that knowledge, can we map the various combinations of all.x and all.y? 

## Left

```{r, eval = FALSE}
left_join()
```


## Right

```{r, eval = FALSE}
right_join()
```

## Inner

```{r, eval = FALSE}
inner_join()
```


## Semi

```{r, eval = FALSE}
semi_join()
```


## Full

```{r, eval = FALSE}
full_join()
```


## Anti

I didn't mention the anti join before! It does exactly what it sounds like -- it finds the things that don't match. A natural curiousity is the potential purpose for such a function. Can anyone think of anything?

```{r, eval = FALSE}
anti_join()
```


## Your Turn



# Summarizing And Grouping

If we recall, we already saw a little bit of grouping and merging (if you don't, you might remember that mess with aggregate). Given that we already saw aggregate, we will just dive right into the tidyverse.

## dplyr

Grouping data and comparing various summary statistics by group is a common task. Sometimes it is just a means of exploration and sometimes it will actually answer the question. No matter the need, you will likely find it quite simple.

```{r}
mtcars %>% 
  summarize(meanMPG = mean(mpg), 
            meanSD = sd(mpg))
```


You can even summarize all of your variables in handy way.

```{r}
mtcars %>% 
  summarize_all(funs(mean, sd), na.rm = TRUE)
```

Because we are dealing with the tidyverse, variable selection is included.

```{r}
mtcars %>% 
  summarize_at(vars(starts_with("c")), 
               funs(mean, sd), na.rm = TRUE)
```


Combining group_by with summarize welcomes even more power to summarize data.


```{r}
mtcars %>% 
  group_by(am) %>% 
  summarize(meanMPG = mean(mpg), 
            sdMPG = sd(mpg))
```


You are not limited to single group_by statements!

## Your Turn

Group and plot


# String Cleaning {.tabset .tabset-fade .tabset-pills}

Data has strings...it is a simple fact of modern data.

If you can clean strings, you can conquer any data task that gets thrown at you. To clean strings, though, you will need to learn how to use magic!

## Regular Expressions

Regular expressions (regex) are wild. Regex's purpose is to match patterns in strings. 

Of everything that we have and will see, regex is something that you can use in places outside of data. 

Some regular expressions are very easy to understand (once you know what they mean): [A-Za-z]+

Others take some intense trial and error: \\(*[0-9]{3}.*[0-9]{3}.*[0-9]{4}

Learning just a little and being able to use them in a variety of settings is most helpful.




## stringr

What is the difference between sub and gsub?

What is the difference between grep and grepl?

Why did grep just return a bunch of numbers?

What does the following do: "^\\s+|\\s+$"

For the love of all that is good, what does regexpr() do?

These are just a few of the questions that will come up when working with strings in base R.

There is also the issue of mixed arguments. Consider grep and gsub.

```{r}
realComments = c("I love wrangling data", "stringz r fun", 
                 "This guy is a hack", "Can't we use excel?")

grep(pattern = "\\b[a-z]{2}\\b", x = realComments, value = TRUE)

gsub(pattern = "(hack)", replacement = "star", x = realComments)
```


It is pretty subtle, but the argument order can be a bit troublesome when you are just learning or have not used them in a while. 

Check these out:

```{r}
library(stringr)

str_subset(string = realComments, pattern = "\\b[a-z]{2}\\b")

str_replace_all(string = realComments, pattern = "(hack)", 
                replacement = "star")
```

We now have consistent argument order and very clear names. 

Clear names and consistent arguments aside, stringr also simplifies some previously cumbersome processes.

```{r}
matchedComments = regexpr(pattern = "love|like|enjoy", 
                          text = realComments)

regmatches(x = realComments, m = matchedComments)

```

This becomes the following with stringr:

```{r}
str_extract_all(string = realComments, 
                pattern = "love|like|enjoy")
```



# Day 2 Wrap

Good times all around today! We got into some pretty meaty issue on data wrangling. Reshaping can be a real challenge to your future wrangling tasks and joining/merging can give you fits if you don't really check your data.

If there is anything to really take away from today is regex. It might seem like you may never use it, but you will find places to use it. Really take some time to play with it.

# Day 3

# Fuzzy Joins {.tabset .tabset-fade .tabset-pills}

We have seen joins and some character work, but now we are going to combine them into one world.

Whenever we use joins we are generally looking for exact matches. In reality, we get about 50/50.

Fuzzy joins allow us to use string distance metrics to join non-matching strings together.

## String Distances

Not including spelling mistakes, there are many different ways to represent words; this is especially true when we are discussing companies.

Would AG Edwards and A.G. Edwards join? Of course not! They are the same company, but not the same words.

We have learned enough about string cleaning to know that we could tidy that one up, but what about something more subtle, like AG Edward and AG Edwards. 

We can use the adist function to figure out the distance between these two strings.

```{r}
string1 = "AG Edward"

string2 = "AG Edwards"

adist(x = string1, y = string2)
```

The adist function uses generalized edit distance as the metric. This does things like calculate the number of characters that need to be inserted, deleted, or substituted to make a match. 

If we are merging, generalized edit distance might not give use the level of granularity that we need for minimizing.


Let's try out a few different strings below.

```{r}
library(stringdist)

string3 = "A G Edwards"

stringdist(string1, string2, method = "jw")

stringdist(string2, string3, method = "jw")

stringdist(string1, string3, method = "jw")
```

And compare them to our standard edit distance:

```{r}
adist(c(string1, string2, string3))
```

We can see that we get a little more fine scoring with the Jaro-Winker distance.


Now let's take a gander at Jaccard's distance:

```{r}
stringdist(string1, string2, method = "jaccard")

stringdist(string2, string3, method = "jaccard")

stringdist(string1, string3, method = "jaccard")
```


You might be wondering why we need much granularity. Consider what happens between Safeway and Subway:

```{r}
stringdist("safeway", "subway", method = "soundex")

stringdist("safeway", "subway", method = "jaccard")

stringdist("safeway", "subway", method = "jw")

```

I think we can imagine the consequences of using metrics like soundex or Jaccard distance to join strings together.

So, is there a good time to use something like soundex: sure. If you find yourself needing to find possible word matches

## Fuzzy Joins

# Tips & Tricks {.tabset .tabset-fade .tabset-pills}

## Row/Group Indices

When you are doing some type of data wrangling tasks (especially when things need grouped or merged), you might find a row or group index to be helpful.

```{r}
carsID = mtcars %>% 
  mutate(rowID = 1:nrow(mtcars)) %>% 
  group_by(cyl) %>% 
  mutate(groupID = 1:n())
```


## Leads and Lags

## Dates and Times

### lubridate

Another glorius thing that will happen to you is working with dates -- they tend to be a pain.

```{r}
Sys.Date()

format(Sys.Date(), "%m-%d-%Y")
```

And you get things like this:

```{r}
date1 = "12012018"

date2 = "20180112"

date3 = "12-01-2018"

date4 = "12/01/2018"
```

If you had to merge data based upon these dates, you would be in trouble.

You could, naturally, use some of the string cleaning stuff that we learned to tear them apart and then re-arrange them in a consistent manner.

```{r}
dateStrings = strsplit(date3, split = "-")

paste(dateStrings[[1]][3], 
      dateStrings[[1]][2], dateStrings[[1]][1], 
      sep = "-")
```


If we wanted to go down the path of that previous chunk, we would need to pass the whole thing into an lapply.

Or...we can just do this:

```{r}
library(lubridate)

mdy(date3)
```


### hms



## Lists

Lists are just a part of life at this point. You will see lists of data frames and you will see columns within data frames that contain lists.

If you recall our previous look at JSON, you will see that there is a variable that actually contains a list.

We also saw our starwars example:

```{r}
glimpse(starwars)
```


There would be a few ways to tackle such an enterprise:

```{r}
movies = unique(unlist(starwars$films))

starwars = starwars %>% 
  mutate(revengeSith = ifelse(grepl(movies[grep("Sith", movies)], films), 
                              1, 0))
```



```{r}

library(tidyr)

data(starwars)

starwars = starwars %>% 
  unnest(films) %>% 
  mutate(id = 1:nrow(.)) %>% 
  spread(key = films, value = films) %>% 
  group_by(name) %>% 
  fill(12:18, .direction = "up") %>% 
  fill(12:18, .direction = "down") %>% 
  slice(1)
```


# Back To The Basics {.tabset .tabset-fade .tabset-pills}

## Functions

I genuinely hope you learned a lot from our time together; however, if you only learn one thing, I hope that you feel empowered to write your own functions. For all of R's greatness, writing your own functions is what really makes it work so well. 

You might not feel like you can write a function -- we all feel that way at some point. You can!

Functions are simply objects that act upon other objects. 

This is a simple function:

```{r, eval = FALSE}
sum(fido)
```


```{r, eval = FALSE}
meanFunction = function(x) {
  xLength = length(x)
  
  res = sum(x) / xLength
  
  return(res)
}
```

Specify what you are passing into the function (x above).

R already has a built-in mean() function, but now you know how easy it is to do such things.

When working on data manipulation tasks, functions become very useful because you will find yourself doing the same thing a lot. 

-- If you find yourself doing something more than twice, write a function!

## Your Turn

1.  Create a vector of something.

Something like the following:

```{r}
x = 1:5

# or...

x = c(1:4, 6)
```

2.  Tweak your vector in some fashion:

```{r}
x = x + 1
```


3.  Create a simple function to do something to your vector:  

Maybe try a series of math operations.

```{r}
testFunc = function(x) {
  res = sum(x * 3)
  return(res)
}
```


## Apply Family

We have seen a lot of different things over the last few hours together. The final bit of wisdom to pass along is the apply family. The apply family (lapply, sapply, mapply, etc.) allows you to pass a function over something like a list and then return something predictable. 

```{r}
testNames = c("Jack", "Jill", "Frank", "Steve", "Harry", "Lloyd")

sapply(testNames, function(x) paste(x, "went over the hill", sep = " "))
```

The sapply function will return a vector -- what do you think an lapply will return? mapply?

Now that is a bit of a goofy example. Since R is vectorized, we would have gotten the same result just by pasting our testNames vector to the string. But, a more realistic situation is as follows:



```{r}
starwarsShips = unique(unlist(starwars$starships))

shipRider = lapply(starwarsShips, function(x) {
  
  riders = starwars$name[which(grepl(x, starwars$starships))]
  
  res = data.frame(ship = x, 
                   riders = riders)
  
  return(res)
})
```



