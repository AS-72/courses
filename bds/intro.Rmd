---
title: | 
      | Introduction To 
      | Behavioral Data Science
author: "Behavioral Data Science"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, comment = "")

library(dplyr)
```


# Week 1

## Person-centric Data

Data from the people, about the people! Person-centric data covers a broad range of potential forms and sources. It might be survey data with attitudinal variables, purchase history, number of visits to the doctor's office, number of alcoholic drinks had during the last seven days, internet browsing behavior, or even Tweets. If we start to consider all of the data that we see, it becomes clear that a great deal of data is person-centric.

Not all data fits into this notion of person-centricity. We often find financial data (think end of day trading) that really does not fit into our notion of person-centric data. Even though this type of stock-flavored financial data does not really fit, the amount of money a person spends certainly does. Furthermore, much public data exists for municipal entities, such as county assessors. It might seem like a properties record is not person-centric, but we could build data that tracks a person's property buying/selling -- we would certainly look at such data and consider it to be behavioral in nature.

## Where Do We Go?

What do we do with person-centric, behavioral data? In general, we want to predict or explain behavior in some way! To do that, we need to create some hypotheses. We must, however, be very careful in crafting our hypotheses. While we can do some fancy analyses that start to lead us down a more causal path, we can never fully generalize what makes people do the things that they do.

This will be a theme that is revisited during our time together. Just so that you can start to fully get your head around hypotheses, let's take the first try at a little game.

I am noticing that visitors to my site spend less time on one page compared to other pages. Here is what that data might look like:

```{r, echo = FALSE}
visitorData = data.frame(visitorID = c(1, 1, 1, 2, 2, 3, 3, 3, 3), 
                         pageID = c("a", "b", "c", "a", "b", "a", 
                                    "b", "c", "d"), 
                         secondsOnPage = rnorm(9, mean = 15, sd = 4))
visitorData
```

I have a visitor identification number, a page identifier, and the number of seconds each user spent on a given page. Do I have data that would allow me to test if there is a difference in the amount of time that people spend on a page? I certainly do! With the data that you see, could I determine exactly why there might be differences in time on page? I would say rather unlikely with the provided data. We always need to make sure that we are not speaking beyond our data!

This was just a demo version of the game -- you will unlock more difficult levels as we progress. 

### Analyzing Behavioral Data

#### On $R^2$ 
Think back to your linear models course. Do you remember examining adjusted $R^2$? It is one of the more popular measures of how well a particular model explains the data (adjusted $R^2$ is noted as how much variance in the dependent variable is explained by the predictor variables in the equation). We traditionally are pushed towards looking for very high values. Students come from a variety of statistical traditions have been taught a great many "thresholds" for acceptable $R^2$ values -- anywhere from .4 to .7 can be mentioned all in the span of a few seconds. If, however, we are trying to explain what someone does (i.e., a particular behavior), what are the chances that we will obtain such adjusted $R^2$ values? Probably not very high, right? People are messy and often unpredictable, so achieving huge $R^2$ values is unlikely. 

Does a small $R^2$ mean that your model is garbage&#8253; Of course not! A small $R^2$ does absolutely nothing to take away from the relationships between your predictor variables and your outcome. A low $R^2$ might mean that you have more error in your prediction, but there are still better ways of figuring that out than just looking at one value and giving up.

This is not to suggest that we should accept an $R^2$ of .000001 as the most predictive model in the world (note -- it probably is not); however, we should not be terribly dejected to see a model with 4 or 5 predictors with an $R^2$ around .2. That means that we have taken one of nature's sloppiest and most unpredictable creatures, and explained 20% of the variance of a behavior.

With that aside on $R^2$ out of the way, we can think more generally about analyzing behavioral data (that is why we are here). The Behavioral Sciences have a rich tradition of interesting analyses: from experimental design to causal models and all points in between. When dealing with behavioral data, we are frequently looking at some type of latent variables; these are constructs we think exist, but have no way to physically touch them. We are going to be dealing in the latent world for the majority of our time together, so start thinking about latent variables now!

## Data Wrangling

Person-centric data comes in many different flavors. 

There is the classic cross-sectional, wide format (tends towards a vanilla taste):

```{r, echo = FALSE}
classicWide = data.frame(id = 1:5, 
                         satisfaction = sample(1:7, 5, replace = TRUE), 
                         leftTip = sample(0:1, 5, replace = TRUE))
```

```{r, echo = FALSE}
classicWide
```


We also have repeated measures:

```{r, echo = FALSE}
repeatedMeasures = data.frame(id = rep(1:3, each = 3),
                              observation = rep(1:3, 3),
                              satisfaction = sample(1:5, 9, replace = TRUE),
                              leftTip = sample(0:1, 9, replace = TRUE)) %>% 
  mutate(rating = satisfaction + sample(1:2, 9, replace = TRUE))

repeatedMeasures
```


And. key-value pairs:

```{r, echo = FALSE}
repeatedMeasures %>% 
  tidyr::gather(key = variable, value = value)
```


And then from there, we can start to do combinations:

```{r, echo = FALSE}
repeatedMeasures %>% 
    tidyr::gather(key = variable, value = value, -id)
```


All of these have their place. Many analyses require traditional wide data. Others, like panel models, require long data (e.g., our repeated measures data).


```{r}
library(plm)

panelModel = plm(rating ~ satisfaction, index = c("id", "observation"), 
                 data = repeatedMeasures)

summary(panelModel)
```

Sometimes, we need to reshape our data for the sake of visualization (such visualizations are especially handy for survey questions).

```{r, echo = FALSE}
library(ggplot2)
datTest = data.frame(id = as.factor(1:3),
           satisfaction1 = sample(1:7, 3, replace = TRUE), 
           satisfaction2 = sample(1:7, 3, replace = TRUE), 
           satisfaction3 = sample(1:7, 3, replace = TRUE))
datTest
```

If we want to show several people's survey responses, we need to do some reshaping:

```{r, echo = FALSE}
datTest %>% 
  tidyr::gather(key = variable, value = value, -id) %>% 
  ggplot(., aes(id, value, color = variable)) +
  geom_point(alpha = .5, size = 3) +
  theme_minimal()
```


See if you can reproduce the data and visualization! Just as a hint, you will need to make your wide data into long data (melt from reshape2 or gather from tidyr should do the trick).

No matter the need, don't be afraid to shape your data in a way that conforms to *your* needs. Always remember that you are in control of your data -- your data is not in control of you! With various packages from base, the tidyverse, and beyond, you have the power to make your data be whatever you need it to be. While you cannot control the sloppy data that people produce, you can control the data format.

*Link to dataWranglingPrimer.html*

## Ethical Considerations

When dealing with person-centric data, we need to always be thinking about ethics. Are we treating the data in a secure manner and honoring people's privacy? Are we being honest to people about how we are going to handle their data? These are questions that require your care and attention. 

This becomes even more important when sharing data. Look at the following data and think about whether or not it should be shared as is:

```{r}
data.frame(age = sample(18:35, 8, replace = TRUE), 
           gender = sample(c("male", "female"), 8, replace = TRUE), 
           department = sample(c("cs", "hr", "it", "accounting"), 8, replace = TRUE), 
           score = sample(50:100, 8)) 
```

There are not any personal identifiers here, so we are good to put it on Github or Google Drive and share away...right? Let's pick this apart a little bit. Any of the demographic-inclined variables, in isolation, are not a cause for any concern; when we have all of them together, though, we might be able to identify individuals through "triangulation". This becomes especially true when there are groups with fewer individuals. Triangulation is a real concern with any type of data, but we need to be diligent in good security and privacy practices when we are dealing with person-centric data. This gets into an area known as de-identification -- the removal of not only concrete identifiers (e.g., names, SSNs), but also anything that can be pieced together to identify a person. So, what items would we need to remove from our data to make it de-identified? 

It might seem far fetched that anyone would ever go through the trouble of identifying your data, but you should think conspiratorially when it comes to your data -- imagine the absolute worst thing that could happen if someone took it and identified it. Could it cause personal problems, job lose, or reveal something embarrassing/illegal? 

Remember, the 4Chan collective found a flag in rural Tennessee using contrail patterns, star alignment, and a car horn -- in 37 hours. Don't forget that they also contributed to the identification and subsequent airstrike of an ISIS training facility. Just imagine what could be done with your interesting data.   

### *On Anonymity*

It really does not exist -- ever. If you are in charge of collecting any type of person-centric data, you can never promise complete anonymity; doing so is truly an amateur move and demonstrates a misunderstanding of behavioral data. To guarantee anonymity is to guarantee that there is no possible way that a person could be identified -- this is almost never the case. You may, on the other hand, promise confidentiality. That means that you will take every possible precaution to ensure that a person's data is not broadcast or imprudently shared.   

# Week 2


## Data Types And Structures

We see many different types of data when working with behavioral data, well beyond what we typically think about with our different levels of measurement (nominal, ordinal, interval, and ratio). 

### Attitudinal Data

Survey data is where we typically find attitudinal data and it tends to present some interesting issues. How are the items measured -- are they traditional Likert response options (*strongly disagree* to *strongly agree*) or are they some type of feelings thermometers (typically some crazy range that could be anywhere between 0 to 100 or -100 to 100). Perhaps it is a Likert-type (*Not at all* to *Very much*). How many response options are there -- 3, 5, 7, or more? 

When dealing with such attitudinal data, we might see data that takes any of the following forms:

```{r}
library(magrittr)
data.frame(agreementNumeric = 1:5, 
           agreementOrdered = ordered(1:5, labels = c("Strongly disagree", 
                                                      "Disagree", "Neither disagree nor agree", 
                                                      "Agree", "Strongly agree")), 
           agreementCharacter = c("Strongly disagree", 
                                  "Disagree", "Neither disagree nor agree", 
                                  "Agree", "Strongly agree"), 
           pseudoLikert = c("Not at all", "A little", "Some", "A lot", "Very much")) %T>%
  glimpse() %>% 
  summary()
```

This is where data familiarization and exploration becomes important -- always take note of variable types. If you are receiving this data from another source, you never know what you might be getting.

With the data as it is in its current form, think about what you might like to do with it. Would you like to plot it or create a correlation matrix? Can you make that happen? Data processing becomes very important when working through any survey-based data.

What functions would you use to convert everything to numeric?

### Censored Data
We might run into data that is *censored*. While it might sound like something exciting at first, censored variables are simply those that we do not know about the values before or after data collection started/ended. You may run into variables that have been *discretized* -- a continuous variable, such as age, broken down into discreet categories. When we get that age variable, we might see categories like "<25", "25-35", "36-45", "46-55", and "55+". When people have an age of "<25", we have left censored data -- we don't know how far to the "left" the value actually is, we just know that it is somewhere under 25. For our observations with "55+", we have right censored data -- we don't know how far to the right the value actually is. 

```{r, echo = FALSE}
data.frame(ageCategory = c("<25", "25-35", "36-45", "46-55", "55+"), 
           censored = c("Left censored", "Uncensored", "Uncensored", 
                        "Uncensored", "Right censored"))
```

While we won't be diving into them, tobit models are one such way for dealing with censored dependent variables.

The following is similar to an example from the AER package. The max number of reported affairs is 12, but that was at the time of reporting -- the philandering partners could have engaged in more affairs after data was collected. To that end, we need to account for right censoring.

```{r, echo = TRUE}
library(AER)

data("Affairs")

fm.tobit = tobit(affairs ~ age + yearsmarried + religiousness,
                   right = 12, data = Affairs)

summary(fm.tobit)
```

#### *Special Note On Discretization*

If you find yourself on the planning end of some broader data collection efforts, absolutely resist the temptation (both internal or external) to discretize data. Some people will say, "But response rates...blah...blah...blah", or, "We have to make it easy.", or, "Think of the children." Consider the following data:

```{r, echo = FALSE}
data.frame(age = c("<25", "25-35", "36-45", 
                   "36-45", "46-55", "55+"))
```

You have 6 respondents -- what is the mean age of those respondents? I have no idea and neither do you! If you allow data to be collected this way, you are immediately imposing limits on what you might do with that data in the future. Someone will likely say, "Just take the middle point of the range and call that the response."  

```{r, echo = FALSE}
data.frame(age = c(25, 30, 40, 40, 50, 55)) %T>% 
  print(.) %>% 
  summarize(mean(age))
```

Great.~ We have an average now. What if the actual ages were as follows:

```{r, echo = FALSE}
data.frame(age = c(21, 30, 36, 45, 52, 74)) %T>% 
  print(.) %>% 
  summarize(mean(age))
```

What is your mean age now? Is it still 40? Will it matter for your analyses? You need to decide if the difference in the answer is important or not. Maybe you decide it is not that different and are comfortable doing this. Let me pose an additional question: is a 25 year old the same as a 35 year old? Just think about that for your own self: is 18 year old you the same as 24 year old you? This becomes an even bigger issue when we have ranges with unequal intervals (commonly seen in income brackets that might start with a few thousand dollars in range, but end with upwards of 100K dollars or more in range).

And here is another common issue: someone will want to discretize a variable and use it as an outcome variable in a model. Such shenanigans is common with discretized income variables. You have the power to make that happen (think back to our friend ordered logistic regression from generalized linear models). To the person who was expecting a standard linear regression, they are going to be really confused when you show them more than 1 intercept! 

Remember, it is okay to make things easy on people, but give them some credit. Most people know how old they are -- if they won't tell you their exact age, they probably won't be inclined to tell you the range either. 

### Hierarchical Data

Also known has multilevel or nested data, *hierarchical* data is best defined by a classical example -- we could have students, nested within classrooms, nested within schools, nested within districts, and so on.

Here is what some nested data might look like:

```{r, echo = FALSE}
data.frame(id = 1:15,
           subgroup = rep(1:5, each = 3), 
           group = rep(1:3, each = 5))
```

We have an id variable, which is giving us an individual identifier. Each person is in 1 of 5 subgroups and each subgroup is in 1 of 3 groups. 

Depending on your needs, this structure can be utilized in your analyses (we are going to be getting into it within the next few weeks).

### Repeated Measures

Repeated measures can be thought of in the same way as longitudinal data -- we are measuring the same thing for a person for an extended period of time. If you want, you can even think of it as nested data; the time-based observations are nested within an individual.

Here is how that data will typically look:

```{r}
data.frame(id = rep(1:3, each = 3), 
           observation = rep(1:3, each = 3), 
           responseTimeSeconds = sample(1:5, 9, replace = TRUE))
```

This data shows us 3 different people, each with 3 different observations. 

### Text

Welcome to the now! With people generating volume after volume of text, we have many questions that we can explore.

We can also have a bit of exploratory fun!

```{r, echo = FALSE}
library(wordcloud2)

presidentialTweets = c("The highly anticipated meeting between Kim Jong Un and myself will take place in Singapore on June 12th. We will both try to make it a very special moment for World Peace!", 
                       "Senator Cryin’ Chuck Schumer fought hard against the Bad Iran Deal, even going at it with President Obama, & then Voted AGAINST it! Now he says I should not have terminated the deal - but he doesn’t really believe that! Same with Comey. Thought he was terrible until I fired him!", 
                       "The Failing New York Times criticized Secretary of State Pompeo for being AWOL (missing), when in fact he was flying to North Korea. Fake News, so bad!", 
                       "The Fake News is working overtime. Just reported that, despite the tremendous success we are having with the economy & all things else, 91% of the Network News about me is negative (Fake). Why do we work so hard in working with the media when it is corrupt? Take away credentials?", 
                       "The Iran Deal is defective at its core. If we do nothing, we know what will happen. In just a short time, the world’s leading state sponsor of terror will be on the cusp of acquiring the world’s most dangerous weapons")

presidentialTweets %>% 
  tokenizers::tokenize_words(., strip_numeric = TRUE, 
                             stopwords = tm::stopwords("SMART")) %>%
  unlist() %>% 
  tm::stemDocument(.) %>% 
  table(.) %>% 
  as.data.frame(.) %>% 
  wordcloud2(., shape = "pentagon")
```

You can likely imagine the source, which gets back to our conversation last week about ethics and confidentiality. A great deal of data is public and out there for the taking, but not all of it is.

## Sources Of Data

When we are dealing in behavioral data, there are many places that we can typically find it. Many are obvious, like surveys. Others, however, might require a little bit more imagination.

### Surveys

Self-reported surveys might be the classic source for behavioral data. With the rise or web-based surveys, collecting data has never been easier. Although response rates have been declining over the years, it is still easy to amass a reasonably-sized sample (especially if you can pay participants). In addition to collecting your own data, there are numerous big surveys that make the data available to the public (e.g., Behavioral Risk Factor Surveillance System and National Survey on Drug Use and Health).

### Client-side Paradata/Metadata

Surveys provide data that people explicitly offer, but what about the behaviors that go into survey esponding (or any behaviors that lead up to an action). Paradata and metadata offer such data. While both relate more to the "hidden" data that people produce, they are a bit different. Metadata is all of the little stuff that happens when people use a site -- login times, time on page and navigation, and other data related to use. What was once thought of in the context of person-to-person surveys, client-side paradata (CSP) details the small things that people do. Mouse movements, clicks, scrolling, and related variables can all be collected and analyzed.

### Records

Records tend to be a bit tricky, but they are out there (in fact, a great many organizations make money hand over fist by pulling public records together). While you might be granted to keys to kingdom and be given access to such databases, you can pull public records from many different places: county assessors websites and county courthouse websites are two such examples (jailtracker is fun to use). Many cities across America are also diving into the open data game and providing a great wealth of data. While it might not all be useful, there is certainly enough there to begin thinking about interesting questions. 

### Social Media

Social media is ripe for data (being mindful of Terms of Service, of course). Post history, both what was posted and in frequency, are excellent data points. Social networks are also a popular research endeavor. 

## More Data Wrangling

*Link to dataWranglingPrimer.html*

## Ethical Considerations


# Week 3

## Data Exploration

Although we have identified our hypotheses and gotten our data into shape, that does not mean that we should just go right into hypothesis testing. 

We can use the power of R to explore our data in many different ways. Perhaps you would like to dive right in and examine the relationships within your data. You might be tempted to construct a correlation matrix:

```{r}
corDat = data.frame(item1 = sample(1:7, 50, replace = TRUE), 
           item2 = sample(1:7, 50, replace = TRUE), 
           item3 = sample(1:7, 50, replace = TRUE), 
           item4 = sample(1:7, 50, replace = TRUE), 
           item5 = sample(1:7, 50, replace = TRUE))

cor(corDat)
```

Quickly, where are the strongest correlations? It probably took a few seconds to process the whole correlation matrix; now imagine doing that with many variables. 

This is where corrplot (or corrgrams to some) come in handy.

```{r}
corrplot::corrplot(cor(corDat))
```

This gives us a much quicker way to find the strong relationships amongst our variables. It can also give us a good idea about how items might shake out into components:

```{r}
set.seed(5)

df = data.frame(item1 = sample(1:7, 10, replace = TRUE), 
           item2 = sample(1:7, 10, replace = TRUE), 
           item3 = sample(1:7, 10, replace = TRUE), 
           item4 = sample(1:7, 10, replace = TRUE), 
           item5 = sample(1:7, 10, replace = TRUE), 
           item6 = sample(1:7, 10, replace = TRUE), 
           item7 = sample(1:7, 10, replace = TRUE), 
           item8 = sample(1:7, 10, replace = TRUE), 
           item9 = sample(1:7, 10, replace = TRUE), 
           item10 = sample(1:7, 10, replace = TRUE), 
           item11 = sample(1:7, 10, replace = TRUE), 
           item12 = sample(1:7, 10, replace = TRUE), 
           item13 = sample(1:7, 10, replace = TRUE), 
           item14 = sample(1:7, 10, replace = TRUE), 
           item15 = sample(1:7, 10, replace = TRUE))
cor(df) %>% 
  corrplot::corrplot(., order = "FPC")
```

Knock Knock

Who's There?

Orange

Orange Who?

Orange you glad that you did not need to look through a 15 by 15 correlation matrix to find the strongest relationships?

While it might take you a bit to appreciate statistical humor, you will immediately see benefits to such plots.

You can even make them more art than utility:

```{r}
df %>% 
  cor() %>% 
  as.matrix() %>%
  reshape2::melt(.) %>% 
  ggplot(., aes(Var1, Var2, fill = value)) +
    geom_raster(interpolate = TRUE) +
  scale_fill_gradient(low = "#00ffd5", high = "#ff5500") +
  theme_minimal()
```

Not only can you quickly explore your data for hotspots, you can also create desktop backgrounds.

Heatmaps can be used in conjunction with distance measures to tell how similar observations might be. Instead of looking at a massive distance matrix, we can compute the distance matrix, and pass it along to our visualization to see how similar (or not) these 50 people are to each other (this gets into clustering, which we will be seeing later in the course).

```{r, echo = FALSE}
library(factoextra)

res.dist <- get_dist(corDat, stand = TRUE, method = "pearson")

fviz_dist(res.dist, 
          gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
```


### Apophenia

Look at this visualization:

```{r, echo = FALSE}
set.seed(2)

data.frame(x = rnorm(100), 
           y = rnorm(100)) %>% 
  ggplot(., aes(x, y)) +
  geom_point() +
  theme_minimal()
```

You see the relationship, don't you? 

Hopefully, you aren't seeing much of anything -- these are just two random normal distributions plotted. If you did start to see some pattern, don't worry -- this just means that you are indeed a human being. We have a natural flaw that makes use see patterns when they are not really there; this is why people often see Jesus in chips and on their toast. 

Let's try this out.


```{r, echo = FALSE}

inferviz::simViz(df, item5, item7, answer = FALSE)

```


Do any of those visualizations pop out to you? In other words, is there one that is absolutley not like the others? Not really. However, one of those visualizations contains the actual data -- the other three have random data for one of the variables. Hopefully, you didn't trick yourself into seeing anything terribly strong.

Let's try it with something that might have slightly stronger relationships:

```{r, echo = FALSE}
inferviz::simViz(mtcars, mpg, wt, answer = FALSE)
```

Now, can you spot the real relationship in this one? Probably!

Let's bump the distractors now:

```{r, echo = FALSE}
inferviz::simViz(mtcars, mpg, hp, distractors = 5, answer = FALSE)
```

How's that? Is it easy to spot the one that is markedly different?

There is a point to all of this -- don't get fooled by false patterns. Just because we are studying human behavior and the data it produces, does not make us immune from the trappings of being human.


## The Hypothesis-Data Consistency Link

> “A foolish consistency is the hobgoblin of little minds, adored by little statesmen and philosophers and divines. With consistency a great soul has simply nothing to do. He may as well concern himself with his shadow on the wall. Speak what you think now in hard words, and to-morrow speak what to-morrow thinks in hard words again, though it contradict every thing you said to-day. — 'Ah, so you shall be sure to be misunderstood.' — Is it so bad, then, to be misunderstood? Pythagoras was misunderstood, and Socrates, and Jesus, and Luther, and Copernicus, and Galileo, and Newton, and every pure and wise spirit that ever took flesh. To be great is to be misunderstood.” 

Emerson's (Ralph Waldo, not Keith) beautiful words are great if we are trying to expand our minds and engage in learning in general. One place that we cannot use this, however, is in connecting our data and hypotheses. If we have hypotheses about data, we had better have the data to test those hypotheses. While this may seem intuitive, behavioral data can often present unforeseen challenges. For example, you might be given some data full of attitudinal measures and someone has some ideas about relationships they want to test. If said person is not fully in the know about how to construct and test hypotheses, he/she might come to you wanting to test hypotheses that do not make sense given the data (maybe they want to "say" something about a scale that is not really measured by the scale).  




## Ethical Considerations

## More Data Issues