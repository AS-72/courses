---
title: "Item Response Theory"
author: "Behavioral Data Science"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```


## Item Response Theory And Factor Analysis

Last week, we learned about latent variables and how we can use factor analysis to construct measures for assessing latent traits. While we are still dealing in the world of latent variables, we are taking a new approach to measuring those latent variables -- Item Response Theory (IRT). While both allow us to measure a person's level of a latent trait, IRT gives us the power to understand the person and the items. We can recall that factor analysis falls under the broader heading of Classical Test Theory (CTT) -- the name is a clear indication that the focus is on the test as a whole, not individual items. 

In addition to this broadened understanding, IRT has a different focus from a measurement perspective than factor analysis. Perhaps most important is the notion of measurement error in IRT. In factor analysis, measurement error is assumed to be consistent across each item; this is not the case for IRT.

## Dichotomous And Polytomous Models

Before we dive into some of the more important parts of IRT, it is important to draw some distinctions between two broad families of models: dichotomous and polytomous. A dichotomous model can best be thought of in testing situations where a correct answer exists (you either get the question correct or not, so the outcome is dichotomous). With that knowledge, you might have already guessed what the polytomous model entails -- items that have ordered responses with no clear correct choices (Likert response options would be a good example). Polytomous models include such models as the graded response model and the partial credit model.

We will see how these models behave differently as we continue discussing the different parts of IRT models.

## Ability

In IRT, we talk a person's ability; essentially it is the level of the latent variable that a person possesses. In the parlance of psychometrics, ability is denoted as $\theta$. The $\theta$ scale is standardized with a mean of 0 and tends to go from -4 to 4 (while this is generally seen, it might not always be the case). 

## Different Parameters

The ability to explore individual items within IRT is based upon three different parameters: location, discrimination, and psuedoguessing. We can incorporate any of those parameters into our model. First, though, we need to understand what those parameters mean.

### Location

In IRT, the location parameter (denoted as $\\b_i$) described the difficulty of the item. If we are talking about a testing situation (e.g., a math or science test), the meaning of $\\b_i$ is nearly self-evident -- it is simply the probability of a correct response to the question in a dichotomous model and the probability of moving from one category to another in polytomous models (the level of passing from one category to the next is called a *threshold*).

Let's look at some examples from the *mirt* documentation of different item locations. The *ltm* package is also a great package for IRT models.

```{r}
library(mirt)

library(ltm)

# The model arguement set to 1 means we are just looking
# at a one factor model.

fit1PL <- mirt(LSAT, model = 1, itemtype = "Rasch")

plot(fit1PL, type = 'trace', which.items = 1:5, 
     facet_items = FALSE)

```


The resulting plot is what is known as an *Item Characteristic Curve*. It contains a lot of information, but we are only going to pay attention to the location right now. Without any prior knowledge, which of the labelled curves would say represents the most difficult item? If you said "3" (the black line), then you are absolutely correct. When we look at the location of an item in an ICC graph, we are looking for its placement along the x-axis around its middle point (i.e., find the middle of a curve and draw a line straight down -- the farther to the right, the more difficult the item). With item 3, we can see that it takes someone with slightly more than average ability to have a better than chance probability of responding correctly.  


### Discrimination

Item discrimination, the $a_i$ parameter, demonstrates the likelihood of people with varying degrees of ability to respond correctly.

Let's specify a slightly different model and take a look at our ICC again:

```{r}
fit2PL <- mirt(LSAT, model = 1, itemtype = "2PL")

plot(fit2PL, type = 'trace', which.items = 1:5, 
     facet_items = FALSE)
```


In our last model, we saw that all of the items had the exact same curve -- this is no longer the case. If we pay attention to item 3 again, we see that a person with very low ability ($\theta = -4$) has nearly a probability of 0 to respond correctly, someone with average ability ($\theta = 0$) has around a 50-50 chance of responding correctly, and high ability people ($\theta = 4$) have nearly 100% chance of responding correctly. We would definitely say that item 3 discriminates between different ability levels pretty well (we will see if it is the most discriminating item later). Which item might not discriminate too well? Without doubt, you said item 1, and you would be correct. While people at lower ability levels have less chance of responding correctly to item 1, we can see that a probability of 1 is obtained pretty soon after crossing $\theta = 0$. So while item 1 may provide a bit of discrimination at the lower end of $\theta$, it does nothing when dealing average or more. This provides even more evidence as to why item 3 is such a good item -- it discriminates very well along every point of $\theta$

So, we have a pretty good idea that item 3 is the most difficult item and the most discriminating item. This, hopefully, provides a demonstration on why IRT models are so powerful -- we can compare individual items to people (on the same scales, no less).

### Guessing

The third parameter, $c_i$, is the psuedoguessing parameter. Even the blind squirrel finds a nut and $c_i$ accounts for this. Essentially, the guessing parameter offers a slight penalty for the chance of guessing a correct answer.


# Models And Parameters

## 1PL

Now that we know what our parameters mean, we can start putting them into models. The one parameter logistic model (1PL) only estimates item difficulty ($b_i$). It assumes that all items discriminate equally and that all items have the same guessing parameter. 

Although some minor differences, you will often see the 1PL and Rasch model used interchangibly.

## 2PL

The two parameter logistic model (2PL) adds the discrimination parameter, $a_i$, into the model.

## 3PL

The three parameter logistic model (3PL) adds in the guessing parameter, $c_i$. You might wonder how in the world a guessing parameter is added! If someone could guess the correct answer (think about a multiple choice test), would the probability of a correct answer ever be 0? I would think not. So, $c_i$ just includes a lower-bound asymtote. 

```{r}
fit3PL = mirt(LSAT, model = 1, itemtype = "3PL")

plot(fit3PL, type = 'trace', which.items = 1:5, 
     facet_items = FALSE)
```

If we compare the ICC plots we saw for the 1PL and 2PL earlier, we see a a significant difference between the lower asymptote.

```{r}
par(mfrow = c(1, 3))

plot(fit1PL, type = 'trace', which.items = 1:5, 
     facet_items = FALSE, main = "1PL")

plot(fit2PL, type = 'trace', which.items = 1:5, 
     facet_items = FALSE, main = "2PL")

plot(fit3PL, type = 'trace', which.items = 1:5, 
     facet_items = FALSE, main = "3PL")
```

While we see that the probability for a low ability individual obtaining a correct answer for item 3 is still very low, it has improved slightly.

With all of these parameters, the probability of a correct response can be expressed as follows: $$p_i(\theta) = c_i + \frac{1 - c_i}{1 + e^{-a_i(\theta - b_i)} }$$


# Scoring

Scoring in IRT models works a bit differently than scoring in CTT. 

An element common to both IRT and CTT is the *true score*, $true_{score} = observed_{score} + e$. The distinction rests in how they treat $e$ -- CTT assumes that error is the same for everyone and IRT allows for error to vary freely across people.

We can see the factor scores returned from our model:

```{r}
facScores = fscores(fit3PL)

head(facScores)

summary(facScores)

```


## Different Measurement Levels

So far, we have discussed models that are largely dealing with dichotomous variables (largely incorrect and correct). These are aptly names dichotomous models. However, we know that not all measures have a binary response. If we have multiple response options (perhaps a Likert response option format), then we need to use a polytomous model. One common model is the Graded Response Model (GRM). 

The GRM is used with the express purpose of ordinal, polytomous response options.

Let's take a look:

```{r}
library(psych)

library(dplyr)

testData = bfi %>% 
  dplyr::select(starts_with("C")) %>% 
  na.omit()

library(ggridges)
library(viridis)
library(ggplot2)

testData %>% 
  tidyr::gather(key = question, value = score) %>% 
  ggplot(., aes(score, question, fill = ..x..)) +
  geom_density_ridges_gradient() +
  scale_fill_viridis(option = "B") +
  theme_minimal()


grmMod = mirt(testData, model = 1, itemtype = "graded")

coef(grmMod, simplify = TRUE, IRTpars = TRUE)

```

In our coefficient output, we see coefficients for each item (b1:b5). These coefficients detail the *thresholds* for moving from one response level to the next (we also see the discrimination value). 

Just like our dichotomous models, we can also see our item characteristic curves.

```{r, fig.height=9, fig.width=9}

plot(grmMod, type = 'trace', which.items = 1:5, 
     facet_items = TRUE, main = "GRM")
```

We can see that these are very different looking ICC plots then what we saw before. Remember, that we are dealing with different models here. Instead of the probabililty of incorrect or correct, we are now dealing with the probability of moving from one response category to the next. To that end, each item's ICC has a curve for each response option over ability. If we look at the ICC for C1, we can see that someone offering a response of 1 on the scale would have a higher probability of having a lower ability. Conversely, someone offering a response of 6 would have a higher probability of being high ability.
