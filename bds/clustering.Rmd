---
title: "Clustering"
author: "Behavioral Data Science"
date: "March 6, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

There is no shortage of ways to cluster data. We can think of several large classes of cluster analyses.

Two of the bigger divisions are hard clustering and fuzzy clustering. Hard clustering algorithms assign observations to a specific cluster, whereas fuzzy clustering algorithms are probablistic in nature (i.e., an observation has a certain probability to belonging to any one cluster).



## Centroid Clustering

Centroid clustering is based upon distances around some centroid (maybe a mean). Many centroid models tend to produce hard clustering by nature.

Careful attention needs to be paid to your data when clustering. Are they variables that naturally contain mean? If so, a k-means clustering might work well.

### Determining Factor Numbers

There are several ways to test the appropriate numbers of factors (scree plots, silhouette), but we are going to use the GAP statistic to select an appropriate number of *k*.

```{r}
library(lavaan)

library(dplyr)

testData = HolzingerSwineford1939 %>% 
  select(starts_with("x"))

kTest = NbClust::NbClust(testData, method = "kmeans")

```

We are going to get a lot of information from the NbClust function, but a majority rule for number of clusters is the most helpful.

With our proposed number of clusters, let's turn to some models.

### k-means

Let's run a k-mean model with our test data.

```{r}
kmeansTest = kmeans(testData, 2)

kmeansTest
```

Our output provides some very useful information. First, we see the cluster means for each item across the two cluster. These give us a pretty good idea of the "location" for each of the clusters and helps us to start to get an idea for what these cluster look like. 

Next, we see our clustering vector. This tells us to which cluster each observation was assigned. 

We can also take a look at the bivariate plots.

```{r}
plot(testData, col = kmeansTest$cluster)
```


And a plot based upon the first two components of a PCA:

```{r}
cluster::clusplot(testData, kmeansTest$cluster)
```


The k-means cluster is great and widely used. But, it can be picky about data (it really likes data without extreme values) and it can have different results based upon item ordering (because of the cluster assignment process).

### medoids

To circumvent the issue within k-means, we can partition around medoids. Whereas our centroid can be an arbitrary value that gets observations clustered around it, a medoid is an actual observation within the data that then gets other observations clustered around it.

```{r}
library(cluster)

pamTest = cluster::pam(testData, k = 2)
```

We can see very similar output, but we get the actual observations that represent the medoids. 


```{r}
cluster::clusplot(testData, pamTest$clustering)
```


If we look at both plots, we can notice some definite differences in cluster assignment when we are getting towards the 0s.

```{r}
par(mfrow = c(1, 2))

cluster::clusplot(testData, pamTest$clustering, main = "PAM")

cluster::clusplot(testData, kmeansTest$cluster, main = ("KMeans"))
```



## Distribution Clustering

Sometimes referred to as model-based clustering, distribution clustering allows us to model variables that have mixed distributions (maybe a mixture of normal distributions or a normal distribution and something else).

```{r, echo = FALSE}
N     = 1000              # this is how many data you want
probs = c(.3,.8)          # these are *cumulative* probabilities; since they 
                          #   necessarily sum to 1, the last would be redundant
dists = runif(N)          # here I'm generating random variates from a uniform
                          #   to select the relevant distribution

# this is where the actual data are generated, it's just some if->then
#   statements, followed by the normal distributions you were interested in
data = vector(length=N)
for(i in 1:N){
  if(dists[i]<probs[1]){
    data[i] = rnorm(1, mean=0, sd=1)
  } else if(dists[i]<probs[2]){
    data[i] = rnorm(1, mean=10, sd=1)
  } else {
    data[i] = rnorm(1, mean=3, sd=.1)
  }
}

plot(density(data))
```

If your data have a mixture of normals or you just want the actual distribution of your data to drive the clustering, then these distribution clustering algorithms will offer better solutions. Furthermore, they are better suited for clustering items that might have a latent structure.

Given that we are using our data's distributions, we need to do another check on the number of clusters. Not only will we get a good number of clusters, but we will also get the clustering shapes that best fit our data.

```{r}
library(mclust)

bicTest = mclustBIC(testData)

bicTest
```

We see that a two cluster solution might work well, but what does VEE mean? It means that our mixture is ellipsoidal in shape with equal volume and orientation. Check the mclustModelNames function for all names.

Let's take a peak at it:

```{r}
mclustTest = Mclust(testData, 2, modelNames = "VEE")


mclustTest
```

We can see our clustering table looks a lot different than our previous methods.

Now, let's look at our clustering center:

```{r}
mclustTest$parameters$mean
```



```{r}
plot(mclustTest, what = "classification")
```


We can also look at the components graph (like we saw earlier):

```{r}
coordProj(testData, parameters = mclustTest$parameters, z = mclustTest$z)
```


Do you notice anything different? The shape of the clustering is a bit different, yes, but there is something else -- some of the observations are marked with a 

## Hierarchical Clustering

Everything we have seen to this point needs some type of work to know how many clusters to find. 

Conversely, hiearchical clustering will clusters without any input:

```{r}
hieararchicalTest = cluster::agnes(testData)
```

```{r}
plot(hieararchicalTest)
```


