---
title: |
        Generalized Additive Models |
        Decision Trees  |
        Random Forest   |
author: "Behavioral Data Science"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Generalized Additive Models

## Parametric...Or Not

Most of the statistical shenanigans you have seen to this point has come from the parametric family. In other words, we are making assumptions about the underlying distributions. What if we don’t make any assumptions or we really have no idea and we want to let it be defined by our actual data? Then, we are operating in a non-parametric space. How do we let our data do the talking?

## Smoothing

There are many types of smooths. You will frequently see the loess (local regression – sometimes you will hear it as locally-weighted scatterplot smoothing or lowess). With a loess line, we are fitting some polynomial (generally the linear or the quadratic) to a small section of our data at a time (i.e., a local group) – this is a little bit more complicated than our moving average window type of smooth. Each small section has an associated line and each line gets joined with the line in the next group (these are referred to as knots). Since we are largely in control here, we get to specify how wiggly things might get.

You will also see regression splines (largely what we will be using here today). The great thing about these is that we can penalize them!

## Additive Models

Very briefly, an additive model is not much different than our normal interpretation of a model. In our additive model, we can look at the affect of a predictor on a dependent variable without any consideration for what other variables might be in the model. We can add these effects to best predict our response.

## GAM

GMAT TIME – lm is to glm, as additive models are to...?

During the last few weeks, we have largely been working in the generalized linear models framework. We are going to stay in the general vicinity, but start moving to some more interesting places! We have mostly seen straight lines being fitted to various things. As many of you have likely noted, it often seems like a straight line doesn’t really fit the relationships that we can see within our data. So, what do you do? We could always go the transformation route, but that seems a bit antiquated at this point...don’t you think? 

What if we fit a smooth line to our data instead of trying to jam a single straight line somewhere it does not want to be or do something like throwing a single quadratic term into the model? Now we are doing something interesting.

The car package is old-school R, but still has some handy stuff for us. 

```{r}
library(car)

library(dplyr)

noLeaders = read.csv("bds/leadershipRatingsAgreement.csv")

scatterplotMatrix(noLeaders[, !(names(noLeaders) %in% c("leaderID"))])
```

The splom that we just saw gives us a really good idea about the relationships within the data. The green line is a linear line and the red line is a smoothed line. If those are not sitting on top of each other, then you might want to think carefully about the relationship that is present.

```{r}
plotDat = noLeaders %>% 
  select(effect, enabling) %>% 
  na.omit

plot(effect ~ enabling, data = plotDat)
lines(sort(plotDat$enabling), 
      fitted(lm(effect ~ enabling, 
                data = plotDat))[order(plotDat$enabling)], col = "red")
lines(sort(plotDat$enabling), 
      fitted(lm(effect ~ I(enabling^2), 
                data = plotDat))[order(plotDat$enabling)], col = "blue")
lines(sort(plotDat$enabling), 
      fitted(lm(effect ~ I(enabling^3), 
                data = plotDat))[order(plotDat$enabling)], col = "green")
```

Let’s check this out:

```{r}
lmTest = lm(effect ~ enabling, data = noLeaders)

summary(lmTest)
```


Nothing too new here, so let’s move along!

```{r}
library(mgcv)

gamTest = gam(effect ~ enabling, data = noLeaders)

summary(gamTest)
```



Let’s try to smooth. In the following code, you will notice how we wrapped out term in s(). Believe it or not, this is to specify a smooth term. We could spend a whole week on different ways to smooth things, but we will just stick with s() and its defaults for now.

```{r}
gamTestSmooth = gam(effect ~ s(enabling), data = noLeaders)

summary(gamTestSmooth)
```


```{r}
plot(effect ~ enabling, data = plotDat)
lines(sort(plotDat$enabling), 
      fitted(lm(effect ~ enabling, 
                data = plotDat))[order(plotDat$enabling)], col = "red")
lines(sort(plotDat$enabling), 
      fitted(lm(effect ~ I(enabling^2), 
                data = plotDat))[order(plotDat$enabling)], col = "blue")
lines(sort(plotDat$enabling), 
      fitted(lm(effect ~ I(enabling^3), 
                data = plotDat))[order(plotDat$enabling)], col = "green")
lines(sort(plotDat$enabling), 
      fitted(gam(effect ~ s(enabling), 
                 data = noLeaders))[order(plotDat$enabling)], 
      col = "orange") 
```

## Bias/Variance Trade-Off

The wiggle can be controlled and you are the one to control it (all models are your monster, so build them in a way that you can control it). An important consideration to make with the wiggle (and with almost all of our decision from here on out) is the bias/variance trade-off. You will see this called other things (e.g., error/variance), depending on with whom you are hanging around. Since we have only talked about bias briefly, we do not need to worry about getting bias in this sense conflated with anything else.

It works like this: you cannot have your cake and eat it too. Do you want your in-sample predicition to be awesome (low bias)? Great! You can count on getting that at the expense of higher variance. The lower the variance, the better your model will predict new data. Well that sounds easy – just go with the lowest variance. But...that might contribute to missing some weird pattern. Again, it is just a decision to make (you likely won't be facing off with your monsters in the Arctic in the end).

With our gam models, the wigglier your line, the lower your bias will be and the better you are doing at predicting in sample. You can see the line really bending to fit the pattern within your data.

```{r}
library(ggplot2)

gamTestLambda1 = gam(effect ~ s(enabling, sp = .1, k = 40), data = noLeaders)

p = predict(gamTestLambda1, type = "lpmatrix")

beta = coef(gamTestLambda1)

s = p %*% beta

plotDat = cbind.data.frame(s = s, enabling = na.omit(noLeaders$enabling))

gam1Plot = ggplot(plotDat, aes(enabling, s)) + 
  geom_line() +
  theme_minimal()

gamTestLambda9 = gam(effect ~ s(enabling, sp = .9, k = 40), data = noLeaders)

p = predict(gamTestLambda9, type = "lpmatrix")

beta = coef(gamTestLambda9)

s = p %*% beta

plotDat = cbind.data.frame(s = s, enabling = na.omit(noLeaders$enabling))

gam9Plot = ggplot(plotDat, aes(enabling, s)) + 
  geom_line() +
  theme_minimal()

library(gridExtra)

gridExtra::grid.arrange(gam1Plot, gam9Plot)
```


# Decision Trees

Decision trees are great for classification (i.e., which category does something belong to). For the type of decision tree that we are going to use, we can think about it almost like we would a logistic model. There are, however, some major differences. The first is that we feed any number of predictors to the tree. At our root node, we will have all of the data and all of the predictors. A scan of the predictor variables is then computed that will result in a “pure” branch – it will select a variable and a split of that variable that will result in a good separation between “0” and “1”. We then create another branch and repeat the process. We do this until we have all of the observations classified.

One beauty of the decision tree is in its ability to select important variables. One drawback, however, is that it does this in a greedy fashion. It finds what splits best and goes with it at that node; it does this without consideration of what variables might help to split better later on.

Another great thing about these trees is the ability to use classification *or* regression.

```{r}
library(RCurl)

url = "https://raw.githubusercontent.com/gastonstat/CreditScoring/master/CleanCreditScoring.csv"

creditScores = getURL(url)

creditScores = read.csv(textConnection(creditScores))
```


```{r}
library(caret)

library(rpart)

library(rpart.plot)

library(e1071)

set.seed(10001)

tree1 = rpart(Status ~ Income + Savings + Debt, 
              data = creditScores, method = "class")

rpart.plot::prp(tree1, box.palette = "Blues", extra = 8)
```


If we notice the “extra” argument in our prp function, we will see an 8. This option is giving us the probability of belonging to that class (e.g., for everyone with a savings over 2, there is a .78 percent chance of them having good credit). There are many options, so be sure to look at the help file for the different options and play around with them.


```{r}
confusionMatrix(predict(tree1, type = "class"), 
                creditScores$Status)
```


Let’s check one out with some more variables:

```{r}
set.seed(10001)

noRecodes = creditScores %>% 
  select(-ends_with("R"))

bigTree = rpart(Status ~ ., data = noRecodes, 
                method = "class")

rpart.plot::prp(bigTree, box.palette = "Blues", extra = 8)

```


```{r}
confusionMatrix(predict(bigTree, type = "class"), 
                noRecodes$Status)
```


And we can mess around with a few different parameters. The “cp” argument represents the complexity parameter (whether a node decreases the lack of fit) and the minsplit represents the minimum number of observations that can be in any bucket.

```{r}
set.seed(10001)

noRecodes = creditScores %>% 
  select(-ends_with("R"))

overgrownTree = rpart(Status ~ ., data = noRecodes,
                      method = "class",
                      control = rpart.control(cp = 0, 
                                              minsplit = 5))

rpart.plot::prp(overgrownTree, box.palette = "Blues", extra = 8)
```


```{r}
confusionMatrix(predict(overgrownTree, type = "class"), 
                noRecodes$Status)
```


For the love of all that is holy, is this really necessary? We already have regression, why do we need to have something else to further cloud the skies? Here is a big hint – if you have a limited set of predictors with clearly linear relationships that are bound by concrete theory, then a linear regression model will outperform these trees. If, on the other hand, you have more than a few predictors and no clear theory to guide variable selection, then the trees will be great.


# Random Forest

Decision trees are clearly a lot of fun and have a wide variety of uses. While sometimes one tree is helpful, everything I know about horror movies leads me to believe that a single tree (perhaps on a desolate hill) always leads to some type of danger. If we remember the issue with trees being greedy and just splitting things how it finds it best, we can imagine what might happen if we only consider one tree.

This brings us to ensemble methods. If we think about what an ensemble is, we can start to guess what ensemble methods might be. There are many ensemble methods, but the one we are going to talk about now is random forests. Trees..forest…ensemble…are the connections starting to happen?

Forest is probably less pretentious than “tree ensemble”, but where does the “random” come into play? What would be the likely result of running the exact same tree 1000 times? Some minor differences aside, almost nothing. In our random forest, we will create randomness in 2 ways: by drawing a random sample of observations for each tree and by drawing a random sample of predictors for each tree.

```{r}
library(randomForest)

set.seed(10001)

rfTest = randomForest(Status ~ ., data = noRecodes, 
                      importance = TRUE, 
                      ntree = 1000)

rfTest
```


Here we see some simple summary output, with our out-of-bag error rate and the same type of confusion matrix we saw earlier. You might be wondering what “out-of-bag” means, and rightfully so! What makes the random forest random? Instead of wasting the unsampled data, it uses it as a testing set (i.e., the data that was not in the sample “bag” gets used to help perform a very basic cross validation).

We can also get the more elaborate confusion matrix from caret (note that the matrix is flipped from the previous output!):

```{r}
confusionMatrix(rfTest$predicted, noRecodes$Status, positive = "good")
```


Next, let’s look at our variable importance:

```{r}
importance(rfTest)
```


```{r}
varImpPlot(rfTest)
```

We can look at the “MeanDecreaseAccuracy” and the “MeanDecreaseGini” to get a sense of how important each variable might be. The help file for importance sheds light on how “MeanDecreaseAccuracy” is computed, but all you really need to know is that it is telling us how poorly a model performed without the variable (variables with a higher value are more important for predicition). Gini is always used as an inequality measure, but it is conceptualized as node impurity here – variables with higher values means that removing those variables resulted in greater node impurity.

## The Black Box!

Now we know what variables are important for our random forest, so when do we interpret our trees from the forest!?! We can’t...so we don’t!!! Interpreting a single tree is statistically non-sense when we are dealing with a random forest. We are really only concerned with prediction at this point.

Outside of Breiman, that does not satisfy anybody.

```{r}
# devtools::install_github('araastat/reprtree')

library(reprtree)

repTree = ReprTree(rfTest, noRecodes)

plot(repTree)
```

What happened? While we do not want to overgrow a tree, a random forest does not care in the slightest. In fact, it will grow the deepest tree that it possibly can. If you would say that this causes over-fitting, then you would be absolutely correct. Remember, though, that we are generating a lot of trees and using all of these trees to votes on the outcomes. Therefore, the over-fitting within one tree really does not make a big differenece – it will come out in the wash. But, you can see where interpretting a tree from a forest is practically nonsense!

```{r}
library(lime)

sampleObs = sample.int(n = nrow(noRecodes), 
                       size = floor(.75 * nrow(noRecodes)), replace = F)

train = noRecodes[sampleObs, ]

test = noRecodes[-sampleObs, ]


# Create Random Forest model on iris data
modelTrain = train(Status ~ ., data = train, method = 'rf')

# Create an explainer object
explainer = lime(train, modelTrain)

# Explain new observation
explanation = explain(test, explainer, n_labels = 2, n_features = 3)

# The output is provided in a consistent tabular format and includes the
# output from the model.
head(explanation)

plot_features(explanation)
```




This is but one type of classification tree/random forest scenario. The “party” package has some different tree/forest algorithms. Let’s take a look at a random forest fit with conditional inference. Instead of using something like the Gini index to maximize information, conditional inference trees use significance testing to perform the splits.

```{r}
library(party)

crfTest = cforest(Status ~ ., data = noRecodes, 
                  controls = cforest_control(ntree = 1000))
```


Let’s see if we did any better:

```{r}
confusionMatrix(predict(crfTest), noRecodes$Status, positive = "good")
```

We can get a sample tree from this forest, but we are not going to do it. One important thing to note is that party will not create a deep tree by default.