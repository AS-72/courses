---
title: "Sentiment Analysis"
output: html_document
---

# Sentiment Analysis

## Words As Data

Words are everywhere. Believe it or not, you are reading words right now! Given our penchant for taking things and making numbers out of them, you are probably already guessing that we can somehow make words tell a story with numbers. If that is what you are guessing, then you are absolutely correct.

## Processing Text

Before we can even begin to dive into analyzing text, we must first process the text. Processing text involves several steps that will be combined in various ways, depending on what we are trying to accomplish.

### Stemming

Tense aside, are jumped, jump, and jumping the same thing? Yes, but what if we compare the actual strings? On a string comparison side, are they the same? No. We have a string with 6, 4, and 7 characters, respectively.

What if we remove the suffixes, "ed" and "ing" -- we are left with three instances of "jump"? Now we have something that is equivalent in meaning and in a string sense. This is the goal of stemming.  

Let's take a look to see how this works (you will need to install <span class="pack">tm</span> and <span class="pack">SnowballC</span> first):

```{r}
jumpingStrings = c("jump", "jumping", "jumped", "jumper")

tm::stemDocument(jumpingStrings)
```


We got exactly what we expected, right? You might have noticed that "jumper" did not get stemmed. Do you have any idea why? Let's think through it together. "Jump", "jumping", and "jumped" are all verbs related to the act of jumping. "Jumper", on the other hand, is a person who jumps -- it is a noun. Martin Porter's stemming algorithm works incredibly well!

Hopefully, this makes conceptual sense; however, we also need to understand why we need to do it. In a great many text-based methods, we are going to create a matrix that keeps track of every term (i.e., word) in every document -- this is know as a document-term matrix. If we know that "jump", "jumping", and "jumped" all refer to the same thing, we want it just represented once within our document-term matrix.

Shall we take a look?

```{r}
library(tm)

documents = c("I like to jump.", 
              "I have jumped my whole life.", 
              "Jumping is in my blood.", 
              "I am a jumper.")

documentsCorp = tm::SimpleCorpus(VectorSource(documents))

documentsDTM = DocumentTermMatrix(documentsCorp)

inspect(documentsDTM)
```

We can see that without stemming, we have 9 terms (things like "I", "a", and "to" get removed automatically). Let's do some stemming now:

```{r}
documentsStemmed = stemDocument(documents)

documentsStemmed
```

And now the document-term matrix:
```{r}
stemmedDocCorp = tm::SimpleCorpus(VectorSource(documentsStemmed))

stemmedDocDTM = DocumentTermMatrix(stemmedDocCorp)

inspect(stemmedDocDTM)
```

If we are trying to find documents that are covering similar content or talking about similar things, this document-term matrix will help to draw better conclusions, because it is clear that the first three documents are talking about the act of jumping and this document-term matrix reflects that.

This is going to be especially important for next week's topic: topic models.

### Stop Words

Some words do us very little good: articles, prepistions, and very high frequency words. These are all words that need to be removed. Fortunately, you don't have to do this on your own -- a great many dictionaries exist that contain words ready for removal.

```{r}
tm::stopwords("en")
```


Removing stopwords takes little effort!

```{r}
documents = c("I like to jump.", 
              "I have jumped my whole life.", 
              "Jumping is in my blood.", 
              "I am a jumper.")

tm::removeWords(documents, words = stopwords("en"))

```

We can even include custom stopwords:

```{r}
tm::removeWords(documents, words = c("blood", stopwords("en")))
```


### Regular Expressions

Stemming and stopword removal will go a long way to helping us process and clean our text. There are times, however, when you need complete control over what you remove. You might need to remove punctuation and other symbols, in addition to a wide array of things that come up in typical analyses (emoticons and wacky encoding characters!). When those cases pop up, you might need to make use of regular expressions to help. You should have already learned about regular expressions, but be prepared to circle back to them constantly when engaging in any matter of data science shenanigans!

Many text processors will deal with hyphenated words. If, however, your data is coming from the web, then what looks like a hyphen might not really be a hyphen!

```{r}
exampleStrings = c("This is a regularly-occuring hyphen", 
                   "This is something that more closely--resembles a manually-produced em dash", 
                   "This is something that came from the darkest recesses of illâ€”conceived web design")
```

We can turn to our regular expression to see what we can do:

```{r}
gsub(pattern = "-", replacement = " ", exampleStrings)
```

Using gsub and just specifying a regular "-" will remove just about everything, except for whatever is in the last sentence. What is it? It looks like a hyphen, but it is not. To remove it, we will need to get creative:

```{r}
gsub(pattern = "(?!\\w)\\S", " ", exampleStrings, perl = TRUE)
```

By using some pattern matching, we are able to get out everything between words, no matter what it might be. This particular little bit of regex is using what is known as a negative lookahead. We have our main expression, the non-space character, \\S. Next, we put an expression in front of our main expression (this is the chunk in the parenthesis that starts with "?!"). When we read this, we would say, "don't match any word characters and then find non-whitespace characters."

While this is certainly a contrived example and would need additional robustness testing, it goes to show you that working with interesting data will yield interesting problems. I have been telling you all along that nothing is ever as it seems.

## Text Processing Tools

There are several R packages that will help us process text. The <span class="pack">tm</span> package is popular and automates most of our work. You already saw how we use the stemming and stopword removal functions, but <span class="pack">tm</span> is full of fun stuff and allows for one pass text processing.

```{r, eval = FALSE}

documents = c("I like to jump.", 
              "I have jumped my whole life.", 
              "Jumping is in my blood.", 
              "I am a jumper.")

documentCorp = SimpleCorpus(VectorSource(documents))

stopWordRemoval = function(x) {
  removeWords(x, stopwords("en"))
}

textPrepFunctions = list(removePunctuation,
                         stemDocument,
                         stopWordRemoval,
                         removeNumbers,
                         stripWhitespace)

tm_map(documentCorp, FUN = tm_reduce, tmFuns = textPrepFunctions)
```


Once you get your text tidied up (or even before), you can produce some visualizations!

```{r}
library(dplyr)

library(tidytext)

library(wordcloud2)

txExecutions = read.csv("txEx.csv", stringsAsFactors = FALSE)

txExecutions %>% 
  dplyr::select(correctedStatements, inmateNumber) %>% 
  unnest_tokens(word, correctedStatements) %>% 
  anti_join(stop_words) %>% 
  count(word, sort = TRUE) %>% 
  filter(n > 25) %>% 
  na.omit() %>% 
  wordcloud2(shape = "cardioid")
```


## Sentiment Analysis

Sentiment analysis is commonly used when we want to know the general *feelings* of what someone has written or said. Sentiment analysis is commonly seen applied to Twitter and other social media posts, but we can use it anywhere where people have written/said something (product reviews, song lyrics, final statements).

Sentiment can take many different forms: positive/negative affect, emotional states, and even financial contexts.

Let's take a peak at some simple sentiment analysis.

### Simple Sentiment

Let's consider the following statements:


```{r}

library(tidytext)

statement = "I dislike programming, but I really love R."

tokens = data_frame(text = statement) %>% 
  unnest_tokens(tbl = ., output = word, input = text)

tokens
```

From there, we get every individual *token*. This brings us to a quick, but necessary, detour. When we are talking about text, tokens are simply elements that have some general meaning. We typically associate tokens with individual words, but we could go deeper than that (spaces, punctuation, *n*-grams). While we won't dive into them too deeply, *n*-grams are also interesting. Just like our typical notion of *n*, an *n*-gram is an *n* length group of words. We can set *n* to be anything, but we would typically look at 2-gram and 3-grams chunks.

```{r}
tokenizers::tokenize_ngrams(statement, n = 2)
```

These are helpful for looking at frequently occuring combinations of words. They are also going to be helpful for us in just a bit.

Now back to the matter at hand.

Now, we can compare the tokens within our statement to some pre-defined dictionary of positive and negative words.

```{r}
library(tidyr)

tokens %>%
  inner_join(get_sentiments("bing")) %>% 
  count(sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative)
```

When we use Bing's dictionary, we see that we get one positive word (love) and negative word (dislike) with a neutral overall sentiment (a sentiment of 0 would indicate neutrality, while anything above 0 has an increasing amount of positivity and anything below 0 has an increasing amount of negativity).

Do you think that disklike and love are of the same magnitude? If I had to make a wild guess, I might say that love is stronger than dislike. Let's switch out our sentiment library to get something with a little better notion of polarity magnitute.

```{r}
tokens %>%
  inner_join(get_sentiments("afinn"))
```

Now this looks a bit more interesting! "Love" has a stronger positive polarity than "dislike" has negative polarity. So, we could guess that we would have some positive sentiment.

If we divide the sum of our word sentiments by the number of words within the dictionary, we should get an idea of our sentences overall sentiment.

```{r}
tokens %>%
  inner_join(get_sentiments("afinn")) %>% 
  summarize(n = nrow(.), sentSum = sum(score)) %>% 
  mutate(sentiment = sentSum / n)
```

Our sentiment of .5 tells us that our sentence is positive, even if only slightly so.

While these simple sentiment analyses provide some decent measures to the sentiment of our text, we are ignoring big chunks of our text by just counting keywords.

For example, it is probably fair to say that "really love" is stronger than just "love". We might want to switch over to some techniques that consider *n*-grams and other text features to calculate sentiment.


### Smarter Sentiment Analysis

When we use sentiment analysis that is aware of context, valence ("love" is stronger than "like"), modifiers (e.g., "really love"), and adversative statements ("but,...", "however,..."), we get a better idea about the real sentiment of the text.

We will use the *jockers* sentiment library, but many more available. Depending on your exact needs, there are some dictionaries designed for different applications. A prime example, and one that should be near to our hearts, is the Loughran and McDonald dictionary for financial documents (e.g., SEC 10K filings). Tim Loughran and Bill McDonald are superstars in the Finance Department at Notre Dame! There are dictionaries designed to measure certain attitudes and opinions (e.g., disgust, excitedness, sadness) and even dictionaries to measure emoji sentiment.

Before we engage in our whole sentiment analysis, let's take a look at a few things.

Here is the dictionary that *jockers* will use.

```{r}
lexicon::hash_sentiment_jockers
```

You might want to use <span class="func">View</span>() to get a complete look at what is happening in there.

We should also take a peak at our valence shifters:

```{r}
lexicon::hash_valence_shifters
```

With all of that out of the way, let's get down to the matter at hand:

```{r}
library(sentimentr); library(lexicon); library(magrittr)

statement = "I dislike programming, but I really love R."

sentiment(statement, polarity_dt = lexicon::hash_sentiment_jockers)
```

We can see that we get a much stronger sentiment score when we include more information within the sentence. While the first part of our sentence starts out with a negative word (dislike has a sentiment value of -1.6), we have an adversarial "but" that will downweight whatever is in the initial phrase and then we will have the amplified (from "really") sentiment of "love" (with a weight of 3.2 in our dictionary).

With all of this together, we get a much better idea about the sentiment of our text.


### Example Text

While the text that we have used so far serves its purpose as an example quite well, we can always take a look at other written words.

Let's consider the following data:

```{r}

txExecutionsShort = txExecutions %>% 
  dplyr::select(inmateNumber, correctedStatements) %>% 
  filter(inmateNumber == 529|
           inmateNumber == 843|
           inmateNumber == 714|
           inmateNumber == 999253)


sentiment(get_sentences(txExecutionsShort), 
          polarity_dt = lexicon::hash_sentiment_jockers) %>% 
  group_by(inmateNumber) %>% 
  summarize(meanSentiment = mean(sentiment))
```

## Other Text Fun

Sentiment analysis is always a handy tool to have around. You might also want to explore other descriptive aspects of your text.

The <span class="pack">koRpus</span> package allows for all types of interesting types descriptives. There are a great number of readability and lexical diversity statistics (Fucks is likely my favorite).

We need to tokenize our text in a manner that will please <span class="pack">koRpus</span>.

```{r}
library(koRpus)

statementTokens = lapply(txExecutionsShort$correctedStatements, function(x) tokenize(x, 
                           format = "obj", lang = "en"))

statementTokens
```


```{r}
lapply(statementTokens, function(x) readability(x, index = "Flesch.Kincaid", quiet = TRUE))
```
