---
title: "Survey Analyses"
output:
  distill::distill_article:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Package Install

```{r, eval = FALSE}
install.packages(c("dplyr", "psych", "mirt", "ltm", "ggplot", "GGally", 
                   "viridis", "ggridges"))
```


# Sample Size Planning

We will discuss two types of sample size planning techniques: population and power.

## Population

If we are surveying a group and want to know our confidence about the response, we need to have a general idea about the number of respondents that we would need. To do that, we need the following:

- The standard deviation of response (generally set at .5 for flexibility)

- The margin of error / confidence interval (2.5% and 5% are popular values)

- The confidence z-score (90% = 1.645; 95% = 1.96; 99% = 2.576)

Once we know those elements, we can plug them into the following formula:

$$\frac{Z^2 * \sigma(1-\sigma)}{\sigma / CI^2}$$

So if we wanted to keep our $\sigma = 0.5$, $CI = 5$, and $Z = 1.96$, we would have the following:

```{r}
stDev = .5

ci = .05

z = 1.96

(1.96)^2 * (.5*.5) / .05^2

sampleSize = ((z)^2 * (stDev * (1 - stDev))) / ci^2


sampleSize
```

If 47% of our 385 people responded a certain way (perhaps they supported a proposal), then we could say that we are 95% sure that between 42% and 52% of the entire population would have responded the same.

If we are dealing with a finite sample (one were we know the population size), we can do the following:

$$\frac{n * N}{n + N - 1}$$

```{r}
n = sampleSize

N = 1000

populationCorrection = (n * N) / (n + N - 1)

populationCorrection
```


# Power

Power is ability to detect an effect.

- In NHST words, we are trying to determine if we correctly reject the null hypothesis.

- Type I errors: Reject a true $H_{o}$ (false positive -- saying something is there when it is not)

- Type II errors: Reject a false $H_{o}$ (false negative -- saying something is not there when it is)

<a href="https://i2.wp.com/flowingdata.com/wp-content/uploads/2014/05/Type-I-and-II-errors1.jpg?resize=620%2C465&ssl=1">Relevant image</a>

## Power Analysis

We need three of the following parameters:

-  Effect size

-  Sample size

-  Significance level

-  Power

We **should** always be doing this *a priori*, but sometimes it is fun to be a "statistical coroner".

## Putting It All Together

Let's use the pwr package.

```{r}
library(pwr)

pwr.f2.test(u = 1, v = NULL, f2 = .05, power = .8)

```

In the function: 

- u is the numerator df (*k* - 1)

- v is the denominator df (*n* - *k*) 

- f2 is the effect size

- \(\Pi = 1 -\beta\)

- \(\beta = Type\,II_{prob}\)

Power is typically set at .8, because it represents a 4 to 1 trade between Type II and Type I errors.

<aside>
The *f^2* effect size is $\frac{R^2_{adj}}{1 - R^2_{adj}}$
</aside>

## Different Test, Different Power Tests

We just did a test for a linear regression model.

Here is one for a *t*-test:

```{r}
tPower = pwr.t.test(n = NULL, d = 0.1, power = 0.8, 
                    type= "two.sample", alternative = "greater")

plot(tPower)
```


# Factor Analysis

## Measurement & Latent Variables

If I asked you your height, could you tell me? How do you know your height and what allowed you to know your height? Like most, you have probably been measured at the doctor's office with some device marked with inches.

Now, if I were to ask you how you felt, what would you tell me? You might say that you are feeling well or not feeling too well, but how did you measure this? Did you use your wellness measuring tape or your wellness balance beam -- unlikely. Instead, you likely thought about everything going on -- your current health, general mood, and whatever else you deemed important at the time. Put another way, I could essentially be asking your about your affect (mood). Unfortunately, we can't measure your affect like we can measure your height. There is no agreed upon method for measuring your current affect (there isn't even an argument between Imperial and metric to be had).

Affect is an example of a latent variable -- we have a pretty good idea that it exists and we have operationalized it, but there is no way that we can physically measure it. If we think of what latent tends to mean, hidden, we start to understand what latent variables are -- variables that are hidden, but still interesting. Instead, we have to rely on a series of questions that gets us close to what we think is affect. This process of "getting at" affect through questions is what is known as *measurement*. You have been involved with measurement for a very long time, even if you did not know it. If you ever took a high-stakes standardized test (e.g., GRE, GMAT, LSAT), then you were directly involved in measurement -- we can't actually measure the mass of your mathematical reasoning ability, so we have to ask questions to measure that ability.

An important part of measurement concerns error. Error can come from many different sources (measure, person, environment, etc.) and "distorts" the observed score.

### On Principal Compenents Analysis And Factor Analysis

You might have already learned about a technique used for taking items and reducing them down -- principal components analysis (PCA). People will even tell you that they are the same thing. The logical question is are factor analysis and pca the same things? They are both matrix factorization techniques, but the similarities start to end pretty quickly after that.

  - Causal direction: In PCA, the items are the cause of the component (think how ingredients combine to make a pancake -- the pancake exists because the ingredients exist). In factor analysis, the latent factor gives rise to the items (i.e., the items reflect the construct -- questions for mathematical reason exist because there is a construct for mathematical reasoning that exists beyond the existence of the items).
  
  - Component/Factor Interpretation: These is no interpretation of a component -- it simply exists to reduce the variables. Factors represent the latent variables are important.
  
  - Variance: PCA attempts to extract as much variance as possible from the items. Each succesive component after the first will extract less variance than the first
  
  - Measurement error: PCA assumes perfect measurement

The psyc package will make life easy for us. Let's see what these differences mean in terms of our output:

```{r}
library(dplyr)

library(psych)

testData = bfi %>% 
  select(starts_with("A", ignore.case = FALSE), 
         starts_with("C"))

testPCA = principal(r = testData, nfactors = 2, rotate = "none")

testFA = fa(r = testData, nfactors = 2, rotate = "none")

testPCA

testFA
```

While items might load on the same factor/component, the magnitudes are much different. We also see differences in communality (h2 -- item correlations with all other items) and item uniqueness (u2 -- variance that is unique to the item and not the factor/component). We can also see that our PCA extracted more variance from the items than did our EFA (.48 to .36).

## Determining Factor Numbers

Determining the appropriate number of factors in an exploratory factor analysis can be complex. How many might theory dictate? Is there a strong theory at all? In an exploratory setting, it is helpful to conduct something called a parallel analysis (PA). PA is a Monte Carlo simulation that takes into account the number of items and rows within your data and then produces a random matrix of the same shape.

```{r}
psych::fa.parallel(testData)
```

This is the most automated way of finding a potentially suitable number of factors. We are given some output appropriate for both factor analysis and principal components analysis, with a graphical representation provided. In this case, parallel analysis would suggest 4 factors to be retained for a factor analysis. If some theory exists, you can also use that information to guide your factor numbers. 

While you will often get some different results, you can also use the nfactors function:

```{r, eval = TRUE}
testData %>% 
  na.omit() %>% 
  cor() %>% 
  psych::nfactors()
```

Now we are provided a variety of different metrics for determining the number of factors to retain. We can see that the suggested number of factors range from 2 (very simple structure with complexity 1 and MAP) to 5 (very simple structure with complexity 2). Given the results of our parallel analysis and our sample size adjusted BIC, we could make a good argument for retaining 4 factors.

## Rotation

You might remember how we rotate axes to try to get our components to fit together better. Generally, PCA forces an orthogonal rotation -- in other words, the vertices will always maintain 90s. Factor analysis, will allow for orthogonal rotations, but also oblique rotations (the vertices can go above or below 90). While pictures certainly help, there is something very important at play here. As you remember from PCA, an orthogonal rotation would hold that the factors (or components) are not correlated. For components, this makes sense. For our scale, however, would you guess that the items being analzyed are not correlated? By using an oblique rotation, we are allowing the factors to be as correlated as necessary.

Our rotations, though not terribly different, will produce different loading patterns.

```{r}
orthRotation = fa(r = testData, nfactors = 2, rotate = "varimax")

obliqueRotation = fa(r = testData, nfactors = 2, rotate = "promax")

orthRotation

obliqueRotation
```

We can see that our oblique rotation produces a correlation of .34 between the two factors. I will leave it up to you to determine if that is a strong enough correlation to warrant an orthogonal rotation. An orthogonal rotation, while limiting the correlation between factors, does produce "cleaner" results. If the factors are not correlated, we can interpret each one in isolation. In an oblique rotation with correlated factors, we have to interpret the factors simultaneously and consider how items loading on multiple factors behave. 

## Factor Loadings

We can think of factor loadings as the correlation between an item and a factor -- they are interpreted in this manner.

Let's take a peak at the factor loadings of our oblique solution from above:

```{r}
obliqueRotation$loadings
```

We can see that our output is hiding values of small magnitude (refer to the previous results for all values) -- this is for ease in examining the loading matrix and we could consider these as neglible loadings given the weakness of the loading. We can read these very much in the same way that we read correlations. For factor 1 (MR1 in the output), we see that items C1 through C5 load pretty strongly (with A4 having a weak loading). So, someone who offers a high response on C5 might have lower values of whatever latent trait is being measured, while they might have high values of the latent trait if they respond with a higher value on question C2.

## Factor Scoring

We have gone through the necessary steps of performing our factor analysis to this point, but what do we ultimately get out of it? In the end, we want to take our factors and produce some type of score.

What kind of score should we produce? If we use the numeric values given by the observed variables (e.g., 1-5, 0-4), then we can imagine producing some type of aggregate score (i.e., a sum or an average score). If you ever took an undergraduate Psychology course, you have probably already done something like this:

```{r}
testData = testData %>% 
  rowwise() %>% 
  mutate(agreeAverage = (sum(A1, A2, A3, A4, A5, na.rm = TRUE) / 5), 
         consAverage = (sum(C1, C2, C3, C4, C5, na.rm = TRUE) / 5))
```


Do those aggregate scores tell the complete truth? Remember how we just talked about loadings? The factor loadings are essentially telling us how much the variable is related to the factor. If we use a simple aggregate score, is that relationship captured? I am afraid not; aggregate scores are going to give the same weight to every item, regardless of how highly it loads on any given factor. What about cross-loadings? This type of aggregate scoring would completely ignore them!

Instead, we can use factor scores. There are several different types of factor scores that we can compute, but we are going to compute Thurstone scores. 

Thurstone score are incredibly easy to produce by hand, so let's give it a try.

To compute those scores, we need to produce a correlation matrix of our observed scores. For simplicity, let see what a one factor model would look like.

```{r}
agreeDat = bfi %>% 
  select(starts_with("A", ignore.case = FALSE))

agreeCorrs = cor(agreeDat, use = "pairwise")
```


Then, we need to get the loadings from our factor analysis results:

```{r}
agreeFA = fa(agreeCorrs, rotate = "promax", scores = "regression")

agreeLoadings = agreeFA$loadings[1:5, 1]
```

Now, we have everything that we need: item loadings, item correlations, and observed scores.

The first step is to get the matrix product between the inverse correlation matrix and the factor loading matrix:

```{r}
w = solve(agreeCorrs, agreeLoadings)
```


Finally, we center and scale the observed data, and do matrix multiplication for the product w that we just found:

```{r}
agreeScaled = agreeDat %>% 
  scale(., center = TRUE, scale = TRUE)

facScores = agreeScaled %*% w

head(facScores)
```


We can check these against those returned from our results:

```{r}
head(predict(agreeFA, agreeDat))
```

Now, let's see how well those scores are correlated with simple aggregate scores.

```{r}
simpleScores = agreeDat %>% 
  mutate(simpleScore = rowMeans(.))
```

```{r}
cor(facScores[, 1], simpleScores$simpleScore, use = "complete")
```

We can see that the scores are certainly correlated, but the factor scores are likely closer to *true scores*.

## Reliability

Factor analysis falls under a broader notion of what is called *Classical Testing Theory* (CTT). In CTT, the notion of reliability is of supreme importance. Reliability is most easily defined as being able to produce the same result from a measure. If I give you the same measure a few months apart and you score similarly on both measures, then we are likely dealing with a reliable test. It is important to note that reliable is not the same as valid. Validity, with regard to measurement, is knowing that our measure is measuring what we intend it to measure (e.g., our measure of likelihood to engage in whistleblowing is actually measuring a person's likelihood to blow the whistle). A measure can be reliable without being valid -- remember it just has to produce the same results time after time. 

Assessing reliability is generally an easy task. The most simple version of reliability, is Cronbach's measure of internal consistency, or $\alpha$. Cronbach's $\alpha$ is simply the best split-half correlation within a measure (internal consistency means that the whole measure is reliable within itself). 

Getting $\alpha$ is easy enough:

```{r}
psych::alpha(agreeDat, check.keys = TRUE)
```

We get a fair chunk of output, but let's just focus on the "std.alpha" value of .71. By most "rules-of-thumb", anything of .7 is acceptable for general purpose use. If we were wanting to make very important decisions with our measure, we might look for something approaching or exceeding .9. 

You likely noticed the "check.keys = TRUE" arguement -- alpha is bound by test length and item correlations. In a case where some items are negatively correlated, that will greatly reduce our $\alpha$ coefficient. Let's try it for ourselves:

```{r}
psych::alpha(agreeDat)
```

Very different, right?

Let's calculate it on our own. We will need to recode our negatively correlated variable (A1) first.

```{r}

agreeDatRecode = agreeDat %>% 
  mutate(A1 = abs(A1-7)) # This recodes our scale direction

nvar = length(agreeDatRecode)

# This computes the covariance matrix of our data.

C = cov(agreeDatRecode, use = "complete.obs")

n = dim(C)[2]

total = rowMeans(agreeDatRecode, na.rm = TRUE)

# The tr function is just adding everything on the 
# diagonal from our covariance matrix.

alpha.raw = (1 - psych::tr(C)/sum(C)) * (n/(n - 1))

alpha.raw
```

How did we do? We perfectly replicated our raw alpha value from the output. This is an easy example, but more elaborate ones don't take much more effort. 

We did all of this to say that $\alpha$ helps us to know how consistent a one factor measure is within itself. In many circumstances where you find yourself with a unidimensional factor structure, $\alpha$ will be more than sufficient for convincing people that your scale is reliable (weaknesses aside).

There are, however, many other forms of reliability ($\alpha$ is based off of something else called the Kuder-Richardson 20). One popular and conceptually helpful form of reliability is McDonald's $\omega$. McDonald's $\omega$ is well-suited for hierarchical factor structures, in which you might have various subscales that assess something bigger. 

Earlier, we played with the bfi data. Let's return to that, but with a hypothesis that the 5 factors of the bfi are actually subfactors of some larger latent personality variable.

```{r}
bfiSubFactors = bfi %>% 
  select(-gender, -education, -age)

omegaTest = psych::omega(bfiSubFactors, nfactors = 5, 
             rotate = "promax", plot = FALSE)

omega.diagram(omegaTest, sl = FALSE)
```

This is what our factor structure would look like in a hierarchical fashion. We have the general factor (g) and our 5 subfactors (also called grouping factors). We can see how strongly our subfactors load onto our general factor with the provided values. We can also see how strongly each individual items loads onto a subfactor.

We can also look at some reliability values (and a cleaner loading matrix):

```{r}
omegaTest
```


While we see that we can get an $\alpha$ value, we also have Guttman's $\lambda_6$, and 3 different $\omega$ values that are of interest to us. Since we are dealing with something of a hierarchical nature, we already know that $\alpha$ and $\lambda_6$ won't be appropriate.  $\omega_h$ gives us an idea about the reliability of the general factor (perhaps not super in this case) and $\omega_t$ gives us the total reliability of the test (very good). 

We can also see item loadings for the general factor and each subfactor. Generally, we find that the items that we would anticipate loading together (e.g., all N items load together, and all O items load together), along with some cross-loading items from other subfactors.

# IRT

## Item Response Theory And Factor Analysis

While we are still dealing in the world of latent variables, we are taking a new approach to measuring those latent variables -- Item Response Theory (IRT). While both allow us to measure a person's level of a latent trait, IRT gives us the power to understand the person and the items. We can recall that factor analysis falls under the broader heading of Classical Test Theory (CTT) -- the name is a clear indication that the focus is on the test as a whole, not individual items. 

In addition to this broadened understanding, IRT has a different focus from a measurement perspective than factor analysis. Perhaps most important is the notion of measurement error in IRT. In factor analysis, measurement error is assumed to be consistent across each item; this is not the case for IRT.

## Dichotomous And Polytomous Models

Before we dive into some of the more important parts of IRT, it is important to draw some distinctions between two broad families of models: dichotomous and polytomous. A dichotomous model can best be thought of in testing situations where a correct answer exists (you either get the question correct or not, so the outcome is dichotomous). With that knowledge, you might have already guessed what the polytomous model entails -- items that have ordered responses with no clear correct choices (Likert response options would be a good example). Polytomous models include such models as the graded response model and the partial credit model.

We will see how these models behave differently as we continue discussing the different parts of IRT models.

## Ability

In IRT, we talk a person's ability; essentially it is the level of the latent variable that a person possesses. In the parlance of psychometrics, ability is denoted as $\theta$. The $\theta$ scale is standardized with a mean of 0 and tends to go from -4 to 4 (while this is generally seen, it might not always be the case). 

## Different Parameters

The ability to explore individual items within IRT is based upon three different parameters: location, discrimination, and psuedoguessing. We can incorporate any of those parameters into our model. First, though, we need to understand what those parameters mean.

### Location

In IRT, the location parameter (denoted as $\\b_i$) described the difficulty of the item. If we are talking about a testing situation (e.g., a math or science test), the meaning of $\\b_i$ is nearly self-evident -- it is simply the probability of a correct response to the question in a dichotomous model and the probability of moving from one category to another in polytomous models (the level of passing from one category to the next is called a *threshold*).

Let's look at some examples from the <span class="pack">mirt documentation of different item locations. The <span class="pack">ltm package is also a great package for IRT models.

```{r}
library(mirt)

library(ltm)

# The model arguement set to 1 means we are just looking
# at a one factor model.

fit1PL = mirt(LSAT, model = 1, itemtype = "Rasch", verbose = FALSE)

plot(fit1PL, type = 'trace', which.items = 1:5, 
     facet_items = FALSE)

```


The resulting plot is what is known as an *Item Characteristic Curve*. It contains a lot of information, but we are only going to pay attention to the location right now. Without any prior knowledge, which of the labelled curves would say represents the most difficult item? If you said "3" (the black line), then you are absolutely correct. When we look at the location of an item in an ICC graph, we are looking for its placement along the x-axis around its middle point (i.e., find the middle of a curve and draw a line straight down -- the farther to the right, the more difficult the item). With item 3, we can see that it takes someone with slightly more than average ability to have a better than chance probability of responding correctly.  


### Discrimination

Item discrimination, the $a_i$ parameter, demonstrates the likelihood of people with varying degrees of ability to respond correctly.

Let's specify a slightly different model and take a look at our ICC again:

```{r}
fit2PL = mirt(LSAT, model = 1, itemtype = "2PL", verbose = FALSE)

plot(fit2PL, type = 'trace', which.items = 1:5, 
     facet_items = FALSE)
```


In our last model, we saw that all of the items had the exact same curve -- this is no longer the case. If we pay attention to item 3 again, we see that a person with very low ability ($\theta = -4$) has nearly a probability of 0 to respond correctly, someone with average ability ($\theta = 0$) has around a 50-50 chance of responding correctly, and high ability people ($\theta = 4$) have nearly 100% chance of responding correctly. We would definitely say that item 3 discriminates between different ability levels pretty well (we will see if it is the most discriminating item later). Which item might not discriminate too well? Without doubt, you said item 1, and you would be correct. While people at lower ability levels have less chance of responding correctly to item 1, we can see that a probability of 1 is obtained pretty soon after crossing $\theta = 0$. So while item 1 may provide a bit of discrimination at the lower end of $\theta$, it does nothing when dealing average or more. This provides even more evidence as to why item 3 is such a good item -- it discriminates very well along every point of $\theta$

So, we have a pretty good idea that item 3 is the most difficult item and the most discriminating item. This, hopefully, provides a demonstration on why IRT models are so powerful -- we can compare individual items to people (on the same scales, no less).

### Guessing

The third parameter, $c_i$, is the psuedoguessing parameter. Even the blind squirrel finds a nut and $c_i$ accounts for this. Essentially, the guessing parameter offers a slight penalty for the chance of guessing a correct answer.


## Models And Parameters

### 1PL

Now that we know what our parameters mean, we can start putting them into models. The one parameter logistic model (1PL) only estimates item difficulty ($b_i$). It assumes that all items discriminate equally and that all items have the same guessing parameter. 

Although some minor differences, you will often see the 1PL and Rasch model used interchangibly.

### 2PL

The two parameter logistic model (2PL) adds the discrimination parameter, $a_i$, into the model.

### 3PL

The three parameter logistic model (3PL) adds in the guessing parameter, $c_i$. You might wonder how in the world a guessing parameter is added! If someone could guess the correct answer (think about a multiple choice test), would the probability of a correct answer ever be 0? I would think not. So, $c_i$ just includes a lower-bound asymtote. 

```{r}
fit3PL = mirt(LSAT, model = 1, itemtype = "3PL", verbose = FALSE)

plot(fit3PL, type = 'trace', which.items = 1:5, 
     facet_items = FALSE)
```

If we compare the ICC plots we saw for the 1PL and 2PL earlier, we see a a significant difference between the lower asymptote.

```{r}
par(mfrow = c(1, 3))

plot(fit1PL, type = 'trace', which.items = 1:5, 
     facet_items = FALSE, main = "1PL")

plot(fit2PL, type = 'trace', which.items = 1:5, 
     facet_items = FALSE, main = "2PL")

plot(fit3PL, type = 'trace', which.items = 1:5, 
     facet_items = FALSE, main = "3PL")
```

While we see that the probability for a low ability individual obtaining a correct answer for item 3 is still very low, it has improved slightly.

With all of these parameters, the probability of a correct response can be expressed as follows: $$p_i(\theta) = c_i + \frac{1 - c_i}{1 + e^{-a_i(\theta - b_i)} }$$


## Scoring

Scoring in IRT models works a bit differently than scoring in CTT. 

An element common to both IRT and CTT is the *true score*, $true_{score} = observed_{score} + e$. The distinction rests in how they treat $e$ -- CTT assumes that error is the same for everyone and IRT allows for error to vary freely across people.

IRT scores also have the added benefit of using the item response functions in the computation of scores. 

We can see the factor scores returned from our model:

```{r}
facScores = fscores(fit3PL)

head(facScores)

summary(facScores)

```

And let's see what scores from a factor analysis would look like compared to these scores:

```{r}
faScores = psych::fa(LSAT, 1, rotate = "promax")$scores

summary(faScores)

cor(faScores, facScores)

plot(faScores, facScores)
```

We can see some very clear similarities and an undeniable correlation between our two different factor scores. While certainly similar, we can be pretty confident that our IRT scores are closer to *true scores*. 

We can also put those scores back into our data to see how they correlate with the other items:

```{r}
library(dplyr)

LSAT %>% 
  mutate(scores = facScores) %>% 
  cor() %>% 
  GGally::ggcorr()
```

We can see that overall scores are most correlated with item 3, followed by item 2, and item 4. If we recall our ICC plot from above, this probably should not surprise us too much -- item 3 is definitely the best item, while items 1 and 5 don't look as nice.

### Different Measurement Levels

So far, we have discussed models that are largely dealing with dichotomous variables (largely incorrect and correct). These are aptly names dichotomous models. However, we know that not all measures have a binary response. If we have multiple response options (perhaps a Likert response option format), then we need to use a polytomous model. One common model is the Graded Response Model (GRM). 

The GRM is used with the express purpose of ordinal, polytomous response options.

Let's take a look:

```{r}
library(dplyr)

library(ggplot2)

library(ggridges)

library(psych)

library(viridis)

testData = bfi %>% 
  dplyr::select(starts_with("C")) %>% 
  na.omit()

testData %>% 
  tidyr::gather(key = question, value = score) %>% 
  ggplot(., aes(score, question, fill = ..x..)) +
  geom_density_ridges_gradient() +
  scale_fill_viridis(option = "B") +
  theme_minimal()

grmMod = mirt(testData, model = 1, itemtype = "graded", verbose = FALSE)

coef(grmMod, simplify = TRUE, IRTpars = TRUE)

```

In our coefficient output, we see coefficients for each item (b1:b5). These coefficients detail the *thresholds* for moving from one response level to the next (we also see the discrimination value). 

Just like our dichotomous models, we can also see our item characteristic curves.

```{r}
plot(grmMod, type = 'trace', which.items = 1:5, 
     facet_items = TRUE, main = "GRM")
```

We can see that these are very different looking ICC plots then what we saw before. Remember, that we are dealing with different models here. Instead of the probabililty of incorrect or correct, we are now dealing with the probability of moving from one response category to the next. To that end, each item's ICC has a curve for each response option over ability. If we look at the ICC for C1, we can see that someone offering a response of 1 on the scale would have a higher probability of having a lower ability. Conversely, someone offering a response of 6 would have a higher probability of being high ability.

# Mixed Effects Models

## Main Points

1.  Mixed models are used when you have some type of grouping variable within your data (e.g., departments).

2.  They can also be used when you have a repeated measurement over the same person (e.g., testing).

3.  Mixed models can also incorporate information from both grouping variables and repeated measurements (e.g., yearly job satisfaction across departments).

## Packages

We will need the following:

```{r, eval = FALSE}
install.packages(c("lme4", "lmerTest", "merTools"))
```


## Terminology

You will hear "mixed models", "mixed effects models", "hierarchical linear models", "nested models", and/or "multilevel models"; these are all slight variations on a common theme. For the sake of our work here, we will keep it at mixed models. Within our mixed model, we have an additional source of cloudiness: fixed and random effects. The random effects don't pose much of an issue (we will define it later), but fixed effects have 4 different definitions depending upon whom you ask (you will commonly see it used for panel or time-series data when people want to remove the variation of a group or individual). For the sake of simplicity (again), we are going to consider fixed effects as an effect on the individual unit of analysis. This will all start to make sense once we take a look at the models.

## Equations 

Let's assume that we are going to try to predict sales for a group of stores:

$$sales = \beta_{intercept} + \beta_{rebate}*Rebate + \epsilon$$

The error term would assumed to be normally distributed with a mean of 0 and some standard deviation:

$$\epsilon \sim \mathscr{N}(0, \sigma) $$

With that, we can take those equations and shuffle them around a bit to capture the data generation process for sales:

$$sales \sim \mathscr{N}(\mu, \sigma)$$

$$\mu = \beta_{intercept} + \beta_{rebate}*Rebate$$

Now with that, we can add the effects for another variable at a higher level (i.e., the effects of store):

$$sales = \beta_{intercept} + \beta_{rebate}*Rebate + (effect_{store} +  \epsilon)$$

And look at the error terms:

$$ effect_{store} \sim \mathscr{N}(0, \tau)$$

Where we can say that the effect of store is normally distributed with a mean of 0 and some standard deviation. 

And rolling it all together, we get:

$$sales = (\beta_{intercept} + effect_{store}) + \beta_{rebate}*rebate + \epsilon)$$

## Why Mixed Models

As mentioned earlier, we will get improved standard errors.

Mixed models don't get bogged down by large groups (i.e., large groups do not dominate the inference) and the smaller groups do not get buried by the larger groups. 

Mixed models will not overfit or underfit when we have repeated samples/measurement. We also get improved estimates for repeated measures when dealing with mixed models.

Our models retain the variation inherent within the data when we used a mixed model.

## Visually Explained

```{r, echo = FALSE}
library(ggplot2)

visualizationData = readr::read_csv("https://www.nd.edu/~sberry5/data/visualizationData.csv")

names(visualizationData) = c("spent", "visits", "adSource")

visualizationData$adSource[visualizationData$adSource == "cancer"] = "print"

visualizationData$adSource[visualizationData$adSource == "cardiac"] = "radio"

visualizationData$adSource[visualizationData$adSource == "general"] = "web"

ggplot(visualizationData, aes(spent, visits)) + 
  geom_point() +
  geom_smooth(method = "lm") + 
  theme_minimal()
```

That is a pretty clear effect for the number of visits and the total spent.

But let's look a bit further:

```{r, echo = FALSE}
ggplot(visualizationData, aes(spent, visits, color = adSource)) + 
  geom_point() +
  geom_smooth(method = "lm") + 
  theme_minimal()
```

This changes things entirely!

## Standard Linear Model

For the sake of conceptual grounding, let's do some data prep and go back to our standard linear model:

```{r}
library(dplyr)

library(ggplot2)

performanceReview <- readr::read_csv("http://www.nd.edu/~sberry5/data/performanceReviewData.csv")

lmTest <- lm(performanceScore ~ Manager + age, data = performanceReview)

summary(lmTest)

ggplot(performanceReview, aes(performanceScore, age, color = as.factor(Manager))) +
  geom_point() + 
  theme_minimal()
```

We have our standard output here. As before, our intercept is the average score when everything is at 0, we have a categorical variable (Manager) with simple constrasts, and the continuous predictor of age. 

Let's see how some additional information changes our viz:

```{r}
ggplot(performanceReview, aes(performanceScore, age, color = as.factor(Manager))) +
  geom_point() + 
  facet_wrap( ~ idBuilding) +
  theme_minimal()
```

## Random Intercepts Model

Let's include the department variable in our model. We are not going to add it as another predictor, but we are going to include it as another level to our model. The `lme4` package will make this very easy. We will start with an intercept-only model:

```{r}
library(lme4)

intMod <- lmer(performanceScore ~  1 + (1|idDepartment), data = performanceReview)
```

Before we look at the summary for this model, let's get an idea about what is happening in the syntax. The first part of our formula should look familiar -- these are the global estimates (fixed effects) within our model and will behave exactly the same as our standard linear model. We are just specifying an intercept here.

The next part in the parentheses is how we denote our random effect. Whenever you see a 1 included in a formula interface, we can be pretty comfortable that it is in reference to a intercept. The `|` specifies a grouping. With that information, we might be able to guess that we are specifying a random intercept for each borough. 

We should probably check out the summary:

```{r}
summary(intMod)
```

<aside>
You might notice that this is being fit by restricted maximum likelihood (REML). This is just a specific version of maximum likelihood estimation (MLE), which seeks to maximize a function so that the proposed model fits the data.
</aside>

We don't really get anything too interesting here for now. Let's do explore the random effects components just a little bit. Let's do the following with our variances:

$$\frac{\text{intercept variance}}{\text{intercept variance} + \text{residual variance}}$$

```{r}
5.22 / (39.09 + 5.22)
```

This is what is known as the intraclass correlation (ICC). It ranges from 0 (no variance between clusters) to 1 (variance between clusters). It can be thought of as how much the proportion of variance in scores is accounted for by the department alone. Computationally the ICC is as follows:

$$\rho = \frac{\tau^2}{\tau^2 + \sigma^2} $$

Where $\tau^2$ is the population variance between clusters and $\sigma^2$ is the population variance within clusters.

We can compute $\sigma^2$ as:

$$\sigma^2 = \frac{\Sigma(n_j - 1)S^2_j}{N-C}$$

Where $S^2_j$ (the variance within the cluster) is defined as:

$$\frac{\Sigma(y_{ij}-\bar{y}_j)}{(n_j - 1)}$$

where

$n_j$ = sample size for cluster *j*

*N* = total sample size

*C* = total number of clusters

The calculation of $\tau^2$ is as follows:

$$\tau^2 = \hat{S}^2_B - \frac{\hat{\sigma}^2}{\tilde{n}}$$

where

$$\hat{S}^2_B = \frac{\Sigma n_j(\bar{y_j - \bar{y}})^2}{\tilde{n}(C-1)}$$
where 

$\bar{y}_j$ = mean on response variable for cluster *j*

$\bar{y}$ = overall mean on response variable

$$\tilde{n} = \frac{1}{C-1} \begin{bmatrix}N-\frac{\Sigma n^2_j}{N}\end{bmatrix}$$

<aside>
This might seem very similar to computing the ANOVA sums of squares. It is similar, but not identical.
</aside>

We could do all of this work by hand and before running the model, but I'll let you do that on your own time if you really feel the need to do so.

We can add a predictor variable to the model:

```{r}
riMod <- lmer(performanceScore ~ Manager + age + (1|idDepartment), data = performanceReview)

summary(riMod)
```

When groups are largely balanced, we would find that our coefficients would be the same (or very close to it).

What should almost always change is our standard errors -- by integrating information about the groups, we are getting a better sense of how much uncertainty our model contains at the global average level.

We also see some additional information -- this is for our random effects. The standard deviation is telling us how much the score moves around based upon department after getting the information from our fixed effects (i.e., the score can move around nearly 2 whole point from department alone). 

We can get a good sense of the random effect estimates:

```{r}
ranef(riMod)
```


We can also use various bits of information in our output to create different *R^2* values. 

For the first level of the model, we can calculate the $R^2$ as:

$$R^2_11 - \frac{\sigma^2_{M1} + \tau^2_{M1}}{\sigma^2_{M0} + \tau^2_{M0}}$$

We can pull that information from our two model outputs:

```{r}
m1Sigma <- 4.899 # full model random effect intercept

m1Tau <- 2.028 # full model random effect residual

m0Sigma <- 5.22 # null model random effect intercept

m0Tau <- 39.09 # null model random effect residual

1 - ((m1Sigma + m1Tau) / (m0Sigma + m0Tau))
```

The fixed effects of manager and age account for nearly 85% of the variation within the scores.

The second level can be calculated as:

$$R^2_2 = 1 - \frac{\sigma^2_{M1} / B + \tau^2_{M1}}{\sigma^2_{M0} / B + \tau^2_{M0}}$$

We see that we added a *B* into the mix here and it is just the average size of the level 2 units (534 / 26). 

<!-- ```{r} -->
<!-- level2Mean <- 534 / 26 -->

<!-- r2Numerator <- m1Sigma / level2Mean + m1Tau -->

<!-- r2Denominator <- m0Sigma / level2Mean + m0Tau -->

<!-- 1 - (r2Numerator / r2Denominator) -->
<!-- ``` -->


We can also plot simulated random effect ranges for each of the random effect groups. We want to pay attention to those that are highlighted with black (i.e., the range does not cross the red line at 0). 

```{r}
library(merTools)

plotREsim(REsim(riMod), labs = TRUE)
```

In examining the plot, we can see a very broad range of department effects on scores, with most of them being significant (noted by the black dots).

Going back to the output, did you notice anything missing: *p*-values! Estimating *p*-values in a mixed model is exceedingly difficult because of varying group sizes, complete sample *n*, and how those relate to reference distributions. If you need something that will help, you can get confidence intervals in the same way that you would anything else:

```{r}
confint(riMod)
```

Those confidence intervals are interpreted how they always are, but we see two things that don't look familiar: .sig01 and .sigma. These are related to our random effects, where .sig01 relates to the random intercept

If you *really* want to see *p*-values, you can get them easily:

```{r}
riModP <- lmerTest::lmer(performanceScore ~ Manager + age + (1|idDepartment), data = performanceReview)

summary(riModP)
```

**NOTE:** I would never load the `lmerTest` package, but would attach with colons! If you load it, you will find that it masks things from lme4 that you don't want to have masked (i.e., lmer) and they are not equivalent objects!

Getting predicted values from our mixed model is no different then getting them from any other model:

```{r}
mixedPred <- predict(riMod)

slimPred <- predict(lmTest)

allPred <- cbind(actual = performanceReview$performanceScore, 
      mixed = mixedPred, 
      slim = slimPred)

head(allPred, 20)
```

While there were cases where the standard linear model did a slightly better job, our mixed model generally did a better job. Plotting these makes it even more clear:

```{r}
plot(allPred[, "actual"], allPred[, "slim"])

plot(allPred[, "actual"], allPred[, "mixed"])
```

Let's add some more information to our model. As we dive into our data, we will notice that we also have building variable. We can add this additional grouping into our model:

```{r}
clusterMod <- lmer(performanceScore ~ Manager + age + (1|idDepartment) + (1|idBuilding), data = performanceReview)
```

This is often called a cross-classified model. 

```{r}
summary(clusterMod)
```


```{r, fig.height=7}
plotREsim(REsim(clusterMod), labs = TRUE)
```


We should also see if our predictions improved:

```{r}
clusterPredict = predict(clusterMod)

allPred <- cbind(actual = performanceReview$performanceScore, 
           clustered = clusterPredict,
           mixed = mixedPred, 
           slim = slimPred)

head(allPred, 20)
```

For many observations, our predictions definitely go tighter, but there isn't a big difference between our two mixed models.

```{r}
plot(allPred[, "actual"], allPred[, "slim"])

plot(allPred[, "actual"], allPred[, "mixed"])

plot(allPred[, "actual"], allPred[, "clustered"])
```


## Hierarchical Models

Hierarchical models are a slight variation on the models that we have just seen. In these models, we have groups nested within other groups. We know that we have a department variable and a building variable. One source of inquiry would be to incorporate the nesting of this information.

The model set-up is just different enough to cause some potential confusion here. Within the parentheses, we have our intercept as before, but we are also saying that we are looking at the departments nested within buildings (it follows an A/B constructions).

```{r}
hierMod <- lmer(performanceScore ~ Manager + age +  (1|idBuilding/idDepartment), data = performanceReview)

summary(hierMod)
```

We have our fixed effect for observation and we have individual intercepts for both idBuilding and department nested within building. Looking at the standard deviation of our random effects, we can see that the scores bounce around a little over 1 point from from department to department within a building. 

We can also look at our effect ranges: 

```{r}
plotREsim(REsim(hierMod), labs = TRUE)
```

And checking our predictions again:

```{r}
hierModPredict = predict(hierMod)

allPred <- cbind(actual = performanceReview$performanceScore, 
                 hierMod = hierModPredict,
                 clustered = clusterPredict,
                 mixed = mixedPred, 
                 slim = slimPred)

head(allPred, 20)
```

For many observations, our predictions definitely go tighter, but there isn't a big difference between our two mixed models.

```{r}
plot(allPred[, "actual"], allPred[, "slim"])

plot(allPred[, "actual"], allPred[, "mixed"])

plot(allPred[, "actual"], allPred[, "clustered"])

plot(allPred[, "actual"], allPred[, "hierMod"])
```

For these predictions, it really does not look like anything changed at all. You might not see massive changes in fit when moving from model to model. The more important question, though, is your model reflecting the reality of your data?

## Random Slopes

Now that we have seen random intercepts and hierarchical models, we can add one final piece: random slopes. In the following model, we will specify a random intercept (recall, it is the 1 within our parenthesis) and a random slope (we are prefixing our grouping variable with a slope of interest). Not only will this model allow the intercept to vary between groups, but it will also allow the slope to vary between groups.

```{r}
randomSlopes <- lmer(performanceScore ~ Manager + age + (age|idDepartment), data = performanceReview)

summary(randomSlopes)
```

Nothing changes with regard to our fixed effects, but we get some added information in our random effects. The random intercept standard deviation for department is telling us the amount that the scores bounce around from place to place and the age variance is telling us how much variability there is within the slope between locations. 

We can look at it graphically:

```{r}
mutate(performanceReview, idDepartment = as.factor(idDepartment)) %>% 
ggplot(., aes(performanceScore, age, group = idDepartment, color = idDepartment)) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal()
  
```

When looking at this visualization, it becomes clear that there really is not too much difference in the slopes of age between departments here.

We can also fit that random effects model to our hierarchical model:

```{r}
observationModSlope <- lmer(performanceScore ~ Manager + age + (age|idBuilding/idDepartment), data = performanceReview)

summary(observationModSlope)
```


## The Alternative

If you don't want to go the mixed model route, the natural thing to do would be to conduct a regression for each individual group. Here is what that might look like:

```{r}
library(purrr)

indPredictions <- performanceReview %>% 
  split(., f = .$idDepartment) %>% 
  map_df(~ data.frame(predicted = predict(lm(performanceScore ~ Manager + age, data = .x)), 
                   score = .x$performanceScore)) 

plot(indPredictions$score, indPredictions$predicted)
```


## Longitudinal Data

If we continue to look at our data (and with some knowledge about how NYC does health inspections), we will see that restaurants are rated yearly -- let's use this information in our model. We won't worry about distance anymore, because now we have a few competing hypotheses. We could imagine two different ways that the works: one in which a restaurant's score improves as observations increase (it takes some time for the owner to get his staff fully up to speed) or one in which the score decreases as the observations increase (the "shine has worn off the penny").

Let's do a bit of data processing first.

```{r}
healthData <- readr::read_csv("https://www.nd.edu/~sberry5/data/healthViolationsDistances.csv")

healthData <- healthData %>% 
  mutate(BORO = as.factor(.$BORO), 
         cuisine = as.factor(.$`CUISINE DESCRIPTION`), 
         distanceCentered = dohDistanceMeter - mean(dohDistanceMeter))

healthDataGrouped <- healthData %>%
  tidyr::unite(col = nameLocation, DBA, BUILDING , remove = FALSE) %>%
  group_by(nameLocation) %>%
  arrange(lubridate::mdy(`GRADE DATE`)) %>%
  mutate(observation = 1:n())

timeReviewed <- healthDataGrouped %>%
  summarize(n = n()) %>%
  filter(n > 10)

reviewedRest <- healthDataGrouped[which(healthDataGrouped$nameLocation %in%
                                         timeReviewed$nameLocation), ]
```

Now we can fit a mixed model, with each location getting its own intercept.

```{r}
observationModInt <- lmer(SCORE ~ observation + (1|nameLocation), data = reviewedRest)
```

In this model, we have a fixed effect for observation and we are allowing each location to have it's own random intercept. We have essentially created a model that will deal with the repeated measures for each of the locations.

```{r}
reviewedRest %>%
  arrange(nameLocation, observation) %>%
  head(., 350) %>%
  ggplot(., aes(observation, SCORE, group = nameLocation)) +
  geom_point() +
  geom_smooth(method = "lm") +
  facet_wrap( ~ nameLocation) +
  theme_minimal()
```

```{r}
summary(observationModInt)
```

Our fixed effect here would indicate that we have a slight increase in scores as our observations increase, but we can also see that scores will bounce around about 5 points on average by location alone.

```{r}
29.46 / (29.46 + 125.82)
```

We see that the location alone accounts for nearly 20% of the variance in health scores.

### Adding Random Slopes

```{r}
observationModSlope <- lme4::lmer(SCORE ~ observation + (1 + observation|nameLocation), data = reviewedRest)

summary(observationModSlope)
```

The observation variance is telling us how much variability there is within the slope between locations

If we use the predicted values of our model, we can see what our results will look like over the observations:

```{r}
plottingData <- reviewedRest %>% 
  ungroup() %>% 
  mutate(mixedPrediction = predict(observationModSlope), 
         lmPrediction = predict(lm(SCORE ~ observation, data = .))) %>% 
  group_by(nameLocation)
  
ggplot() + 
  geom_line(mapping = aes(observation, mixedPrediction, group = nameLocation), 
            data = plottingData, alpha = .25) + 
  geom_smooth(mapping = aes(x = observation, y = lmPrediction), 
              data = plottingData, method = "lm", 
              color = "red") +
  theme_minimal()
```

Amazing! We can really see the varying intercepts for each restaurant and we can see how the slopes are completely different (some are similar, but we have a completely mixed bag of directions and magnitudes).

It looks like the standard regression line would do okay for many of our locations, but we can see that it would do poorly with many others. This should help to provide an illustration of just how flexible and powerful mixed model can be in your hands.

## Extensions

No matter what type of model you want to run, there is a mixed model extension for it. Any distribution can be used with the `glmer` function, nonlinear models can be fit with `nlme` from the *nlme*, and generalized additive models can be fit with `gamm` from mgcv.