---
title: |
      | Distributions
      | Sampling
output:
  html_document:
    toc: true
    toc_float: true
    theme: flatly
    highlight: zenburn
    css: documentCSS.css
---

```{r setup, include=FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = FALSE, comment = "")
```

## The Normal Distribution

The normal distribution should not be too much of a mystery to us.

```{r}
library(ggplot2)

set.seed(1001)

population = data.frame(population = rnorm(n = 1000000, mean = 0, sd = 1))

regions = data.frame(sdPlus1 = mean(population$population) + sd(population$population), 
                     sdMinus1 = mean(population$population) - sd(population$population), 
                     sdPlus2 = mean(population$population) + (2 * sd(population$population)), 
                     sdMinus2 = mean(population$population) - (2 * sd(population$population)), 
                     sdPlus3 = mean(population$population) + (3 * sd(population$population)), 
                     sdMinus3 = mean(population$population) - (3 * sd(population$population)))

ggplot(population, aes(population)) +
  geom_density() +
  theme_minimal()

```


If we are observing a population that is normally distributed, we can know some things about it: the mean and the standard deviation. We also know that the mean, median, and mode are all the same. 

There is also a convenient rule: the 68-95-99.7 rule. This rule dictates that 68% of the distribution is contained within $\pm1\sigma$, 95% is contained within $\pm2\sigma$, and 99.7% is contained within $\pm3\sigma$. It is not functionally part of the rule, but 99.99% is contained under $\pm4\sigma$.

```{r}
ggplot(population, aes(population)) +
  geom_density() +
  geom_vline(xintercept = regions$sdPlus1, color = "red") +
  geom_vline(xintercept = regions$sdMinus1, color = "red") +
  geom_vline(xintercept = regions$sdPlus2, color = "blue") +
  geom_vline(xintercept = regions$sdMinus2, color = "blue") +
  geom_vline(xintercept = regions$sdPlus3, color = "green") +
  geom_vline(xintercept = regions$sdMinus3, color = "green") +
  theme_minimal()
```


The normal distribution is important, as many things are naturally normally distributed.

If we look at the density plots above, we can see that we are likely dealing with a standard normal distribution ($\mu=0$, $\sigma=1$). Although not requisite, we might want to standardize our variables to fit a standard normal distribution: $z = \frac{x_i-\mu}{\sigma}$. Transforming variables into *z*-scores makes it easy to compare values.

For example, we might take a person who scored a 3.2 on the auditor exam (CIA) in 2015. In 2014, a different auditor also scored a 3.2. Clearly, they both scored the same; however, the story should not end there. Let's consider the following information:

In 2014, the CIA had $\mu=3.04$ with a $\sigma=1.41$.

In 2015, the CIA had $\mu=2.86$ with a $\sigma=1.34$.

With that knowledge, which CIA examinee performed better compared to the population of examinees?

```{r, echo = TRUE}
person2014Z = (3.2-3.04) / 1.41

person2015Z = (3.2-2.86) / 1.34
```

Let's play with this for a little bit:


```{r, echo = TRUE}
ciaExam2014 = rnorm(n = 5000, mean = 3.04, sd = 1.41)

plot(density(ciaExam2014))
abline(v = c(3.04, 3.2), col = c("black", "red"))
```

We can find out how many people our person in 2014 bested:
```{r, echo = TRUE}
pnorm(3.2, 3.04, 1.41)
pnorm(person2014Z)
```

You can see that giving the *z* or the actual values produced the same results!


We can consider that everything that falls under our curve is 1 (i.e., it is 100%). So, if we wanted to find the proportion of people doing better than our reference people, we would just subtract our distribution function from 1.

```{r}
1 - pnorm(person2014Z)
```


If we use pnorm to find the proportion/probability, we can feed a proportion to qnorm to find z.

```{r, echo = TRUE}
qnorm(.25)
```



## Populations and Samples

### Central Limit Theorem


The CLT dictates that as we increase the number of samples from a population, we will begin to approach normally distributed means.

```{r}
library(gridExtra)

set.seed(123)
r = 10000
n = 200     


sample.means = function(samps, r, n) {
  rowMeans(matrix(samps,nrow=r,ncol=n))
}

qqplot.data = function (vec) {
  y = quantile(vec[!is.na(vec)], c(0.25, 0.75))
  x = qnorm(c(0.25, 0.75))
  slope = diff(y)/diff(x)
  int = y[1L] - slope * x[1L]

  d = data.frame(resids = vec)
  
  return(d)
}

generate.plots = function(samps, samp.means) {
  p1 = qplot(samps, geom="histogram", bins=30, main="Sample Histogram") + theme_minimal()
  p2 = qplot(samp.means, geom="histogram", bins=30, main="Sample Mean Histogram") + theme_minimal()
  grid.arrange(p1,p2,ncol=2)
}

samps = runif(r*n)

samp.means = sample.means(samps, r, n)

generate.plots(samps, samp.means)

```


```{r}
samps = rpois(r*n,lambda=3)

samp.means = sample.means(samps, r, n)

generate.plots(samps, samp.means)
```


```{r}
samps = rexp(r*n,rate=1)

samp.means = sample.means(samps, r, n)

generate.plots(samps, samp.means)
```


## Other Distribution Fun

Let's start with a Gaussian distribution of 10000 observations:

```{r, echo = TRUE}
set.seed(1001)

population = rnorm(10000)

plot(density(population))
```

Now, let's take a small sample of our population:

```{r, echo = TRUE}

set.seed(1001)

smallSample = sample(population, 75, replace = FALSE)

plot(density(smallSample))
```

And now something a little bigger:

```{r, echo = TRUE}
set.seed(1001)

mediumSample = sample(population, 250, replace = FALSE)

plot(density(mediumSample))
```

And bigger still:

```{r, echo = TRUE}
set.seed(1001)

biggerSample = sample(population, 1000, replace = FALSE)

plot(density(biggerSample))
```


And finally:

```{r, echo = TRUE}
set.seed(1001)

biggestSample = sample(population, 2500, replace = FALSE)

plot(density(biggestSample))
```

Original:

```{r}
plot(density(population))
```


### What Is The Point?

We had our "population", so how well did our samples replicate the population distribution?

This starts to illustrate the *t*-distribution (more on this in a few weeks).

We are also getting into issues related to point estimation.

Let's consider the following:

```{r}
mean(population)

mean(biggerSample)

mean(biggestSample)
```


We can even take another sample from our population:

```{r}
mean(sample(population, 2500, replace = FALSE))
```

Let's take a bigger sample:

```{r}
mean(sample(population, 5000, replace = FALSE))
```

In and of itself, this is interesting. It has applications, however, to null hypothesis significance testing.

