---
title: "Bayesian Regression"
output: distill::distill_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, message = FALSE, warning = FALSE, error = FALSE)
```

# Main Points

1.  Bayesian Data Analysis (BDA) offers an alternative approach to Frequentist approaches.

2.  The results from a Bayesian model are more directly interpretable.

3.  Sample size issues are practically gone.

4.  Models are ultimately flexible with regard to prior distributions.

# Package Roundup

Some of these might require some work.

```{r, eval = FALSE}
install.packages(c("rstanarm", "jsonlite", "R.utils", 
                   "tidyr", "rstudioapi"))
```

We won't use it, but I also want to introduce you to the <a href="http://docs.zeligproject.org/articles/index.html#section-core-zelig-model-details">Zelig</a> package.  Pick just about any common model and you will find that Zelig likely has an implementation of it.

You can also do something similar with <a href="https://topepo.github.io/caret/available-models.html">caret</a>.

# Our Data

Should be familiar:

```{r}
library(dplyr)

crimeScore = data.table::fread("http://nd.edu/~sberry5/data/crimeScore.csv")

crimeScore = crimeScore %>% 
  filter(SEX_CODE_CD != "X") %>%
  # dplyr::select(SSL_SCORE, SEX_CODE_CD, WEAPONS_ARR_CNT) %>% # Mean weapons = 1.206
  mutate(WEAPONS_ARR_CNT = ifelse(is.na(WEAPONS_ARR_CNT), 0, WEAPONS_ARR_CNT),
    WEAPONS_ARR_CNT = (WEAPONS_ARR_CNT - mean(WEAPONS_ARR_CNT)), 
    SEX_CODE_CD = as.factor(SEX_CODE_CD), 
    `Zip Codes` = as.factor(`Zip Codes`))
```


## An Aside On Interactions & Mixed Models

### Interactions

```{r}
intMod <- lm(SSL_SCORE ~ WEAPONS_ARR_CNT * SEX_CODE_CD, data = crimeScore)

summary(intMod)

whatTheActualMod <- lm(SSL_SCORE ~ WEAPONS_ARR_CNT * SEX_CODE_CD * `Zip Codes`, data = crimeScore)

summary(whatTheActualMod)
```


### Mixed Model

```{r}
library(lme4)

meMod <- lmer(SSL_SCORE ~ WEAPONS_ARR_CNT * SEX_CODE_CD + (WEAPONS_ARR_CNT|`Zip Codes`), data = crimeScore)

summary(meMod)
```


```{r}
library(merTools)

randomEffects <- REsim(meMod)

randomEffects

plotREsim(randomEffects, labs = TRUE)
```



```{r}
plotFEsim(FEsim(meMod))
```

# Bayesian Data Analysis

We all remember our discussion about probability: 

$$P(A|B) = \frac {P(B|A)P(A)} {P(B)}$$

And conditional probability:

$$P(B|A) = \frac{P(A \cap B)}{P(A)}$$

At its core, BDA is just a fancy conditional probability model.

This might be more intuitive:

$$p(hypothesis|data) \propto p(data|hypothesis)p(hypothesis)$$

What is the probability of our hypothesis being correct, given our data, in proportion to prior beliefs about the hypothesis.

In the end, we get this:

$$updated\ belief = current\ evidence\ *\ prior\ belief  $$

In the end, we get some *posterior distribution* of effects, which is defined as $$posterior = \frac{prior * likelihood}{evidence}$$

![](bayes.png)

We already should have an idea of what a prior is, but the likelihood is essentially relating to the plausibility of the data. The posterior is where the Bayes actually comes into play -- for every unique combination of data, likelihood, parameter, and prior, there is a unique set of estimates. For these estimates, we get some idea of how plausible each unique value is, conditional on the data.

## How Is It Different?

We have two different worlds: Frequentist (where we have largely been living) and Bayesian.

There is a wild amount of debate within stats, but we are going to focus on what BDA does for your models.

Here are the differences:

1.  Probability

    - Frequentist: I am going to assume that my parameter is zero. What is the probability that my observed parameter is a certain magnitute different than zero?
  
    - Bayesian: What is the probability that my parameter is not zero?

2.  Interval Estimates

    - Frequentist: I will conduct my analyses an infinite number of times and calculate an interval each time, than a certain percentange of those intervals will contain the true value. I will now show you just one of those intervals.
  
    - Bayesian: The probability that the true value falls in this interval is *P*.

A Frequentist might say something like the following: I reject the null hypothesis that variable x has no bearing on y. Given the *p*-value of my test statistic, the probability of obtaining my large test statistic is very small if the null hypothesis is indeed true.

A Bayesian might say something like this: I am 95% sure that variable x had this effect on y.

Another significant departure is the iterative nature of Bayesian models. Since these models are running many times and producing estimates, we eventually get a distribution of estimates.

In the Frequentist notion, sample size plays an important part in all of our statistics (e.g., different sample sizes would produce different *t* and *p* values). 

## The Basics

1.  We have a feeling about the way our world works (we might even have some data to support this feeling). This defines our *prior*.  We can specify a lot of different parameters here (the shape of the distribution and the properties of the distribution, such as the mean).

2.  After running our model on new data, we can *update* what we know about our priors.

3.  We can also create a *posterior probability distribution*.

We are obtaining the probability of a hypothesis being true, given the evidence and our prior beliefs.

Although in the end we will be able to get some point estimates (think our regression coefficients), we are going for something else with BDA -- the distribution of possible effects.  Essentially, we get potentially many possible values and a probability for each one.

## Stan

The stan language has quickly turned into *the* language for BDA.  Stan, in and of itself, is a fully functioning language with hooks to other languages (R, Python, Matlab, Stata).  We are going to focus on the "rstanarm" package.  In essence, it creates a nice wrapper for Stan models and helps to serve as a nice bridge.

## Constructing Our Model

For our reference, here is our Frequentist model:

```{r}
slim = lm(SSL_SCORE ~ WEAPONS_ARR_CNT, data = crimeScore)

coef(slim)
```


In our Bayesian model, we are going to use "informative" priors.  We are giving it something to work on, but it is pretty generic (e.g., giving a uniform distribution would be uninformative and practically the same as our standard linear model). *Weakly informative* priors offer various levels of protection of very unlikely parameter values, while also allowing for some outlier action if it arises in the data (this is also why they are sometimes called regularizing priors). 

The specification of priors is the sticky point here.  They can be somewhat subjective, especially in the case when you do not have any background evidence. The mantra in BDA is *priors are subjective, not arbitrary*.

**WARNING** -- This might take a while to run as it is! Of all the things that we have worked on, nothing is more computationally intensive than this stuff.  We are using a little more than half of the necessary chains and iterations to get reasonably quick results.

```{r}
library(rstanarm)

blm = stan_lm(SSL_SCORE ~ WEAPONS_ARR_CNT, 
              data = crimeScore, 
              prior = R2(.1, "mean"), seed = 10001, chains = 6,
              cores = (parallel::detectCores() - 1),
              iter = 4000)
```

There are a few things to unpack in that code, namely the `prior` argument. We are going to specify a prior distribution based upon our proposed $R^2$ value. A reasonable question to ask is how does $R^2$ relate to a prior distribution? We know that the range of $R^2$ is 0-1, so this maps nicely onto the beta distribution! This $R^2$ method is a handy shortcut for specifying the total model parameters.

We can fit a line, as per usual course:

```{r}
draws = as.data.frame(as.matrix(blm))

library(ggplot2)

ggplot(crimeScore, aes(WEAPONS_ARR_CNT, SSL_SCORE)) +
  geom_point() +
  geom_abline(data = draws, aes(intercept = `(Intercept)`, 
                                slope = `WEAPONS_ARR_CNT`), 
              color = "#ff5500", alpha = .2) +
  geom_abline(data = draws, aes(intercept = coef(blm)[1], 
                                slope = coef(blm)[2]))
```



```{r, eval = TRUE}
summary(blm)
```


Well what do we have here? A lot of this output looks unfamiliar.  There are two columns to give some attention.  Instead of a point estimate (e.g., our regression coefficients), we are given the mean values of the posterior distribution (which *are* just your standard model coefficients). We are also given the credible intervals. The sigma is the standard deviation of the error and the mean_PPD is the predicted value for the average observation.  

We also have some diagnostics. The "mcse" is the Monte Carlo standard error (this accounts for the uncertainty of having a finite number of posterior draws), "Rhat" tells us how well the chains mix (we want it to be 1 or very close to it), and "n_eff" tells us the number of effective *n* over the chains (it accounts for autocorrelation within the chain; it should be close to the number of iterations).   

We can also check out our posterior intervals:

```{r, eval = TRUE}
posterior_interval(blm, prob = .95, pars = "WEAPONS_ARR_CNT")
```

This is where our simplified interpretation becomes useful.  Here, we can say that there is a 95% chance that the true parameter rests within this interval.  Nice, simple, and done!

Let's also do some graphical checking:

```{r, eval = TRUE}
rstan::stan_trace(blm, pars = "WEAPONS_ARR_CNT")
```

Do you see a caterpillar or grass?  

### Markov Chains (an aside)

Much like flipping a log over will yield an assortment of creatures, peeling back the layers of many methods will reveal a Markov Chain. And just like those assorted critters, you don't really know what you are looking at when you see them.

Here is a conceptual example of markov chains. This example is adapted from Richard McElreath's excellent book, *Statistical Rethinking*.

You manage 10 teams, named from 1 to 10. Each team name is proportional to the number of people on the team and each team is on a separate floor in the building.

You need to visit a team everyday, proportional to the number of people on the team (i.e., you would visit team 10 more often than team 1).

At the end of the day, you randomly select whether a proposed move will take you up a floor or down a floor.

After randomly selecting up or down, you grab a number of pens that is equal to the number of the current team (for team 10, you would grab 10 pens).

Next, you would grab a number of pencils corresponding to the proposed move (your randomly selected proposal would take you up a floor, so starting back at the bottom with team 1). So you would have 10 pens and 1 pencil.

If you have more pencils than pens, you will always move to the proposed floor.

If you have more pens than pencils, you set down a number of pens equal to the number of pencils and put them in a drawer.

You reach back into the drawer to randomly select a pen or a pencil.

Your selection decides where you go -- pen is stay and pencil is go!

Over 1000 moves, here is what our chain would look like:

```{r, echo = FALSE}
markovSim = function(daysRun, startDay) {
  
  position = rep(0, daysRun)
  
  current = startDay
  
  for(i in 1:daysRun) {
    position[i] = current
    
    proposal = current + sample(c(-1, 1), size = 1)
    
    if(proposal < 1) proposal = 10
    
    if(proposal > 10) proposal = 1
    
    probabilityMove = proposal / current
    
    current = ifelse(runif(1) < probabilityMove, proposal, current)
    
    # print(paste(position[i], proposal, probabilityMove, current, sep = " -> "))
  }
  
  return(position)
}

test1 = markovSim(1000, 5)

test2 = markovSim(1000, 6)

library(ggplot2)

ggplot() + 
  geom_line(data = as.data.frame(test1), aes(1:length(test1), test1), 
            color = "#ff5500", size = .5, alpha = .5) +
  theme_minimal()
```


We can also look at our posterior predictive distributions:

```{r}
pp_check(blm, plotfun = "hist", nreps = 5)
```

We can do many of them and put them together with density overlays:

```{r, eval = TRUE}
pp_check(blm, plotfun = "dens_overlay", nreps = 60)
```

Here, we have our observed predicted value density with simulated densities for 60 draws.  The simulated densities are coming from datasets simulated from the posterior predictive distribution.  In an ideal world, they look pretty close to each other.

"Okay, sometimes science is more art than science" -- Rick Sanchez

### Model Selection

Let's now try this model:

```{r}
blm2 = update(blm, formula = . ~ WEAPONS_ARR_CNT + SEX_CODE_CD)
```


```{r, eval = TRUE}
summary(blm2)
```

```{r, eval = TRUE}
rstan::stan_trace(blm2)
```


```{r, eval = TRUE}
pp_check(blm2, plotfun = "dens_overlay", 
         nreps = 60)
```



## Logistic Regression

```{r}
kickstarter = readr::read_csv("https://www.nd.edu/~sberry5/data/kickstarter.csv")

kickstarter <- kickstarter %>% 
  filter(state == "successful" | state == "failed") %>% 
  mutate(state = ifelse(state == "successful", 1, 0)) %>% 
  filter(backers < 100)

ggplot(kickstarter, aes(backers, y = ..density.., fill = state == 1)) +
  geom_histogram(alpha = .5)  +
  theme_minimal()

logTest = glm(state ~ backers, 
              data = kickstarter[sample(1:nrow(kickstarter), 1000, replace = FALSE), ], 
              family = binomial)

summary(logTest)
```

```{r, eval = FALSE}
# Do not run unless you have some time!

bLog = stan_glm(state ~ backers, data = kickstarter, family = binomial,
              prior = student_t(df = 7, location = 0, scale = 2.5), 
              prior_intercept = student_t(df = 7, location = 0, scale = 2.5), 
              seed = 10001, chains = 6,
              cores = (parallel::detectCores() - 1),
              iter = 4000)

summary(bLog)
```

```{r, echo=FALSE}
load("bLog.RData")

summary(bLog)
```

Our coefficients follow the same interpretation, but we should talk about our priors some more. In our logisitc regression model, what response are we ultimately trying to model? The probability of being a 0 or 1 is the goal, but our log odds are the first point we need to go through. With that, we can remember that a log odds of 0 would indicate no effect and the log odds can take on a positive or negative value. If we want something centered around 0, with equal likelihood of values of both sides on the 0, we could use a normal distrubtion with a mean of 0 and set the scale as needed (a small SD would mean that we are less likely to observe larger coefficients and a larger SD would indicate that we might see a larger range of effects.). An alternative outside of the normal distribution is the t-distribution. The t-distribution is described by its degrees of freedom (in addition to the mean and sd) and looks like a normal distribution when the degrees of freedom approach infinity:

```{r}
library(ggplot2)

t.values <- seq(-4,4,.1)

data.frame(values = t.values, 
           df1 = dt(t.values, 1),
           df3 = dt(t.values, 3),
           df7 = dt(t.values, 7),
           dfInf = dnorm(t.values)) %>% 
  reshape2::melt(., id.vars = "values") %>% 
  ggplot(., aes(x = values, y = value, color = variable)) +
  geom_line() +
  theme_minimal()
```

If you want to provide a higher chance of something appearing in the tails of the distribution, then the large tails of a small DF t-distribution might be a better alternative to a normal distribution.

## Model Comparisons

```{r}
blm <- stan_lm(SSL_SCORE ~ WEAPONS_ARR_CNT, 
              data = crimeScore, 
              prior = R2(.1, "mean"), seed = 10001, chains = 6,
              cores = (parallel::detectCores() - 1),
              iter = 4000)

blm2 <- stan_lm(SSL_SCORE ~ WEAPONS_ARR_CNT + SEX_CODE_CD, 
              data = crimeScore, 
              prior = R2(.1, "mean"), seed = 10001, chains = 6,
              cores = (parallel::detectCores() - 1),
              iter = 4000)
```


Leave-one-out cross-validation is probably nothing new to you at this point and we can use it with our Bayesian models:

```{r, eval = FALSE}
library(loo)

looMod1 <- loo(blm, cores = parallel::detectCores() - 1)

looMod1

looMod2 <- loo(blm2, cores = parallel::detectCores() - 1)

looMod2

compare(looMod1, looMod2)
```

```{r, eval = TRUE, echo = FALSE}
load("C:/Users/sberry5/Documents/teaching/courses/inf/notes/looOut.RData")

load("C:/Users/sberry5/Documents/teaching/courses/inf/notes/loo2.RData")

library(loo)

looMod1

looMod2

compare_models(looMod1, looMod2)
```

We are going to look for a few things here. The first is `elpd` (the expected log pointwise predictive density). Here, we are looking for which model has the highest elpd value; this indicates that it has the highest posterior probability (i.e., how accurate is any single prediction for a value). The next thing we would examine is the looic; this has the exact same interpretation as the AIC. <a href="https://arxiv.org/pdf/1307.5928.pdf">Here</a> is a really nice article with additional measures of model fit.

## New Predictions

If we like our two predictor model better, let's see how it will fair with some new data:

```{r}
weaponsArrests <- seq(0, 15, by = 1)

newMale <- posterior_predict(blm2, newdata = data.frame(SEX_CODE_CD = "M", 
                                                        WEAPONS_ARR_CNT = weaponsArrests))

newFemale <- posterior_predict(blm2, newdata = data.frame(SEX_CODE_CD = "F", 
                                                        WEAPONS_ARR_CNT = weaponsArrests))

par(mfrow = c(1:2))
    
boxplot(newMale, ylim = c(0, 1000))

boxplot(newFemale, ylim = c(0, 1000))    
```

