---
title: "Generalized Linear Models"
output: distill::distill_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, error = FALSE, comment = "")
```

# Logistic Regression Key Points

1.  Generalized linear models exist to map different distributions into linear space.

2.  Logistic regression is for binary outcome variables (e.g., closed/open, broke/working, dead/alive) and we are predicting the outcome.

3.  Our coefficients are going to be log odds -- which we can relate back to probability.

4.  Predicted probabilities are likely the easiest way to interpret your logistic model.

# Generalized Linear Models

GLM is telling you exactly what it is within the title.

But, how is it generalized?

## Distributions...

Remember how linear models really enjoy the whole Gaussian distribution scene?

- Not all data follows a Gaussian distribution. Instead, we often find some form of an exponential distribution (or.

So, we need a way to incorporate different distributions of the DV into our model.

Distributions cannot do it alone!

## And Link Functions

From a theoretical perspective, link functions are tricky to get your head around.

- *Find the exponential of the response's density function and derive the canonical link function*...

From a conceptual perspective, all they are doing is allowing the linear predictor to "link" to a distribution function's mean.

- If you know a distribution's canonical link function, that is all the deeper you will probably every need.

At the end of the day, these link functions will convert the outcome (DV) to an unbounded continuous variable.

# Linear Regression

We already saw linear regression.

A linear regression deals with real numbers between $-\infty$ to $\infty$ in the dependent variable.

It is the most vanilla within the GLM.

- Gaussian distribution with an "identity" link function

## In R

```{r}
crimeScore = read.csv("http://nd.edu/~sberry5/data/crimeScore.csv")

lmTest = glm(SSL_SCORE ~ WEAPONS_ARR_CNT, data = crimeScore, family = gaussian)

summary(lmTest)
```

We see a lot of the same information, but we have a new model fit statistic to look at: deviance.

Deviance is *badness*. The null deviance is telling us how well our dependent variable is predicted by a model with only the intercept -- this is generally the case with all null models.

Our model, captured in the residual deviance, showed a marked decrease in deviance with very little lost in degrees of freedom.

We can test it with the following:

```{r}
1 - pchisq(lmTest$deviance, lmTest$df.residual)
```


The value of AIC is not really interpretable -- it is used only for model comparison.

We can also produce our confidence intervals:

```{r}
confint(lmTest)
```


### A Quick Aside On CIs

Confidence intervals express that 95% of samples will contain the true value, which we can never really know.

They are not related to the probability of a result being correct.

They are not a 95% chance that the true estimate falls within.

They are not meant to say that the mean will fall into the confidence interval 95% of the time.

"Doesn't contain 0" does not apply to everything.

# Logistic Regression

Logistic regression is substantially different than linear regression.

- It is also a bit confusing, because it is named after its link function.

Instead of that nice continuous dv, we are dealing with a binomially-distributed DV.

- 0's and 1's as responses.

- no $\mu$ or $\sigma^2$ to identify the shape of the distribution; instead we have *p* and *n*, where *p* is a probability and *n* is the number of trials.

We tend to talk about *p* with regard to the probability of a specific event happening (heads, wins, defaulting, etc.).

Let's see how the binomial distribution looks with 100 trials and probabilities of .25, .5, and .75:

```{r}
library(ggplot2)

binom.25 <- dbinom(1:100, size = 100, prob = .25)

binom.5 <- dbinom(1:100, size = 100, prob = .5)

binom.75 <- dbinom(1:100, size = 100 , prob = .75)

as.data.frame(rbind(binom.25, binom.5, binom.75)) %>% 
  mutate(prob = row.names(.)) %>% 
  tidyr::gather(., "key", "value", -prob) %>% 
  mutate(key = as.numeric(gsub("V", "", key))) %>% 
  ggplot(., aes(x = key, y = value, color = prob)) + 
  geom_line() + 
  scale_color_brewer(type = "qual", palette = "Dark2") + 
  theme_minimal()
```

If we examine the line for a probability of .5 (the orange line title "binom.5"), we will see that it is centered over 50 -- this would suggest that we have the highest probability of encountering 50 successes. If we run 20 trials and the outcome is 50/50, we would expect to see success in half the trials and a decreasing number of trials for more or less successes. Shifting our attention to a .75 probability of success, we see that our density is sitting over 75. 

Since we are dealing with a number of trials, it is worth noting that the binomial distribution is a discreet distribution. If have any interest in knowing the probability for a number of success, we can use the following formula:

$$P(x) = \frac{n!}{(n-x)!x!}p^xq^{n-x}$$


```{r}
library(dplyr)

kickstarter = data.table::fread("https://www.nd.edu/~sberry5/data/kickstarter.csv")

kickstarter = kickstarter %>% 
  filter(state == "successful" | state == "failed") %>% 
  mutate(state = ifelse(state == "successful", 1, 0))
```

Let's start with a visual breakdown of failure and success:

```{r}
ggplot(kickstarter, aes(state)) +
  geom_bar() +
  theme_minimal()
```

And a table:

```{r}
addmargins(table(kickstarter$state))
```

If we take the number of successes and divide by the total, we will get the probability of being funded:

```{r}
113081 / 281302
```

The probability of being funded is .4

## Continuous Predictors

While just knowing the probability is great, it does not provide much of a model. So, we can add predictor variables to model their relationship with the outcome variable.

```{r}
logTest = glm(state ~ backers, data = kickstarter, 
              family = binomial)

summary(logTest)
```

We are now dealing with log odds in the coefficients. We would say that for every unit increase in backers, the log odds of a campaign being funded is  ~.03.

## Some Logistic Grounding

### Log Odds

As its default, R produces the log odds for the regression coefficient in a logistic model. 

Probability lies at the heart of all of this.  We can look at the relationship between the probability, odds, and log odds.

```{r}
probabilityList = c(.001, .01, .15, .2, 
                    .25, .3, .35, .4, .45, 
                    .5, .55, .6, .65, .7, 
                    .75, .8, .85, .9)
```


We have a list of probability values (always between 0 and 1).  Now, let's write a function to convert them to odds. We will use the $\\p\, / 1 - p$ equation.

```{r}
oddsConversion = function(p) {
  res = p / (1 - p)
  return(res)
}

odds = oddsConversion(probabilityList)

plot(probabilityList, odds)
```

Now, we can convert them to log odds:

```{r}
plot(odds, log(odds))
```


### The Intercept

The intercept is offering the log odds of a campaign with 0 backers being funded -- converting this back to probability (just adding the exponentiation into solving back to probability), we get the following:

```{r}
exp(coef(logTest)["(Intercept)"]) / (1 + exp(coef(logTest)["(Intercept)"]))
```

We are dealing with a pretty small probability (but certainly not 0) that a campaign with 0 backers could be successful.

### The Predictor

Recall that the coefficient in log odds for the backers was .02645174.

Let's start at the median value of backers:

```{r}
median(kickstarter$backers)
```

Now, let's solve our equation for that one. This will produce the conditional logit.

```{r}
medLogit = logTest$coefficients["(Intercept)"] + 
  (logTest$coefficients["backers"] * median(kickstarter$backers))

names(medLogit) = NULL

medLogit
```

Let's do the same thing for the next sequential value of backers:

```{r}
medLogitPlus1 = logTest$coefficients["(Intercept)"] + 
  (logTest$coefficients["backers"] * (median(kickstarter$backers) + 1))

names(medLogitPlus1) = NULL

medLogitPlus1
```

Now we have two sequential conditional logits that we can subtract from each other:

```{r}
backersCoef = medLogitPlus1 - medLogit

backersCoef
```

This is exactly the coefficient that we got from the model. If we exponentiate the log odds, we are "unlogging" it to get the odds ratio that we saw eariler (just plain old odds at this point):

```{r}
exp(backersCoef)
```

For every unit increase in backer, we have a 2% increase in the odds that the campaign will be successful. These odds are mostly stacking as we increase (see the plot below). When we look at the odds, anything above 1 is in favor of moving from the "0" category to the "1" category, whereas anything below 1 is in favor of not moving from the "0".  From a technical question perspective, this makes absolute sense -- we would expect that having more backers contributes to a better chance of funding.

## Putting It Together (Hopefully)

When we do our linear model, we are trying to predict the response -- not the case with logistic regression.  Instead we are trying to predict the probability of going from 0 to 1 (not funded to funded).  So, normal plots that we might usually create with linear models will do us no good here.

First, we need to make our predicitions and put them back into the data:

```{r}
library(dplyr)

kickstarter = kickstarter %>% 
  mutate(predictedProbs = predict(logTest, type = "response"))
```

The predict() function is how we get the predictions from any model.  Do note the type argument with the "response" specification; this ensures that we are using the type of response particular to the model.

Now, we can plot it:

```{r}
kickstarter %>% 
  filter(backers < 1000) %>% 
  ggplot(., aes(backers, predictedProbs)) +
  geom_line(size = 1.5) +
  theme_minimal()
```

For our data, once you start to get above 250 backers, there is a probablity of 1 that it will be successful.

If we wanted to apply new data to our model, we would also use the predict() function, but give it new data.


## Categorical Predictors

Let's pick a few categories to work on.

```{r}
catData = kickstarter %>% 
  select(state, main_category) %>% 
  filter(main_category == "Film & Video" |
           main_category == "Music" |
           main_category == "Games")
```


Let's look a the crosstabs for those variables:

```{r}
addmargins(table(catData))
```

We can start to look at the odds for each category being successful:

```{r}
filmOdds = (21404 / 51057) / (29653 / 51057) # You can reduce this to 21404 / 29653

gamesOdds = (9385 / 22398) / (13013 / 22398)

musicOdds = (21763 / 40956) / (19193 / 40956)
```

```{r}
filmOdds

gamesOdds

musicOdds
```

And we can take each one back to a probability:

```{r}
filmOdds / (1 + filmOdds)

gamesOdds / (1 + gamesOdds)

musicOdds / (1 + musicOdds)
```

We can take our probability formulation and divide 1 - the probability to recover our odds:

```{r}
filmOdds / (1 + filmOdds) / (1 - (filmOdds / (1 + filmOdds)))

musicOdds / (1 + musicOdds) / (1 - (musicOdds / (1 + musicOdds)))
```

When we consider what our odds mean now, we can envision them as the probability of the outcome occuring divided by the probability that it does not happen.

We could also compare those two directly:

```{r}
(21763 / 19193) / (21404 / 29653)
```

So the odds that a music campaign will be funded are about 57% higher than the odds for a film.

And we could also compare film to games:

```{r}
(9385 / 13013) / (21404 / 29653)
```

Here, the odds that a game will be funded are less than 1% lower than the odds for a film.

Let's run our logistic regression now:

```{r}
categoryLogTest = glm(state ~ main_category, data = catData, family = binomial)

summary(categoryLogTest)
```


```{r}
exp(coef(categoryLogTest))
```

```{r}
filmOdds
```

The film group is about 28% less likely to be funded.

## Cautions About Logistic Regression

Bivariate relationships are really important for logistic models.

- Empty cells can wreck a model.

Your model needs to see some dispersion of values over the bivariate tables.

- Otherwise, you get what is known as separation (perfect prediction over some levels)

Logistic regression requires a larger sample size than what a linear regression needs.

- The "exact test" is a small sample alternative.

## A Big "Gotcha"

$R^2$ does not apply to a logistic regression.

- There are many pseudo-$R^2$, but they really do not mean the same thing as in linear regression.

- You might be asked for them and many people present them.

## Logistic Practice

We all know and love the attrition data and we can finally start looking at predicting attrition:

```{r}
library(rsample)

data("attrition")

glm(Attrition ~ Age, data = attrition, family = binomial)
```

Check to see if any other variables might provide a suitable prediction for whether or not any employee attrited.

# Poisson Regression

## Key Points

1.  Poisson regression is for count-based dependent variables (e.g., how many touchdowns does a team get, how many safety violations does a factory have, how many credit cards do you have under your name)

2.  If you have a lot of zeroes in your data, you might want to consider a zero-inflated Poisson.

3.  If your count variable does not follow a Poisson distribution, then you might want to use a negative binomial model.

## The (Sometimes) Thin Line

This gets into an area where we need to think long and hard about our dependent variable and what it actually might be.

- Since Poisson regression gets its name from the Poisson distribution, we should probably see if it follows the Poisson distribution.

### Checking...

```{r}
library(vcd)

shroudData = readr::read_csv("https://www.nd.edu/~sberry5/data/shroudData.csv")

poissonTest = goodfit(shroudData$shroudsProduced, type = "poisson")

summary(poissonTest)

plot(poissonTest)
```

This is a $\chi^2$ to test if the distribution deviates from a Poisson.

- It probably does not.

### Dispersion

For models of this nature (our dependent variable is a count variable), we may have two different distributions with which to operate: the Poisson distribution (which will use a natural log link function) or the negative binomial distribution.

Let’s check this out (it will be important later on!).

```{r}
library(dplyr)

shroudData %>% 
  dplyr::select(shroudsProduced, employeeCount) %>% 
  group_by(employeeCount) %>% 
  summarize(mean = mean(shroudsProduced), var = var(shroudsProduced))
```


What is the purpose of this? We are checking the conditional means and variances. Why is this important? If our variances are larger than our means, we have *overdispersion*. We would expect values to be equally distributed over levels, but if they are really spread out, this qualifies as overdispersion – this is not good for our Poisson model because it will cause downward bias (bias, while not tricky conceptually, presents interesting thought questions).

As a very simple check, we can also compare the mean and the variance of our outcome variable -- they should be close to equal!

```{r}
mean(shroudData$shroudsProduced)

var(shroudData$shroudsProduced)
```

It looks like everything is mostly okay (for now), so let’s proceed onward with our poisson model:

```{r}
poissonTest = glm(shroudsProduced ~ employeeCount, 
                  data = shroudData, 
                  family = poisson)

summary(poissonTest)

exp(poissonTest$coefficients)
```

**Important Note:** We are going to interpret this almost the same as a linear regression. The slight wrinkle here, though, is that we are looking at the log counts. In other words, an increase in one employee leads to an expected log count increase of ~.029. Just like our logisitc regression, we could exponentiate this to get 1.029189 – every employee we add gets us a 3% increase in shrouds produced. Let’s see what this looks like in action:

```{r}
shroudData = shroudData %>% 
  mutate(predValues = poissonTest$fitted.values)

library(ggplot2)

ggplot(shroudData, aes(employeeCount, predValues)) + 
  geom_count() +
  scale_size_area() +
  theme_minimal()
```


Finally, we can look at the residual deviance (it is comparing our model to a model with intercepts only) to get at our model fit:

```{r}
pchisq(poissonTest$deviance, poissonTest$df.residual, lower.tail = FALSE)
```

This is a *p*-value – it should not be significant.

With everything coupled together, we have a meaningful coefficient, a clear plot, and adequate model fit. Therefore, we might conclude that there is a positive relationship between number of employees on shift and shrouds produced.

In addition to checking our data for over dispersion before running the model, we can also check it after running our model:

```{r}
library(AER)

dispersiontest(poissonTest)
```

The dispersion value that we see returned (0.9452052 in our case) should be under 1. A dispersion value over 1 means that we have overdispersion. Our dispersion value, coupled with our high *p*-value, indicates that we would fail to reject the null hypothesis of equidispersion

We can also look back to our model results to compare our residual deviance to our residual deviance degrees of freedom; if our deviance is greater than our degrees of freedom, we might have an issue with overdispersion. Since we are just a bit over and our overdispersion tests do not indicate any huge issue, we can be relatively okay with our model. If we had some more extreme overdispersion, we would want to flip to a quasi-poisson distribution -- our coefficients would not change, but we would have improved standard errors.

# Zero-inflated (ZIP)

Sometimes we have a seeming abundance of zero values within our data. We can have employees with zero absence periods, lines with zero quality failures, and days without safety issues. What is the process that generated the zeros? Are they coming from our count model (“true” zeroes) or something else (some random process)? This is where zero-inflated models become important. ZIP models are mixture models. We are not going to dive too deeply into this, but all you need to know is that a mixture model contains a “mixture” of different distributions.

```{r}
redlights = readr::read_csv("https://www.nd.edu/~sberry5/data/redlights.csv")

poissonRedlight = glm(citation_count ~ as.factor(camera_installed_ny), 
            data = redlights, 
            family = poisson)

summary(poissonRedlight)
```

With this output, we are comparing citation counts against intersection without a camera and those with a camera.

We see that our coefficient is -.88529 –- this means that having a camera leads to having .88529 less log counts than without having a camera.

We can also exponentiate that value to get an indicent rate:

```{r}
exp(poissonRedlight$coefficients)
```

Now, we could say that the incident rate (redlight citations) for intersections with a camera is about .58 times less than for intersections without a camera.

```{r}
tapply(redlights$citation_count, redlights$camera_installed_ny, mean) #Old school group_by and summarize!
```

```{r}
4.086172/9.903564
```

Dividing the mean of the target group by the mean of the reference group will get us the incidence rate (i.e., for every one time someone runs a red light without a camera, it happens .41 times with camera).

If we take a look at citation_count’s distribution, we will see more than a few 0’s.

```{r}
hist(redlights$citation_count)
```


For our redlight data, we saw that having a camera present had an effect on citations, but would it cause 0 citations? Or might there be something else contributing to the 0’s (e.g., no cars going through that intersection due to construction, no police nearby)? If there are no cars going through the intersection due to construction, is there even a chance of obtaining a non-zero response?

```{r}
library(pscl)

zipTest = zeroinfl(citation_count ~ as.factor(camera_installed_ny),
                   dist = "poisson", data = redlights)

summary(zipTest)
```

Here we have two sets of coefficients: one for our count model (our actual Poisson regression) and one model attempting to account for excessive 0s. Our count model does not change, but we also see that our zero-inflational model will not account for the 0s within our data.


# Negative Binomial


Remember that whole issue with our conditional means and standard deviations? If we would have had problems those means and variances, we would need to abandon our poisson distribution in favor of the negative binomial. The poisson distribution works when the sample mean and variance are equal – the negative binomial distribution frees that constraint and allows them to vary freely.

Remember this:

```{r}
redlights %>% 
  dplyr::select(citation_count, camera_installed_ny) %>% 
  group_by(camera_installed_ny) %>% 
  summarize(mean = mean(citation_count), var = var(citation_count))
```

Those look like the start of problems. Let’s check our whole sample now:

```{r}
mean(redlights$citation_count)
```


```{r}
var(redlights$citation_count)
```

There is clearly a problem here!

```{r}
library(MASS)

nbTest = glm.nb(citation_count ~ as.factor(camera_installed_ny), data = redlights)

summary(nbTest)
```

The interpretation of our negative binomial is exactly the same as our Poisson model -- we have only relaxed the assumptions of our distribution.