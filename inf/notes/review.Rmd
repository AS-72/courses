---
title: "Your Path To Statistical Enlightenment"
output:
  html_document:
    theme: "readable"
    highligh: "tango"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE, comment = "", warning = FALSE, message = FALSE)
```

# Conceptual Grounding Places {.tabset .tabset-fade .tabset-pills}

## Hypothesis Generation

The very first thing that needs to be done is to generate your hypotheses -- hypotheses are critical for exercises in inference (estimating parameters about a population through sampling). There needs to be a 1-to-1 link between your hypotheses and your data; in other words, you need to map your data onto your hypotheses. Remember the scientific process -- it is okay to tweak your hypotheses, but you need to be transparent about how those hypotheses were generated, tweaked, and then revised (not liking a result does not serve as grounds to tweaking).


## Power Analyses {.tabset .tabset-fade .tabset-pills}

Before you embark on any data collection procedures, you need to have an idea about how many rows of data you need to be able to actually find an effect (consider an effect to be a significant result and an effect size is the magnitude of that finding).  This is where power analysis becomes helpful.  Given a target power ($\beta$ = .8; it gives a good ratio of Type I to Type II error -- the higher the power, the less of a chance of making a Type II error), a hypothesized effect size, and the proposed *p*-value.  Remember that each statistical test has its own effect size measure and the hypothesized effects can come frome previous research or an informed guess (just be conservative).

All power analyses will work the same: the bigger the anticipated effect, the smaller the needed sample. In situations were we have a large effect (remember, the magnitude of the effect), we need fewer people to find that effect. 

As an example, I have two groups of people: C-suite folks (average salary of \$193K) and Target sales associates (average salary of roughly \$22758). If I am curious about the difference in salary between those two groups, how many from each group would I need to sample to be able to find a difference if one truly exists? Not very many; I could find the true difference with likely a dozen or so from each group. 

As a counter-example, I have another two groups: people who live in Redmon Illinois (average number of teeth 23, sd = 1) and people who live in Paris Illinois (average number of teeth 24.5, sd = 1). I might be curious to know if there is any difference in the number of teeth that each of the respected townsfolks have in their mouth. Humans generally have 32 teeth, but the people in these two towns, on average, have less. If we wanted to know if there was a difference between these two groups, we would probably need to sample far more than a dozen people from each town to find a significant difference between the two groups.


### *t*-test

The following is a pretty standard *t*-test power analysis. 

```{r}
pwr::pwr.t.test(n = NULL, 
                d = .2, 
                sig.level = .05, 
                power = .8)
```

This will tell us how many people we would need in each group, given our proposed effect size (*d*).

### ANOVA

The ANOVA power analysis is just a bit different than the *t*-test power analysis.  Remember that ANOVA is testing the difference in means between more than two groups.  To that end, we need to specify how many groups we will have.

```{r}

pwr::pwr.anova.test(k = numberGroups, 
                    n = NULL, 
                    f = .1, 
                    sig.level = .05, 
                    power = .8)
```


### Regression 

To compute sample size for a regression, you will need to know how many degrees of freedom you will have in your numerator -- this is equal to the number of terms that you have in your model (e.g., if you have an intercept and one predictor, the numerator df is 1). 

```{r}
pwr::pwr.f2.test(u = numeratorDegreeFreedom, 
                 v = NULL, 
                 f2 = .1, 
                 sig.level = .05, 
                 power = .8)
```

Afterwards, you will get the value for "v" -- the degrees of freedom in the denominator.  You get your total sample size from this by taking the value, adding 1, and then adding the number of terms that you have.


### Post-hoc

If you need to know your observed power, then you can set the "power" argument to NULL in any of the functions and fill in the missing number for the sample.  


## Effect Sizes {.tabset .tabset-fade .tabset-pills}

How do we know about the absolute magnitude of a finding? We use effect sizes. Each statistical test has its own effect size test.

### *t*-test

For Cohen's d, we typically see .01 as very small, .2 as small, .5 as medium, .8 as large, 1.2 as very large, and 2 as huge. What do these mean? Cohen's d is for group mean differences between 2 groups, so a d of .2 means that the mean difference between the two groups is small. 

It is computed as: $$ \frac{M_2-M_1}{SD_{pooled}} $$

With pooled standard deviation as: $$ \sqrt\frac{SD_1^2 + SD_2^2}{2}$$

In R, it would look like this:

```{r}
(M2 - M1) / (sqrt((SD1^2 + SD2^2) / 2)) 
```


More simply, you can get an effect size for a *t*-test like this:

```{r, eval = TRUE}
library(dplyr)

mtcars %>% 
  group_by(am) %>% 
  summarize(mean = mean(mpg), 
            sd = sd(mpg),
            sdSquared = sd^2, 
            n = n())

tTest = t.test(mpg ~ am, data = mtcars)

tTest

compute.es::tes(t = -3.7671, 
                n.1 = 19, 
                n.2 = 13)
```

Cohen's *d* is the first thing reported and we have a value of -1.36. We can just take the absolute value of it and we know that we have a pretty large effect (i.e., the difference in mpg between automatic and manual transmissions is big difference). 

### $f^2$ 

One of the main model statistics in a linear regression is the *f*. Anything that uses an *f*-test will use the $f^2$ effect size, which are generally seen as .02 is small, .15 is medium, and .35 as large.

The $f^2$ is computed as follows: $$ \frac{R^2}{1 - R^2} $$

Since very little effort is required to produce it, we don't even need a function; we can just use our adjusted $R^2$ from a model.

```{r, eval = TRUE}
lmTest = lm(mpg ~ hp, mtcars)

r2 = summary(lmTest)$adj.r.squared

f2 = r2 / (1 - r2)

f2
```

Given that we could consider .35 as being large, it would be safe to say that the effect(s) of our predictor variable (hp) on our dependent variable (mpg) is very large.



## Data Exploration

Before analyses began, it is a good idea to explore your data.  This can be done by checking out summary tables, correlations, and visualizations.

To get a quick summary of your data, the summary() function is helpful:

```{r}
summary(yourData)
```

You can also explore the correlations within your data:

```{r}
cor(yourData$x, yourData$y)

cor(yourData)
```

Do note that the cor() function will only work with numeric data and on whole data (if you have any NAs, you will need to drop them).

Checking all of the bivariate relationships is easy with a scatterplot matrix:

```{r}
car::scatterplotMatrix(youData)
```


# Generalized Linear Models {.tabset .tabset-fade .tabset-pills}

## Continuous DV {.tabset .tabset-fade .tabset-pills}

When your dependent variable is continuous (i.e., real numbers that could range from $-\infty$ to $\infty$), then the general linear model will be a good place to start.

The general linear model is primarily composed of three nearly-identical tests: *t*-tests, Analysis of Variance (ANOVA), and linear regression.

While these 3 tests have common computational underpinings, their uses are different.

If you are interested in group differences only (e.g., differences between treatment and control groups), then *t*-tests and ANOVAs will work for you.

### *t*-tests

If you want to compare two groups' mean values on a variable (e.g., wage differences between men and women within an organization), the *t*-test is the appropriate test to use.  You will need to decide about your tails (one or two tails) and the precise type of *t*-test (paired, independent, etc.), but the basic *t*-test can be performed as follows:

```{r}
tTest = t.test(x = score, y = group, 
       data = yourData)

summary(tTest)
```

#### Example

```{r}
t.test(mpg ~ as.factor(am), data = mtcars)
```

The *t*-test is only going to tell you the difference between two groups. You will not produce predicted values with a *t*-test.


### ANOVA

The ANOVA is very much like the *t*-test; the only exception is that it can handle more groups (is there a performance difference between 1st, 2nd, and 3rd shift employees on the floor).  

```{r}
anovaTest = aov(y ~ x, 
    data = yourData)
```


#### Example

```{r}
anovaTest = aov(mpg ~ as.factor(cyl), data = mtcars)

summary(anovaTest)

TukeyHSD(anovaTest)
```

Like the *t*-test, an ANOVA will only tell you if any difference exists between the groups. If you have a significant ANOVA, then you will need to find out which groups differ from each other -- this is where Tukey's Honestly Significant Difference tests comes into play. Tukey's HSD will perform pairwise comparisons between the groups to find which groups are different from each other.

And like the *t*-test, you cannot get predictions from an ANOVA -- you will only get differences.

ANOVAs really prefer to work on balanced data (i.e., each group has the same size). They can work without, but might not yield the best results. The further out of balance they get, the worse they do.

### Linear Regression

If you are trying to predict a response variable given a set of predictor variables (e.g., I want to predict ad clicks, using gender and age to make the prediction), linear regression will be the best test to use.

In R, the basic linear model is as follows:

```{r}
linearRegression = lm(formula = y ~ x, 
                      data = yourDat)

summary(linearRegression)

plot(linearRegression)
```

#### Example

```{r, eval = TRUE}
testMod = lm(formula = mpg ~ hp + as.factor(am), 
             data = mtcars)

summary(testMod)
```

We want to focus most of our attention on the coefficients. The intercept is telling us the model's mean miles per gallon when horsepower (hp) is at 0 and am is at zero. The coefficient for hp is indicating that every unit increase in horsepower leads to a .06 reduction in the average mpg. Since am is being treated as a factor variable, we are going to compare the reference level (0 = automatic in this case) to the listed level(s). To that end, we would say that a vehicle with a manual transmission gets on average 5.28 more miles per gallon than an automatic transmission.

We might also want to check for heteroskedasticity

We need to know if we have contant variance (good) or not (bad).  We can get a good look by plotting our fitted values against our residuals.

```{r, eval = TRUE}
plot(testMod$fitted, testMod$residuals)
```

#### Cautions & Considerations

Linear regression models can be sensitive to outliers (they exert leverage on our regression line).

We should be looking at heteroskedasticity (we hope for constant variance).

Pay careful attention to centering your variables -- this impacts your intercepts interpretability.

Remember, we are talking about the conditional means.

## Dichotomous DV

If your dependent variable is binary (no/yes, absent/present, etc.), a logistic regression is what you seek.

```{r}
logisticRegression = glm(yBinary ~ x, 
                         family = binomial)
```

### Example
```{r, eval = TRUE}
logisticTest = glm(am ~ mpg, data = mtcars, family = binomial)

summary(logisticTest)
```

Now, our interpration is that every increase in mpg increases the log odds of an automobile having an automatic transmission as opposed to a manual transmission by .3 (we are essentially learning the chances of flipping from one category to the next).

We can exponentiate it to get odds:

```{r, eval = TRUE}
exp(.3070)
```

```{r, eval = TRUE}

library(dplyr)

library(ggplot2)

mtcars %>% 
  mutate(predictedProbs = predict(logisticTest, type = "response")) %>% 
  ggplot(., aes(mpg, predictedProbs)) +
  geom_line(size = 1.5) +
  theme_minimal()
```


### Cautions About Logistic Regression

Bivariate relationships are really important for logistic models.

Your model needs to see some dispersion of values over the bivariate tables.

Logistic regression requires a larger sample size than what a linear regression needs.

$R^2$ does not apply to a logistic regression.
