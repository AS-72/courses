---
title: "Marketing Experiments"
output:
  revealjs::revealjs_presentation:
    theme: night
    highlight: zenburn
    center: true
    transition: fade
    slide_level: 2
---

## {data-background="https://media.giphy.com/media/14mdHcHFki3Vo4/giphy.gif" data-background-size="contain"}

## Why Experiments?

Beyond being a building block of Science, experiments are important across many business contexts.

A sizable portion of the stuff you interact with (websites, fliers) has been tested through some level of experimentation.

When we need to optimize our marketing expenditures, experiments will provide a *testable* method for making decisions.

## {data-background="https://66.media.tumblr.com/ab162fb6a5df19e1db642cc0aea64c8e/tumblr_n19mihbKl51rqgjjao1_500.gif" data-background-size="contain"}

## 

As <a href = "https://adparlor.com/wp-content/uploads/2015/09/AdParlor-CaseStudy-Facebook-2015-Molson-Coors.pdf">marketing channels</a> increase, experiments are becoming increasingly important.

##

<section data-transition="fade-in slide-out">
    You need to consider...
</section>

<section data-transition="slide">
    Desireability
</section>

<section data-transition="slide">
    Would a customer  want this?
</section>

<section data-transition="slide">
    Feasibility
</section>

<section data-transition="slide">
    Can it actually be done?
</section>

<section data-transition="slide">
    Profitability
</section>

<section data-transition="slide">
    Will it make us money?
</section>

<section data-transition="fade-in">
    Yes needs to be the answer!
</section>

## Modern Needs

Theory-shaping research takes a long time...

But the *customer-centric* world dictates raw speed.

So experiments should be *small* and *fast* for deployment and completion.

## {data-background="https://farm8.static.flickr.com/7123/13763731323_42cf430679_b.jpg"}

In case you don't know...

## {data-background="https://media.giphy.com/media/3o6Ztra7k2FN3MNEDm/giphy.gif" data-background-size="contain"}

## Customer-centric

Satisfaction, while important, is starting to give way towards <a href="https://hbr.org/2016/08/an-emotional-connection-matters-more-than-customer-satisfaction">emotional connection</a>.

Experiments are the only way to establishing, maintaining, and tweaking that emotional connection.

An emotionally-connected customer is a valuable customer.

##

> On a lifetime value basis, emotionally connected customers are more than twice as valuable as highly satisfied customers. These emotionally connected customers buy more of your products and services, visit you more often, exhibit less price sensitivity, pay more attention to your communications, follow your advice, and recommend you more – everything you hope their experience with you will cause them to do. Companies deploying emotional-connection-based strategies and metrics to design, prioritize, and measure the customer experience find that increasing customers’ emotional connection drives significant improvements in financial outcomes.


## Necessary Components

Experiments are used to test *hypotheses*.

They need to be:

<p class="fragment fade-in-then-semi-out">Testable</p>

<p class="fragment fade-in-then-semi-out">Falsifiable</p>

<p class="fragment fade-in-then-semi-out">A research question is not a hypothesis.</p>

## {data-background="http://imgs.xkcd.com/comics/p_values.png" data-background-size="500px"}

## *p*-values are dead!

Not really, but you should understand what you need.

How often are you willing to miss a real effect ($\beta$)?

How often are you willing to be fooled by random chance ($\alpha$)?

These two lead to *statistical power* -- the ability to detect a real effect.

<a href="https://juliasilge.shinyapps.io/power-app/">A handy demonstration</a>

## {data-background="https://i2.wp.com/flowingdata.com/wp-content/uploads/2014/05/Type-I-and-II-errors1.jpg?resize=620%2C465&ssl=1" data-background-size="500px"}

## On Power {data-transition="zoom"}

How do you decide how many people to have in your experiment?

Is it 20 people per condition?

20 people per predictor?

<p class="fragment fade-up">What else did your *Intro to Stats* professor tell you?</p>


## {data-background="https://media1.giphy.com/media/xiMUwBRn5RDLhzwO80/giphy.gif?cid=3640f6095c5f78d96c6439692e325152" data-background-size="contain"}

## Power Analysis

<span class="fragment fade-in">
		<span class="fragment highlight-blue">Effect size</span>
	</span>

<span class="fragment fade-in">
		<span class="fragment highlight-blue">Significance level</span>
</span>

<span class="fragment fade-in">
		<span class="fragment highlight-blue">Power</span>
</span>	
	
##

```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(pwr)

tPower = pwr.t.test(n = NULL, d = 0.1, power = 0.8, 
                    type= "two.sample", alternative = "two.sided")

plot(tPower) + 
  ggplot2::theme_minimal()
```


##

```{r, echo = FALSE, message = FALSE, warning = FALSE}
tPower = pwr.t.test(n = NULL, d = 0.5, power = 0.8, 
                    type= "two.sample", alternative = "two.sided")

plot(tPower) + 
  ggplot2::theme_minimal()
```


## About *t*-tests

Here is a snapshot of some data from 1000 rows:

```{r, echo = FALSE, warning = FALSE, message = FALSE, comment = ""}
library(dplyr)

clicks = data.frame(linksClicked = round(runif(1000, min = 4, max = 10)))

clicks$gender = sample(c("male", "female"), 1000, replace = TRUE) 

clicks$linksClicked = ifelse(clicks$gender == "male", clicks$linksClicked + 3, clicks$linksClicked) 

knitr::kable(head(clicks))
```

## 

Consider these regression results:

```{r, echo = FALSE, warning = FALSE, message = FALSE, comment = ""}
broom::tidy(summary(lm(linksClicked ~ gender, clicks))) %>% 
  knitr::kable()
```

## 

And these summary stats:

```{r, echo = FALSE, warning = FALSE, message = FALSE, comment = ""}
clicks %>% 
  group_by(gender) %>% 
  dplyr::summarize(mean = mean(linksClicked), 
            sd = sd(linksClicked), 
            n = n()) %>% 
  knitr::kable()
```

## 

And this *t*-test output:

```{r, echo = FALSE, warning = FALSE, message = FALSE, comment = ""}
broom::tidy(t.test(linksClicked ~ gender, clicks, alternative = "less")) %>% 
  knitr::kable(col.names = c("difference", "group 1 mean", "group 2 mean", 
                             "t stat", "p", "df", "conf low", "conf high", 
                             "method", "alternative"))
```

## {data-background="https://media.giphy.com/media/26ufdipQqU2lhNA4g/giphy.gif" data-background-size="contain"}

## Computation

$$t = \frac{\bar{x_1} - \bar{x_2}}{{\sqrt{\frac{S^2}{n_1}+\frac{S^2}{n_2}}}}$$  

where:  


$$s^2 = \frac{\Sigma(i_1 - \bar{x_1})^2 + \Sigma(i_2 - \bar{x_2})}{n_1+n_2-2}$$  

## Or...

Just make your life easy:

```{r, eval = FALSE}
BSDA::tsum.test(mean.x, s.x, n.x, 
                mean.y, s.y, n.y)
```

I will say no more...

## On Effect Sizes

Effect sizes provide a standardized metric for the size of a difference.

Cohen's *d* is generally used for *t*-tests.

Computationally, it is pretty simple:

$$ \frac{\mu_1 - \mu_2}{\sigma_{pooled}}$$

Where...

$$\sigma_{pooled} = \sqrt{\frac{(n_1 - 1)SD_1^2 + (n_2 - 1)SD_2^2}{n_1 + n_2 - 2}}$$

## Why Use Effect Sizes?

Consider: 

```{r, echo = FALSE, message = FALSE, warning = FALSE}
datFrame = data.frame(mean = c(60, 55), 
                      sd = c(3, 2.75),
                      n = c(100, 95),
           row.names = c("Offer on landing", "Offer on cart"))

knitr::kable(datFrame, caption = "Time on page")

# compute.es::mes(m.1 = 60, m.2 = 55, sd.1 = 3, sd.2 = 2.75, n.1 = 100, n.2 = 95)
```

And:

```{r, echo = FALSE, message = FALSE, warning = FALSE}
datFrame = data.frame(mean = c(5, 4), 
                      sd = c(.75, .3),
                      n = c(100, 95),
           row.names = c("Emailed offer", "Mailed offer"))

knitr::kable(datFrame, caption = "Store Visits")

# compute.es::mes(m.1 = 5, m.2 = 4, sd.1 = .75, sd.2 = .3, n.1 = 100, n.2 = 95)
```

## 

Both are statistically significant.

But which one would be "more" significant?

## {data-background="https://media1.giphy.com/media/3OSo3PPaXdw0U/giphy.gif?cid=3640f6095c6387354c6f697436b01cb4" data-background-size="contain"}

<p class="fragment fade-in-then-semi-out">The magnitude of the effect is almost identical!</p>

## Necessary Experiments Components

You will need at least two conditions: 

<p class="fragment fade-in-then-semi-out">Treatment</p>

<p class="fragment fade-in-then-semi-out">Control</p>

## On Controls {data-transition="convex"}

The control is how you establish a baseline.

Do not skip out on a control condition!

There is no way to test for an effect if you do not have a control group!

<p class="fragment fade-in-then-semi-out">You do not want to find yourself:</p>

## {data-background="https://media.giphy.com/media/DtLEOehAWfwiY/giphy.gif" data-background-size="contain"}

## On Testing

In many experimental settings, we would want to have observations before and after the application of the treatment. 

You will likely not be afforded such an opportunity. 

<p class="fragment highlight-green">Post-test only experiments are fine here! </p>


## Design -- A Fisherian Perspective

If we go back to conditions, we know that we have at least one treatment and a control group.

This is the basis for our independent variables (i.e., what we have manipulated).

Ideally, people will be randomly assigned to a group. 

## Independent Measures

Also called a between groups design.

In a large portion of our marketing experiments, we are going to be using a between groups design.

In this design, people are assigned to only one condition and tests are conducted across the various groups.

## Repeated Measures

Commonly called a within groups design.

In this type of experiment, participants take part in every condition.

<p class="fragment fade-in-then-semi-out">Counter-balancing is important.</p>

## Factorial Designs

In a factorial design, we have multiple independent variables that get crossed into conditions. 

In a standard $2 X 2$ design, you might have:

```{r, echo=FALSE, warning=FALSE, message=FALSE}
tDat = data.frame(Print = c("New logo, printed", "Old logo, printed"), 
               Digital = c("New logo, digital", "Old logo, digital"), 
               row.names = c("New Logo", "Old Logo"))

knitr::kable(tDat)
```

Does anybody remember that regression issue when values on your outcome can change for a variable based upon the values/levels for another variable?

## Other Research-based Techniques

While not experiments in a strict sense, *conjoint analyses* are useful for determing customer preferences.

*A/B tests* will be our primary focus next time!


## {data-background="https://media0.giphy.com/media/d9zdrVleeAc0g/200.webp?cid=3640f6095c5f8da34836477255719655" data-background-size="contain"}