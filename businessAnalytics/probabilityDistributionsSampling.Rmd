---
title: |
      | Probability, 
      | Distributions,
      | Sampling
output:
  html_document:
    toc: true
    toc_float: true
    theme: flatly
    highlight: zenburn
    css: documentCSS.css
---

```{r setup, include=FALSE, echo = FALSE, warning=FALSE, message = FALSE, comment = ""}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, comment = "")
```

## Necessary Packages

```{r, eval = FALSE}
install.packages(c("animation", "ggplot2", "gridExtra", "plotly", "prob"))
```


## Probability

Probability is a foundation for stats. Even though it provides this nice foundation, it can be useful by itself. For example, you might be interested to know the probability that someone will quit within 6 months of working. To find the probability, all we need to do is to take the target outcome and divide it by all possible outcomes: $$P = \frac{target}{all \, \,outcomes}$$. If we see that we have 1200 total employees and 300 of them quit at 6 months, we would find a probability of $$300 / 1200 = .25$$. We could determine that people have a 25% chance of quitting at the 6-month mark.

### Joint Probability

Not everything occurs in isolation. Some things, like turnover, might have several things happening at the same time.

Let's look at the following table:

```{r}
library(dplyr)

data.frame(quitNY = c(0, 1, 0, 1), 
           department = c("Shipping", "Customer Service", 
                          "Customer Service", "Customer Service")) %>% 
  knitr::kable()
```

If we look at the probability of an employee quitting, we can see that we would have .5 -- the number of target events, quitting, divided by the total number of events. Is that the whole story? Not really. What is the probability that someone works in "Customer Service" (3 / 4 = .75). If we assume that these are not related to each other (in other words, one event does not influence the other), we can just multiply these probability values to find the joint probability of of working in "Customer Service" and quitting ($$P(quitting, CS)$$:

```{r}
.5 * .75
```

This gives us the ability to say 37.5% of the employees worked in customer service *and* quit. 


### Conditional Probability

Everything we just discussed assumes a level of independence. However, we might be faced with a situation in which we have *conditional probability* -- what is the probability that something will occur given the probability of something occuring before. Playing cards offer a nice example here. If we know that there are 4 aces in a deck of 52 playing cards, then we have a *P(A)* of 4/52 for drawing an ace. If we draw another card, what is the probability that it will also be an ace? It is a *P(B)* of 3/51. We can annotate this as *P(B|A) = 3/51*. If that first card was not an ace, our second card would have a probability of 4/51 of being an ace. Now we have two separate probability statements:

$$P(B|A) = 3/51, \, P(B|A^C) = 4/51$$

It would look like this:

$$P(B|A) = \frac{P(A \cap B)}{P(A)}$$

If we want to know the probability that we are going to get two aces, we can compute it as follows:

$$P(both\,aces) = P(A \cap B) = P(A)*P(B|A)$$

```{r}
pA = 4/52

pBA = 3/51

pA * pBA
```


Let's try it out:

```{r}
library(prob)

playingCards = cards()

# The following produces every possible combination of 2 drawn cards!

draw2Cards = urnsamples(playingCards, size = 2)

# The following will give the probability of each event.

probabilitySpace = probspace(draw2Cards)

# Now we can look at the probability of getting two aces.

Prob(probabilitySpace, all(rank == "A"))
```

Now let's go back to our example about employees:

```{r}
data.frame(quitNY = c(0, 1, 0, 1), 
           department = c("Shipping", "Customer Service", 
                          "Customer Service", "Customer Service")) %>% 
  knitr::kable()
```

We can take our conditional probability and expand it to the Bayes Theorem. 

$$ Pr(A|B) = \frac{Pr(A)Pr(B|A)}{Pr(B)} $$

Let's break it down a little bit more. $Pr(A|B)$ is given the conditional probability of A given that B occurred. This is what is known as the *posterior probability*. This is the probability of a customer service employee quitting $Pr(quit|CS)$. 

Now we come to the *prior probability* -- $Pr(A)$. This is the probability of quitting by itself -- $Pr(quit)$ We would get a probability of .5

Next comes the *likelihood*, which is the inverse of the posterior probability -- $Pr(B|A)$. This is the likelihood that any person in customer service quit -- $Pr(CS|quit)$ How many CS people are there? There are 3. How many quit? Two employees quit. Therefore, $2/3 = .67$.

We also have our *marginal probability* -- $Pr(CS)$. Recall that we have 3 customer service employees out of 4 total employees, so our probability of being a customer service employee is .75.

If we put it all together, we get:

$$ Pr(quit|CS) = \frac{.5 * .67}{.75} = .45$$
This means that the percent probability that someone will quit, given that they work in customer service, is 45%.

$$ Pr(quit|cs) = \frac{Pr(cs|quite)Pr(quit)}{Pr(cs)} $$
We can also use Bayes Theorem for looking at base rates!

Vampirism, while on a temporary cultural downward trend, will always be of interest to some. Let's suppose that we have a test for vampirism. This test will correctly detect vampirism 95% of the time, but 1% of the time there is a false positive (i.e., it says that someone is a vampire, when they are just a mere mortal). Thankfully, vampires are an exceedingly rare lot -- only .1% of the population is actually a vampire. 

So the natural question would be: what is the probability that you are actually a vampire if you test positive?

We can use the Bayes Theorem to find our answer.

We can first see it in the standard form:

$$ Pr(vampire|positive) = \frac{Pr(positive|vampire)Pr(vampire)}{Pr(positive)} $$

We can get the average probability of a positive test as follows:

$$ Pr(positive) = Pr(positive|vampire)Pr(vampire)+Pr(positive|mortal)(1-Pr(vampire)$$

We can code this in R as follows:

```{r}
positiveVampire = .95

falsePositive = .01

probabilityVampire = .001

probabilityPositive = positiveVampire * probabilityVampire + 
  falsePositive * (1 - probabilityVampire)

probabilityActualVampire = positiveVampire * probabilityVampire / probabilityPositive

probabilityActualVampire
```

So, there is roughly an 8.7% chance that a person who tests positive as a vampire is actually a vampire.

This notation for probability, while certainly useful, can be a bit difficult to get your head into -- mostly because we don't deal in probability very much. We do, however, deal with frequency...frequently.

Let's restate our problem in terms of frequency:

1.  In a population of 100000 people, 100 of them are vampires (.01%)
2.  Of those 100 vampires, 95 will test positive for vampirism (95% true positive)
3.  Of the 99900 mortal, 999 will test positive (1% false positive)

How many people, in total, will test positive?

```{r}
truePositive = 95

falsePositive = 999

totalPositive = truePositive + falsePositive

totalPositive
```

Now, of those `r totalPositive` positive tests, what is the probability of being a vampire:

```{r}
truePositive / totalPositive
```

While this might seem silly and contrived (it is), it provides a nice demonstration for what happens in most of our statistics:

1. We specify some state of the world (e.g., our hypotheses is false or not)
2. We get some type of imperfect information about our hypothesis (data)
3. We determine some type of uncertainty estimate

While we will get into during the coming weeks, this is essentially what we are doing when we are specifying the power of a test (probability of a hypothesis being correct is .8) and our significance level (there is less than a .05 chance that we have a false positive).

## The Normal Distribution

The normal distribution should not be too much of a mystery to us.

```{r}
library(ggplot2)

set.seed(1001)

population = data.frame(population = rnorm(n = 1000000, mean = 0, sd = 1))

regions = data.frame(sdPlus1 = mean(population$population) + sd(population$population), 
                     sdMinus1 = mean(population$population) - sd(population$population), 
                     sdPlus2 = mean(population$population) + (2 * sd(population$population)), 
                     sdMinus2 = mean(population$population) - (2 * sd(population$population)), 
                     sdPlus3 = mean(population$population) + (3 * sd(population$population)), 
                     sdMinus3 = mean(population$population) - (3 * sd(population$population)))

ggplot(population, aes(population)) +
  geom_density() +
  theme_minimal()

```


If we are observing a population that is normally distributed, we can know some things about it: the mean and the standard deviation. We also know that the mean, median, and mode are all the same. 

There is also a convenient rule: the 68-95-99.7 rule. This rule dictates that 68% of the distribution is contained within $\pm1\sigma$, 95% is contained within $\pm2\sigma$, and 99.7% is contained within $\pm3\sigma$. It is not functionally part of the rule, but 99.99% is contained under $\pm4\sigma$.

```{r}
ggplot(population, aes(population)) +
  geom_density() +
  geom_vline(xintercept = regions$sdPlus1, color = "red") +
  geom_vline(xintercept = regions$sdMinus1, color = "red") +
  geom_vline(xintercept = regions$sdPlus2, color = "blue") +
  geom_vline(xintercept = regions$sdMinus2, color = "blue") +
  geom_vline(xintercept = regions$sdPlus3, color = "green") +
  geom_vline(xintercept = regions$sdMinus3, color = "green") +
  theme_minimal()
```


The normal distribution is important, as many things are naturally normally distributed.


```{r}
pos = replicate(1000, sum(runif(16, -1, 1)))

plot(density(pos))
```


```{r}
library(plyr)

library(ggplot2)

dataSteps = function(stepSize) {
  walks = data.frame(person = rep(1:100, each = stepSize), 
           position = unlist(rlply(100, cumsum(c(0, runif((stepSize - 1), -1, 1))))), 
           step = rep(1:stepSize, times = 100))
  
  return(walks)
}

walks = dataSteps(16)

ggplot(walks, aes(step, position, group = person)) + 
  geom_line(color = "#ff5500", alpha = .5) + 
  theme_minimal()
```


### Galton Boards

```{r, eval = FALSE}
library(animation)

ani.options(nmax = 215, interval = .5, autoplay = FALSE)

quincunx()
```

Although not requisite, we might want to standardize our variables to fit a standard normal distribution: 

$$z = \frac{x_i-\mu}{\sigma}$$

Transforming variables into *z*-scores makes it easy to compare values.

For example, we might take a person who scored a 3.2 on the auditor exam (CIA) in 2015. In 2014, a different auditor also scored a 3.2. Clearly, they both scored the same; however, the story should not end there. Let's consider the following information:

In 2014, the CIA had $\mu=3.04$ with a $\sigma=1.41$.

In 2015, the CIA had $\mu=2.86$ with a $\sigma=1.34$.

With that knowledge, which CIA examinee performed better compared to the population of examinees?

```{r, echo = TRUE}
person2014Z = (3.2 - 3.04) / 1.41

person2015Z = (3.2 - 2.86) / 1.34
```

Let's play with this for a little bit:


```{r, echo = TRUE}
ciaExam2014 = rnorm(n = 5000, mean = 3.04, sd = 1.41)

plot(density(ciaExam2014))

abline(v = c(3.04, 3.2), col = c("black", "red"))
```

We can find out how many people our person in 2014 bested:

```{r, echo = TRUE}
pnorm(3.2, 3.04, 1.41)

pnorm(person2014Z)
```

You can see that giving the *z* or the actual values produced the same results!

We can consider that everything that falls under our curve is 1 (i.e., it is 100%). So, if we wanted to find the proportion of people doing better than our reference people, we would just subtract our distribution function from 1.

```{r}
1 - pnorm(person2014Z)
```

We can also through a proportion to qnorm to find z.

```{r, echo = TRUE}
qnorm(.45)
```

We can also use the `vistributions` package to play around with probability distributions visually.

```{r, eval = FALSE}
install.packages(c("shinyBS", "vistributions"))

vistributions::vdist_launch_app()
```


## Populations and Samples

### Central Limit Theorem

The CLT dictates that as we increase the number of samples from a population, we will begin to approach normally distributed means.

```{r}
library(gridExtra)

set.seed(123)

r = 10000

n = 200     

sample.means = function(samps, r, n) {
  rowMeans(matrix(samps, nrow = r, ncol = n))
}

qqplot.data = function(vec) {
  y = quantile(vec[!is.na(vec)], c(0.25, 0.75))
  x = qnorm(c(0.25, 0.75))
  slope = diff(y) / diff(x)
  int = y[1L] - slope * x[1L]

  d = data.frame(resids = vec)
  
  return(d)
}

generate.plots = function(samps, samp.means) {
  p1 = qplot(samps, geom = "histogram", bins = 30, 
             main = "Sample Histogram") + 
    theme_minimal()
  p2 = qplot(samp.means, geom = "histogram", bins = 30, 
             main="Sample Mean Histogram") + 
    theme_minimal()
  grid.arrange(p1,p2,ncol=2)
}
```


### Uniform

Likely the most vanilla of all distributions, the uniform distribution is pretty simple. We don't even get any fancy Greek letters to give us an idea about its shape, just a minimum and a maximum. Why? Because knowing the min and max will tell us that there is an equal probability of drawing a value anywhere within that range.

```{r}
samps = runif(r*n)

samp.means = sample.means(samps, r, n)

generate.plots(samps, samp.means)
```


### Poisson

The Poisson is an interesting distribution -- it tends to deal with count-related variables (and will be the distribution we use when we get into Poisson regression). It tells us the probability of a count occurring.  We know its $\lambda$, or average number of events (incidence rate).

```{r}
samps = rpois(r * n, lambda = 3)

samp.means = sample.means(samps, r, n)

generate.plots(samps, samp.means)
```


### Exponential

The exponential distribution is excellent when we are looking at how long something lasts or arrivals within a process (car part life, people joining a line, survival). We can only know one thing about the exponential distribution: $\mu$

```{r}
samps = rexp(r * n, rate = 1)

samp.means = sample.means(samps, r, n)

generate.plots(samps, samp.means)
```


## Other Distribution Fun

Let's start with a Gaussian distribution of 10000 observations:

```{r, echo = TRUE}
set.seed(1001)

population = rnorm(10000)

plot(density(population))
```

Now, let's take a small sample (*n* = 75) of our population:

```{r, echo = TRUE}
set.seed(1001)

smallSample = sample(population, 75, replace = FALSE)

plot(density(smallSample))
```

And now something a little bigger (*n* = 250):

```{r, echo = TRUE}
set.seed(1001)

mediumSample = sample(population, 250, replace = FALSE)

plot(density(mediumSample))
```

And bigger still (*n* = 1000):

```{r, echo = TRUE}
set.seed(1001)

biggerSample = sample(population, 1000, replace = FALSE)

plot(density(biggerSample))
```


And finally *n* = 2500:

```{r, echo = TRUE}
set.seed(1001)

biggestSample = sample(population, 2500, replace = FALSE)

plot(density(biggestSample))
```

Original:

```{r}
plot(density(population))
```


### What Is The Point?

We had our "population", so how well did our samples replicate the population distribution?

This starts to illustrate the *t*-distribution (more on this in a few weeks).

We are also getting into issues related to point estimation.

Let's consider the following:

```{r}
mean(population)

mean(biggerSample)

mean(biggestSample)
```


We can even take another sample from our population:

```{r}
mean(sample(population, 2500, replace = FALSE))
```

Let's take a bigger sample:

```{r}
mean(sample(population, 5000, replace = FALSE))
```

In and of itself, this is interesting. It has applications, however, to null hypothesis significance testing.

