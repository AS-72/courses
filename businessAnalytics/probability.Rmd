---
title: "Probability"
output: distill::distill_article
---

```{r setup, include=FALSE, echo = FALSE, warning=FALSE, message = FALSE, comment = ""}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, comment = "")
```


## Necessary Packages

We will need the following packages:

```{r, eval = FALSE}
install.packages(c("animation", "ggplot2", "gridExtra", "plotly", "prob"))
```


## Probability

Probability is a foundation for stats. Even though it provides this nice foundation, it can be useful by itself. For example, you might be interested to know the probability that someone will quit within 6 months of working. To find the probability, all we need to do is to take the target outcome and divide it by all possible outcomes: $$P = \frac{target}{all \, \,outcomes}$$. If we see that we have 1200 total employees and 300 of them quit at 6 months, we would find a probability of $300 / 1200 = .25$. We could determine that people have a 25% chance of quitting at the 6-month mark. Just as a reminder, probability is 0-1 bound.

### Why It's Important

Probability creeps up everywhere -- people are always curious to know about the chances of something happening. When talking about classification (e.g., determining if an employee is at risk for turnover or not), probability values can be assigned. It also allows for a very simple value that most people will understand.

### Joint Probability

Not everything occurs in isolation. Some things, like turnover, might have several things happening at the same time.

Let's look at the following table:

```{r, echo = FALSE}
library(dplyr)

data.frame(quitNY = c(0, 1, 0, 1), 
           department = c("Shipping", "Customer Service", 
                          "Customer Service", "Customer Service")) %>% 
  knitr::kable()
```

If we look at the probability of an employee quitting, we can see that we would have .5 -- the number of target events, quitting, divided by the total number of events. Is that the whole story? Not really. What is the probability that someone works in "Customer Service" (3 / 4 = .75). If we assume that these are not related to each other (in other words, one event does not influence the other), we can just multiply these probability values to find the joint probability of of working in "Customer Service" and quitting -- $P(quitting, CS)$:

```{r}
.5 * .75
```

This gives us the ability to say 37.5% of the employees worked in customer service *and* quit. 


### Conditional Probability

Everything we just discussed assumes a level of independence. However, we might be faced with a situation in which we have *conditional probability* -- what is the probability that something will occur given the probability of something occuring before. Playing cards offer a nice example here. If we know that there are 4 aces in a deck of 52 playing cards, then we have a *P(A)* of 4/52 for drawing an ace. If we draw another card, what is the probability that it will also be an ace? It is a *P(B)* of 3/51. We can annotate this as *P(B|A) = 3/51*. If that first card was not an ace, our second card would have a probability of 4/51 of being an ace. Now we have two separate probability statements:

$$P(B|A) = 3/51, \, P(B|A^C) = 4/51$$

It would look like this:

$$P(B|A) = \frac{P(A \cap B)}{P(A)}$$

If we want to know the probability that we are going to get two aces, we can compute it as follows:

$$P(both\,aces) = P(A \cap B) = P(A)*P(B|A)$$

```{r}
pA = 4/52

pBA = 3/51

pA * pBA
```


Let's try it out:

```{r}
library(prob)

playingCards = cards()

# The following produces every possible combination of 2 drawn cards!

draw2Cards = urnsamples(playingCards, size = 2)

# The following will give the probability of each event.

probabilitySpace = probspace(draw2Cards)

# Now we can look at the probability of getting two aces.

Prob(probabilitySpace, all(rank == "A"))
```

Now let's go back to our example about employees:

```{r, echo = FALSE}
data.frame(quitNY = c(0, 1, 0, 1), 
           department = c("Shipping", "Customer Service", 
                          "Customer Service", "Customer Service")) %>% 
  knitr::kable()
```

We can take our conditional probability and expand it to the Bayes Theorem. 

$$ Pr(A|B) = \frac{Pr(A)Pr(B|A)}{Pr(B)} $$

Let's break it down a little bit more. $Pr(A|B)$ is given the conditional probability of A given that B occurred. This is what is known as the *posterior probability*. This is the probability of a customer service employee quitting -- $Pr(quit|CS)$. 

Now we come to the *prior probability* -- $Pr(A)$. This is the probability of quitting by itself -- $Pr(quit)$ We would get a probability of .5.

Next comes the *likelihood*, which is the inverse of the posterior probability -- $Pr(B|A)$. This is the likelihood that any quitters were in CS -- $Pr(CS|quit)$ -- the probability of being in CS given that a person quit. How many quit? Two employees quit. How many of the quitters were in CS? Two. Therefore, $2/2 = 1$.

We also have our *marginal probability* -- $Pr(CS)$. Recall that we have 3 customer service employees out of 4 total employees, so our probability of being a customer service employee is .75.

If we put it all together, we get:

$$ Pr(quit|CS) = \frac{.5 * 1}{.75} = .67$$

This means that the percent probability that someone will quit, given that they work in customer service, is 67%.

$$ Pr(quit|cs) = \frac{Pr(cs|quite)Pr(quit)}{Pr(cs)} $$

This gives us a pretty good idea about base rates. If you just looked at how many people in customer service quit, you might overstate/understate that probability a bit (you would not be paying attention to the base rates and additional information). However, by working through the conditional probability, we can be a lot more confident in our probability value.

Here is a play on the classic example:

Vampirism, while on a temporary cultural downward trend, will always be of interest to some. Let's suppose that we have a test for vampirism. This test will correctly detect vampirism 95% of the time, but 1% of the time there is a false positive (i.e., it says that someone is a vampire, when they are just a mere mortal). Thankfully, vampires are an exceedingly rare lot -- only .1% of the population is actually a vampire. 

So the natural question would be: what is the probability that you are actually a vampire if you test positive?

We can use the Bayes Theorem to find our answer.

We can first see it in the standard form:

$$ Pr(vampire|positive) = \frac{Pr(positive|vampire)Pr(vampire)}{Pr(positive)} $$

We can get the average probability of a positive test as follows:

$$ Pr(positive) = Pr(positive|vampire)Pr(vampire)+Pr(positive|mortal)(1-Pr(vampire)$$

We can code this in R as follows:

```{r}
positiveVampire = .95

falsePositive = .01

probabilityVampire = .001

probabilityPositive = positiveVampire * probabilityVampire + 
  falsePositive * (1 - probabilityVampire)

probabilityActualVampire = positiveVampire * probabilityVampire / probabilityPositive

probabilityActualVampire
```

So, there is roughly an 8.7% chance that a person who tests positive as a vampire is actually a vampire. This really changes the complexion of the problem!

This notation for probability, while certainly useful, can be a bit difficult to get your head into -- mostly because we don't deal in probability very much. We do, however, deal with frequency...frequently.

Let's restate our problem in terms of frequency:

1.  In a population of 100000 people, 100 of them are vampires (.01%)
2.  Of those 100 vampires, 95 will test positive for vampirism (95% true positive)
3.  Of the 99900 mortal, 999 will test positive (1% false positive)

How many people, in total, will test positive?

```{r}
truePositive = 95

falsePositive = 999

totalPositive = truePositive + falsePositive

totalPositive
```

Now, of those `r totalPositive` positive tests, what is the probability of being a vampire:

```{r}
truePositive / totalPositive
```

While this might seem silly and contrived (it is), it provides a nice demonstration for what happens in most of our statistics:

1. We specify some state of the world (e.g., our hypotheses is false or not)
2. We get some type of imperfect information about our hypothesis (data)
3. We determine some type of uncertainty estimate

While we will get into during the coming weeks, this is essentially what we are doing when we are specifying the power of a test (probability of a hypothesis being correct is .8) and our significance level (there is less than a .05 chance that we have a false positive).

