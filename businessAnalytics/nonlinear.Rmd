---
title: "The Nonlinear World"
output: distill::distill_article
---

```{r setup, include=FALSE, echo = FALSE, warning=FALSE, message = FALSE, comment = ""}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, comment = "")
```

Linear regression is a powerful tool and can be used on a great number of problems. However, not everything is linear. Within many discipline, there are well-known non-linear effects (e.g., magnetic force and distance, sound pressure and distance). Stress and performance provides an excellent example. Stress comes in a few different forms, namely *eustress* and *distress*. We should all know what distress is, but eustress is commonly called positive stress. It can be experienced whenever we encounter fun challenges or problems. When you have all of the resources (emotional and physical) to cope with a challenge, you should experience eustress. The stress specturm has an interesting relationship to job performance, in that we tend to see an inverted U. At the lowest levels of stress (what we might call complete calm), people tend to not perform at their best. As stress increases (approaching eustress), we see a rapid increase in performance levels. As stress hits a certain point, it becomes distress and performance drops off quickly. It might look something like the following:

```{r, echo = FALSE}

library(ggplot2)

library(simstudy)

def <- defData(varname = "stressLevels", dist = "uniform", formula = '0;100')

theta1 <- c(0.1, .2, .3, .5, .8, .5, .3, .2, .1)

knots <- c(0.25, 0.5, 0.75)

stressPerformance <- genData(1000, def)

stressPerformance <- genSpline(dt = stressPerformance, newvar = "performance",
                predictor = "stressLevels", theta = theta1,
                knots = knots, degree = 5,
                newrange = "10;100",
                noise.var = 1)

ggplot(stressPerformance, aes(stressLevels, performance)) + 
  geom_point() +
  theme_minimal()
```

You might need the following packages:

```{r, eval = FALSE}
install.packages(c("car", "caret", "mgcv", "splines"))
```



Let's read in some similar data and play around:

```{r}
stressPerformance = read.csv("http://www.nd.edu/~sberry5/data/stressPerformance.csv")
```

The `car` package is old-school R, but still has some handy stuff for us. 

```{r}
library(car)

scatterplotMatrix(stressPerformance)
```

The splom gives us a really good idea about the relationships within the data -- don't use if for big data sets, but having the bivariate relationships in one place can be handy for initial data exploration.

We can see very quickly what would happen if we fit a linear regression model to this data:

```{r}
ggplot(stressPerformance, aes(stressLevels, performance)) + 
  geom_point() +
  geom_smooth(method = "lm") +
  theme_minimal()
```

And that would be pretty close to a 0 coefficient. We clearly see a relationship, but the linear model is not exactly built for such relationships.

```{r}
linearMod = lm(performance ~ stressLevels, 
               data = stressPerformance)

summary(linearMod)

caret::RMSE(linearMod$fitted.values, stressPerformance$performance)
```

We can see that our linear model does not do very well and our root mean squared error (RMSE) is pretty bad.

<aside>
The RMSE is defined as the standard deviation of the residuals. The lower the value, the less dispersion there is around our regression line. It is a popular metric for assessing prediction accuracy.
</aside>

# Polynomial Regression

When encountering something that looks nonlinear, one of the first steps is to transform the predictor variable. We can take our predictor variable, stressLevels, and take it to a higher power.

```{r}
ggplot(stressPerformance, aes(stressLevels, performance)) + 
  geom_point() +
  stat_smooth(method = "lm", formula = y ~ x + I(x^2)) +
  theme_minimal()
```

While that looks pretty good, we could try to go up one more term:

```{r}
ggplot(stressPerformance, aes(stressLevels, performance)) + 
  geom_point() +
  stat_smooth(method = "lm", formula = y ~ x + I(x^2) + I(x^3)) +
  theme_minimal()
```

We could keep going, but we aren't going to see too much improvement past a certain point.

For the sake of it, let's see how a model will perform:

```{r}
summary(lm(performance ~ stressLevels + I(stressLevels^2), 
           data = stressPerformance))
```

For this model, we have: $y = b_0 + b_1X + b_2X^2$. The coefficient for $b_1$ will give us the rate of change (just like always) and $b_2$ can tell us the direction and steepness of the curve (a positive value means the curve opens upwards, while a negative value is downwards).

As a shortcut, we could just use the `poly` function:

```{r}
ggplot(stressPerformance, aes(stressLevels, performance)) + 
  geom_point() +
  stat_smooth(method = "lm", formula = y ~ poly(x, 2, raw = TRUE)) +
  theme_minimal()
```

We could also see which model has the best performance:

```{r}
quadMod = lm(performance ~ poly(stressLevels, 2), 
             data = stressPerformance)

summary(quadMod)
```

```{r}
cubicMod = lm(performance ~ poly(stressLevels, 3), 
              data = stressPerformance)

summary(cubicMod)
```

```{r}
caret::RMSE(quadMod$fitted.values, stressPerformance$performance)
```

This gets to an interesting point -- we see that we have a nonlinear relationship, but we are still using the linear regression. Why might that be? It is because our coefficients are still linear (there is also this wild stuff about the parameters being linear in multidimensional space -- check it out sometime). In other words, we are applying the higher order term to the data, not the coefficient.

# Non-linear Regression

Real talk time -- a non-linear regression is really hard to fit. Why is it hard to fit? You have to know the form that your nonlinearity takes so that you can model that form. If you don't know the mathematical shape of your nonlinear function, then you would be guessing. Some things in the world take well-known mathematical forms and others could be a wiggly mess.

Since our inverted U is a parabola, we can supply the necessary formula: $a(x - b)^2 + c$. The *a* paramter controls our parabola's direction -- a positive value makes a normal U and a negative makes an inverted U. It also controls how steep the curve is. The *b* and *c* parameters specify the vertex.

We also have to supply some initial starting points for the nonlinear regression. Whenever we deal in nonlinear space, crazy things happen and we will see more of that when we talk about optimization.

```{r}
nonLinearMod = nls(performance ~ I((-a * (stressLevels - b)^2) + c), 
                   data = stressPerformance, 
                   start = list(a = .5, b = 20, c = 20))

summary(nonLinearMod)
```

What can we take from this? Really just that our data fits this model pretty well.

We also get the following formula: $-.01853(stressLevels - 49.6278)^2 + 62.35989$

```{r}
predict(nonLinearMod, newdata = stressPerformance)[1]

(-.01853*(stressPerformance$stressLevels[1] - 49.6278)^2) + 62.35989
```

That is pretty good!

```{r}
caret::RMSE(fitted(nonLinearMod), stressPerformance$performance)

plot(stressPerformance$stressLevels, stressPerformance$performance)

points(stressPerformance$stressLevels, predict(nonLinearMod, newdata = stressPerformance), 
       col = "cornflowerblue")
```


# Smoothing

There are many types of smooths. You will frequently see the loess (local regression – sometimes you will hear it as locally-weighted scatterplot smoothing or lowess). With a loess line, we are fitting some polynomial (generally the linear or the quadratic) to a small section of our data at a time (i.e., a local group) – this is a little bit more complicated than our moving average window type of smooth. Each small section has an associated line and each line gets joined with the line in the next group (these are referred to as *knots*). Since we are largely in control here, we get to specify how wiggly things might get.

```{r}
library(splines)

knots = quantile(stressPerformance$stressLevels, p = c(0.25, 0.5, 0.75))

splineModel = lm(performance ~ bs(stressLevels, knots = knots), 
                 data = stressPerformance)

summary(splineModel)
```

Check out those coefficients! What do they mean? Nobody knows! Once we start fitting splines, the coefficients are not interpretable. What is important, though, is the prediction.

```{r}
caret::RMSE(splineModel$fitted.values, stressPerformance$performance)
```

We can see that our prediction rate is getting better now.

## GAM

Very briefly, an additive model is not much different than our normal interpretation of a model. In our additive model, we can look at the effect of a predictor on a dependent variable without any consideration for what other variables might be in the model. We can add these effects to best predict our response.

```{r}
plot(performance ~ stressLevels, data = stressPerformance)
lines(sort(stressPerformance$stressLevels),
      fitted(lm(performance ~ stressLevels, 
                data = stressPerformance))[order(stressPerformance$stressLevels)], col = "red")
lines(sort(stressPerformance$stressLevels), 
      fitted(lm(performance ~ I(stressLevels^2), 
                data = stressPerformance))[order(stressPerformance$stressLevels)], col = "blue")
lines(sort(stressPerformance$stressLevels), 
      fitted(lm(performance ~ I(stressLevels^3), 
                data = stressPerformance))[order(stressPerformance$stressLevels)], col = "green")
```

The preceding figure shows us 3 different lines: a linear regression line, and two higher-order trends. We will use them as a reference.

Let’s check this out:

```{r}
lmTest = lm(performance ~ stressLevels, data = stressPerformance)

summary(lmTest)
```


Nothing too new here, so let’s move along!

```{r}
library(mgcv)

gamTest = gam(performance ~ stressLevels, data = stressPerformance)

summary(gamTest)
```

You should notice that there is no difference between our standard linear model and our gam with regard to the coefficient. If we do not smooth a variable, it gets treated just like it would in a linear regression model. We also get some output such as adjusted R^2 (interpreted as per normal) and we also have deviance explained, which is giving us very similiar information to adjusted R^2 (instead of looking at the sums of square error between fitted and observed, it just uses a different error calculation). The scale estimate, in this case, is the residual standard error squared. GCV is the minimized generalised cross-validation and it gives us an idea about our prediction error (ideally, we want this to be a small value).

Let’s try to smooth. In the following code, you will notice how we wrapped out term in <span class="func">s</span>(). Believe it or not, this is to specify a smooth term. We could spend a whole week on different ways to smooth things, but we will just stick with <span class="func">s</span>() and its defaults for now.

```{r}
gamTestSmooth = gam(performance ~ s(stressLevels), 
                    data = stressPerformance)

summary(gamTestSmooth)

plot(gamTestSmooth)
```

After smoothing our term, we can see that our output has changed. Instead of getting a linear regression coefficient, we get an edf (estimated degrees of freedom). While these edf values lack the clean interpretation of our linear regression coefficients, we can still get a great deal of information from them. The closer edf is to 1, the more linear in nature the term actually is. However, as edf goes beyond 1, we have an increasingly wigglier relationship. 

Since we included a smooth term, we can see that our model fit has improved from our previous gam without a smooth term.

```{r}
caret::RMSE(gamTestSmooth$fitted.values, stressPerformance$performance)
```


If we plot our newly-fitted gam model back onto our previous visualization, here is what we get:

```{r}
plot(performance ~ stressLevels, data = stressPerformance)
lines(sort(stressPerformance$stressLevels),
      fitted(lm(performance ~ stressLevels, 
                data = stressPerformance))[order(stressPerformance$stressLevels)], col = "red")
lines(sort(stressPerformance$stressLevels), 
      fitted(lm(performance ~ stressLevels + I(stressLevels^2), 
                data = stressPerformance))[order(stressPerformance$stressLevels)], col = "blue")
lines(sort(stressPerformance$stressLevels), 
      fitted(lm(performance ~ stressLevels + I(stressLevels^2) + I(stressLevels^3), 
                data = stressPerformance))[order(stressPerformance$stressLevels)], col = "green")
lines(sort(stressPerformance$stressLevels), 
      fitted(gam(performance ~ s(stressLevels), data = stressPerformance))[order(stressPerformance$stressLevels)], 
      col = "orange") 
```

The soft orange line is our gam fit. This is where things are getting interesting. We can see that our gam line is not really a parabola like the polynomial terms. 

## Bias/Variance Trade-Off

The wiggle can be controlled and you are the one to control it (all models are your monster, so build them in a way that you can control it). An important consideration to make with the wiggle is the bias/variance trade-off. You will see this called other things (e.g., error/variance), depending on with whom you are hanging around. Since we have only talked about bias briefly, we do not need to worry about getting bias in this sense conflated with anything else.

It works like this: you cannot have your cake and eat it too. Do you want your in-sample predicition to be awesome (low bias)? Great! You can count on getting that at the expense of higher variance. The lower the variance, the better your model will predict new data. Well that sounds easy – just go with the lowest variance. But...that might contribute to missing some weird pattern. Again, it is just a decision to make (you likely won't be facing off with your monsters in the Arctic in the end).

<aside>
While conceptually dissimilar, the bias/variance trade is in the same boat as Type I/Type II errors (or sensitivity -- true positive -- and specificity -- true negative -- in machine learning parlance). They are just decisions that need to be made.
</aside>

With our gam models, the wigglier your line, the lower your bias will be and the better you are doing at predicting in sample. 

```{r}
library(ggplot2)

gamTestLambda1 = gam(performance ~ s(stressLevels, sp = 0, k = 80, bs = "cr"), 
                     data = stressPerformance, 
                     method = "REML")

p = predict(gamTestLambda1, type = "lpmatrix")

beta = coef(gamTestLambda1)

s = p %*% beta

plotDat = cbind.data.frame(s = s, stress = na.omit(stressPerformance$stressLevels))

gam1Plot = ggplot(plotDat, aes(stress, s)) + 
  geom_line(color = "#ff5500", size = 2.5) +
  geom_point(data = stressPerformance, aes(stressLevels, performance), alpha = .5) +
  theme_minimal()

gamTestLambda9 = gam(performance ~ s(stressLevels, sp = .9, k = 40), 
                     data = stressPerformance)

p = predict(gamTestLambda9, type = "lpmatrix")

beta = coef(gamTestLambda9)

s = p %*% beta

plotDat = cbind.data.frame(s = s, stress = na.omit(stressPerformance$stressLevels))

gam9Plot = ggplot(plotDat, aes(stress, s)) + 
  geom_line(color = "#ff5500", size = 2.5) +
  geom_point(data = stressPerformance, aes(stressLevels, performance), alpha = .5) +
  theme_minimal()

library(gridExtra)

gridExtra::grid.arrange(gam1Plot, gam9Plot)
```

In the top plot, we have allowed our line a bit more flexibility to wiggle -- you can see the line bending more to fit the pattern within your data. We are going to get very good in-sample prediction here, at the expense of out-of-sample prediction. The bottom plot, is a bit more reserved. It will undoubtedly do better out-of-sample, but might be missing something within the in-sample data.

Here is a closer look at it:

```{r}
gam1Plot
```


```{r}
caret::RMSE(gamTestLambda1$fitted.values, stressPerformance$performance)
```

## Your Turn

Use the `mtcars` data (it is already there). Do some visual exploration and look at the relationship between `mpg` and `disp`. For fun, try to fit a linear model to the data. After fitting that model, try any of the non-linear stuff we just discussed. Does one do better than the other?