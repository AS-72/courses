---
title: "Distributions"
output: distill::distill_article
---

```{r setup, include=FALSE, echo = FALSE, warning=FALSE, message = FALSE, comment = ""}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, comment = "")
```

### The Normal Distribution

The normal distribution should not be too much of a mystery to us.

```{r}
library(ggplot2)

library(gridExtra)

set.seed(123)

r = 10000

n = 200     

population = data.frame(population = rnorm(n = 1000000, mean = 0, sd = 1))

sample.means = function(samps, r, n) {
  rowMeans(matrix(samps, nrow = r, ncol = n))
}

qqplot.data = function(vec) {
  y = quantile(vec[!is.na(vec)], c(0.25, 0.75))
  x = qnorm(c(0.25, 0.75))
  slope = diff(y) / diff(x)
  int = y[1L] - slope * x[1L]

  d = data.frame(resids = vec)
  
  return(d)
}

generate.plots = function(samps, samp.means) {
  p1 = qplot(samps, geom = "histogram", bins = 30, 
             main = "Sample Histogram") + 
    theme_minimal()
  p2 = qplot(samp.means, geom = "histogram", bins = 30, 
             main="Sample Mean Histogram") + 
    theme_minimal()
  grid.arrange(p1,p2,ncol=2)
}

regions = data.frame(sdPlus1 = mean(population$population) + sd(population$population), 
                     sdMinus1 = mean(population$population) - sd(population$population), 
                     sdPlus2 = mean(population$population) + (2 * sd(population$population)), 
                     sdMinus2 = mean(population$population) - (2 * sd(population$population)), 
                     sdPlus3 = mean(population$population) + (3 * sd(population$population)), 
                     sdMinus3 = mean(population$population) - (3 * sd(population$population)))

ggplot(population, aes(population)) +
  geom_density() +
  theme_minimal()
```


If we are observing a population that is normally distributed, we can know some things about it: the mean and the standard deviation. We also know that the mean, median, and mode are all the same. 

There is also a convenient rule: the 68-95-99.7 rule. This rule dictates that 68% of the distribution is contained within $\pm1\sigma$, 95% is contained within $\pm2\sigma$, and 99.7% is contained within $\pm3\sigma$. It is not functionally part of the rule, but 99.99% is contained under $\pm4\sigma$.

```{r}
ggplot(population, aes(population)) +
  geom_density() +
  geom_vline(xintercept = regions$sdPlus1, color = "red") +
  geom_vline(xintercept = regions$sdMinus1, color = "red") +
  geom_vline(xintercept = regions$sdPlus2, color = "blue") +
  geom_vline(xintercept = regions$sdMinus2, color = "blue") +
  geom_vline(xintercept = regions$sdPlus3, color = "green") +
  geom_vline(xintercept = regions$sdMinus3, color = "green") +
  theme_minimal()
```


The normal distribution is important, as many things are naturally normally distributed.

### Galton Boards

```{r, eval = FALSE}
library(animation)

ani.options(nmax = 215, interval = .5, autoplay = FALSE)

quincunx()
```


## Uniform Distribution

Likely the most vanilla of all distributions, the uniform distribution is pretty simple. We don't even get any fancy Greek letters to give us an idea about its shape, just a minimum and a maximum. Why? Because knowing the min and max will tell us that there is an equal probability of drawing a value anywhere within that range.

```{r}
samps = runif(r * n)

samp.means = sample.means(samps, r, n)

generate.plots(samps, samp.means)
```


### Poisson

The Poisson is an interesting distribution -- it tends to deal with count-related variables. It tells us the probability of a count occurring.  We know its $\lambda$, or average number of events (incidence rate).

```{r}
samps = rpois(r * n, lambda = 3)

samp.means = sample.means(samps, r, n)

generate.plots(samps, samp.means)
```


### Exponential

The exponential distribution is excellent when we are looking at how long something lasts or arrivals within a process (car part life, people joining a line, survival). We can only know one thing about the exponential distribution: $\mu$

```{r}
samps = rexp(r * n, rate = 1)

samp.means = sample.means(samps, r, n)

generate.plots(samps, samp.means)
```

## Determining Distributions

There might be times when you want to see how a variable might be distributed:

```{r, echo = TRUE}

library(fitdistrplus)

normalTest = rnorm(1000, mean = 2.5, sd = 1)

descdist(normalTest)

plot(fitdist(normalTest, distr = "norm"))

```

Of course we were able to fit a normal distribution to our data -- we created it by drawing from a normal distribution! Determining the underlying distribution of a variable has implications when we start using variables in models. 


<!-- Although not requisite, we might want to standardize our variables to fit a standard normal distribution:  -->

<!-- $$z = \frac{x_i-\mu}{\sigma}$$ -->

<!-- Transforming variables into *z*-scores makes it easy to compare values. -->

<!-- For example, we might take a person who scored a 3.2 on the auditor exam (CIA) in 2015. In 2014, a different auditor also scored a 3.2. Clearly, they both scored the same; however, the story should not end there. Let's consider the following information: -->

<!-- In 2014, the CIA had $\mu=3.04$ with a $\sigma=1.41$. -->

<!-- In 2015, the CIA had $\mu=2.86$ with a $\sigma=1.34$. -->

<!-- With that knowledge, which CIA examinee performed better compared to the population of examinees? -->

<!-- ```{r, echo = TRUE} -->
<!-- person2014Z = (3.2 - 3.04) / 1.41 -->

<!-- person2015Z = (3.2 - 2.86) / 1.34 -->
<!-- ``` -->

<!-- Let's play with this for a little bit: -->


<!-- ```{r, echo = TRUE} -->
<!-- ciaExam2014 = rnorm(n = 5000, mean = 3.04, sd = 1.41) -->

<!-- plot(density(ciaExam2014)) -->

<!-- abline(v = c(3.04, 3.2), col = c("black", "red")) -->
<!-- ``` -->

<!-- We can find out how many people our person in 2014 bested: -->

<!-- ```{r, echo = TRUE} -->
<!-- pnorm(3.2, 3.04, 1.41) -->

<!-- pnorm(person2014Z) -->
<!-- ``` -->

<!-- You can see that giving the *z* or the actual values produced the same results! -->

<!-- We can consider that everything that falls under our curve is 1 (i.e., it is 100%). So, if we wanted to find the proportion of people doing better than our reference people, we would just subtract our distribution function from 1. -->

<!-- ```{r} -->
<!-- 1 - pnorm(person2014Z) -->
<!-- ``` -->

<!-- We can also through a proportion to qnorm to find z. -->

<!-- ```{r, echo = TRUE} -->
<!-- qnorm(.45) -->
<!-- ``` -->

<!-- We can also use the `vistributions` package to play around with probability distributions visually. -->

<!-- ```{r, eval = FALSE} -->
<!-- install.packages(c("shinyBS", "vistributions")) -->

<!-- vistributions::vdist_launch_app() -->
<!-- ``` -->


## Populations and Samples

### Central Limit Theorem

The CLT dictates that as we increase the number of samples from a population, we will begin to approach normally-distributed means.

```{r}
pos = replicate(1000, sum(runif(16, -1, 1)))

plot(density(pos))
```


```{r}
library(plyr)

library(ggplot2)

dataSteps = function(stepSize) {
  walks = data.frame(person = rep(1:100, each = stepSize), 
           position = unlist(rlply(100, cumsum(c(0, runif((stepSize - 1), -1, 1))))), 
           step = rep(1:stepSize, times = 100))
  
  return(walks)
}

walks = dataSteps(16)

ggplot(walks, aes(step, position, group = person)) + 
  geom_line(color = "#ff5500", alpha = .5) + 
  theme_minimal()
```



### Sampling From Distributions

Let's start with a Gaussian distribution of 10000 observations:

```{r, echo = TRUE}
set.seed(1001)

population = rnorm(10000)

plot(density(population))
```

Now, let's take a small sample (*n* = 75) of our population:

```{r, echo = TRUE}
set.seed(1001)

smallSample = sample(population, 75, replace = FALSE)

plot(density(smallSample))
```

And now something a little bigger (*n* = 250):

```{r, echo = TRUE}
set.seed(1001)

mediumSample = sample(population, 250, replace = FALSE)

plot(density(mediumSample))
```

And bigger still (*n* = 1000):

```{r, echo = TRUE}
set.seed(1001)

biggerSample = sample(population, 1000, replace = FALSE)

plot(density(biggerSample))
```


And finally *n* = 2500:

```{r, echo = TRUE}
set.seed(1001)

biggestSample = sample(population, 2500, replace = FALSE)

plot(density(biggestSample))
```

Original:

```{r}
plot(density(population))
```


### What Is The Point?

We had our "population", so how well did our samples replicate the population distribution?

This starts to illustrate the *t*-distribution (more on this in a few weeks).

We are also getting into issues related to point estimation.

Let's consider the following:

```{r, echo = TRUE}
mean(population)

mean(biggerSample)

mean(biggestSample)
```

We can even take another sample from our population:

```{r, echo = TRUE}
mean(sample(population, 2500, replace = FALSE))
```

Let's take a bigger sample:

```{r, echo = TRUE}
mean(sample(population, 5000, replace = FALSE))
```

In and of itself, this is interesting. It has applications, however, to null hypothesis significance testing.